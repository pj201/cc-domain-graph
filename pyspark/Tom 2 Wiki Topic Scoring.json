{"paragraphs":[{"text":"%pyspark\n\n# PySpark CommonCrawl Topic Modelling\n# Tom V / Paul J - 13/2/2018\n\n# SET THE spark.driver.maxResultSize PROPERTY TO 16g\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\nimport pycld2 as cld2\n\n#wetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2017-04/wet.paths.gz\") # April(?) 2017 Crawl - 57000 files\n# Latest blog/documentation: http://commoncrawl.org/2017/10/october-2017-crawl-archive-now-available/\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2017-43/wet.paths.gz\") # October 2017 Crawl - 89100 files (10.58TB)\n\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x)[0].lang # Maybe we can get away with looking at less characters, or do something less expensive?\n    except Exception as e:\n        return None\n        \ndef detect2(x):\n    try:\n        isReliable, textBytesFound, details = cld2.detect(x)\n        return details[0][1]\n    except Exception as e:\n        print(e)\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file: # Approx 53k web pages per WET file\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                \n                # TODO: Limit number of bytes read per record e.g. read(200000)\n                \n                domain = None if not url else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8') #.limit(100) # TODO: Limit this read to ensure max length (for improving parallizability)\n                lang = detect2(text[:300]) # Use PyCLD2, not langdetect, which was killing performance!\n                yield domain, url, text, lang\n            except Exception as e:\n                yield e\n                \ndef process_wet_simple(id_, iterator):\n    count=0\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                count=count+1\n                # TODO: Output total size of pages, rather than number of pages\n                # Histogram.\n            except Exception as e:\n                pass\n        #print(count)\n        yield count","dateUpdated":"2018-02-13T14:51:25+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518533447131_1852027600","id":"20171020-102243_1718178582","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2193","user":"anonymous","dateFinished":"2018-02-13T14:51:51+0000","dateStarted":"2018-02-13T14:51:25+0000"},{"text":"%pyspark\ndetect2(\"this is a test\")","dateUpdated":"2018-02-13T14:50:47+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"'en'\n"}]},"apps":[],"jobName":"paragraph_1518533447135_1850488604","id":"20171027-134322_549744379","dateCreated":"2018-02-13T14:50:47+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2194"},{"text":"%pyspark\n\n# PARAMETER - number of input files\nnfiles = 1024 # Total 89100\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles)) #, numSlices=nfiles/32) # TODO: Try numSlices=nfiles/32 etc, or just default!\n\n# Should parallelize\nprint(files.getNumPartitions())\nrdd=files.mapPartitionsWithIndex(process_wet)\n\nprint(str(rdd))\ndocs = rdd.toDF([\"host\", \"url\", \"text\",\"lang\"]) #  \"lang\"\n#docs.cache()\n#docs.count() # Total docs in all languages","dateUpdated":"2018-02-13T15:35:32+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"320\nPythonRDD[179] at RDD at PythonRDD.scala:48\n"}]},"apps":[],"jobName":"paragraph_1518533447135_1850488604","id":"20171020-101220_2022546189","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2195","user":"anonymous","dateFinished":"2018-02-13T15:35:34+0000","dateStarted":"2018-02-13T15:35:32+0000"},{"text":"%pyspark\n\n# Filter for English only\ndocs_en = docs.filter(docs.lang == 'en')","dateUpdated":"2018-02-13T15:35:37+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518533447136_1836252895","id":"20171020-110218_1507019685","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2196","user":"anonymous","dateFinished":"2018-02-13T15:35:37+0000","dateStarted":"2018-02-13T15:35:37+0000"},{"title":"Load saved vectors from Wikipedia model (created by python Wikipedia Text Processing.ipynb)","text":"%pyspark\nfrom pyspark.ml import Pipeline,PipelineModel\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\nfrom pyspark.ml.clustering import LocalLDAModel\n\ntextModel = PipelineModel.load('s3://billsdata.net/CommonCrawl/wikipedia/text_model')\nldaModel = LocalLDAModel.load('s3://billsdata.net/CommonCrawl/wikipedia/lda_model')","dateUpdated":"2018-02-13T15:34:24+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518533447136_1836252895","id":"20171027-152148_622560673","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2197","user":"anonymous","dateFinished":"2018-02-13T15:34:28+0000","dateStarted":"2018-02-13T15:34:24+0000"},{"text":"%pyspark\n\n# Test the models - for debugging only\nimport numpy as np\nimport pandas as pd\n\nX=ldaModel.topicsMatrix().toArray()\nvocab = np.array(textModel.stages[2].vocabulary)\n\ntopicLabels = [' '.join(vocab[np.argsort(X[:,i])[::-1][:5]]) for i in range(100)]\n\ndef score_topics(text):\n    df = sqlContext.createDataFrame(pd.DataFrame({'text':[text]}))\n    vec = textModel.transform(df)\n    scores = ldaModel.transform(vec).select('topicDistribution').collect()[0].topicDistribution.toArray()\n    return pd.Series(dict(zip(topicLabels, scores)))\n    \n# Try it on an arbitary sentence\nprint(score_topics(\"This is the latest news about North Korea and their involvement in the Winter Olympics in PyongChang\"))","dateUpdated":"2018-02-13T15:35:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"africa south african singh harris                    0.001191\nage population census county average                 0.001250\nair flight aircraft airlines airline                 0.001200\naircraft air wing flying first                       0.001206\nairport florida miami international south            0.001217\nal virginia ohio states tennessee                    0.001265\nalbum band released song music                       0.001321\nalso cells cell protein used                         0.001235\narea city town region located                        0.001317\narmy th war military force                           0.001256\nart museum work artist painting                      0.001265\naustralia australian sydney navy south               0.001248\nbishop catholic st italian church                    0.001255\nblue wales white welsh black                         0.001216\nbr plant oil flowers leaves                          0.001264\nbuilding house built historic buildings              0.001295\nc p r g b                                            0.001337\ncanada canadian member assembly ontario              0.001286\ncar race racing class cars                           0.001215\ncentury one book also first                          0.001268\ncentury th castle polish ii                          0.001251\nchurch christian god religious day                   0.001263\ncity county street center community                  0.001298\nclub league football season played                   0.001293\nclub season signed born league                       0.001201\ncode used system number memory                       0.001194\ncollege university school born served                0.001323\ncolorado iowa arizona arkansas utah                  0.001211\ncommune serbian hungarian serbia hungary             0.001254\ncompany business bank million group                  0.001286\n                                                       ...   \nnational united organization states international    0.001272\nnew newspaper editor york norwegian                  0.001259\nnew york city ny jersey                              0.001237\nnuclear kelly allen energy burns                     0.001181\none ball game player players                         0.001210\none time man first also                              0.001295\nparty election elected elections democratic          0.001271\nradio station television channel tv                  0.447358\nriver lake park mountain creek                       0.001316\nroad parish village england centre                   0.001257\nrussian russia moscow soviet alexander               0.001225\nsan el francisco puerto philippines                  0.001245\nschool high schools girls boys                       0.001261\nschool students education university college         0.001276\nseason team first teams cup                          0.001276\nseries book published books novel                    0.001261\nseries show television also episode                  0.001292\nship ships two navy war                              0.001220\nsocial one may also people                           0.001240\nspace earth light solar star                         0.001223\nspecies found also large may                         0.001261\nstation line railway service train                   0.001261\nteam season coach football first                     0.001253\ntom oliver ghost haiti kay                           0.001183\nukrainian ukraine dog dogs stamps                    0.001198\nuniversity research professor published science      0.001323\nwar union soviet communist political                 0.001213\nwater company construction new coal                  0.001240\nworld olympics championships summer women            0.430250\nzealand new grand auckland prix                      0.001216\nLength: 100, dtype: float64\n"}]},"apps":[],"jobName":"paragraph_1518533447136_1836252895","id":"20180209-102157_2110186523","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2198","user":"anonymous","dateFinished":"2018-02-13T15:35:49+0000","dateStarted":"2018-02-13T15:35:44+0000"},{"text":"%pyspark\n\n# Now score pages from our WET files\ndocs_en.show(5)\nvec = textModel.transform(docs_en)\nvec.show(5)\n","dateUpdated":"2018-02-13T15:35:53+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+----+\n|                host|                 url|                text|lang|\n+--------------------+--------------------+--------------------+----+\n|                null|                null|Software-Info: ia...|  en|\n|1000daysofwriting...|http://1000daysof...|1000 Days of Writ...|  en|\n|100unhappydays.bl...|http://100unhappy...|100 Unhappy Days:...|  en|\n|          10in30.com|http://10in30.com...|LearnOutLoud_300x...|  en|\n|123-free-download...|http://123-free-d...|MusicBoxTool - [3...|  en|\n+--------------------+--------------------+--------------------+----+\nonly showing top 5 rows\n\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n|                host|                 url|                text|lang|               words|            filtered|                 vec|\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n|                null|                null|Software-Info: ia...|  en|[software, info, ...|[software, info, ...|(20000,[88,152,33...|\n|1000daysofwriting...|http://1000daysof...|1000 Days of Writ...|  en|[days, of, writin...|[days, writing, d...|(20000,[0,2,3,5,7...|\n|100unhappydays.bl...|http://100unhappy...|100 Unhappy Days:...|  en|[unhappy, days, d...|[unhappy, days, d...|(20000,[26,48,61,...|\n|          10in30.com|http://10in30.com...|LearnOutLoud_300x...|  en|[learnoutloud, x,...|[learnoutloud, x,...|(20000,[3,22,39,4...|\n|123-free-download...|http://123-free-d...|MusicBoxTool - [3...|  en|[musicboxtool, ho...|[musicboxtool, ho...|(20000,[3,5,22,59...|\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518533447136_1836252895","id":"20171020-101912_1610139389","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2199","user":"anonymous","dateFinished":"2018-02-13T15:35:56+0000","dateStarted":"2018-02-13T15:35:53+0000"},{"text":"%pyspark\n\n# Create topic distribution vectors and tidy upp\nscores = ldaModel.transform(vec)\nscores2 = scores.drop(\"url\").drop(\"text\").drop(\"lang\").drop(\"words\").drop(\"filtered\").drop(\"vec\")\nscores2.show(5)","dateUpdated":"2018-02-13T15:35:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|                host|   topicDistribution|\n+--------------------+--------------------+\n|                null|[0.13311231517264...|\n|1000daysofwriting...|[3.26238961502545...|\n|100unhappydays.bl...|[5.81230792144733...|\n|          10in30.com|[8.42785330446217...|\n|123-free-download...|[6.44166735802645...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518533447137_1835868146","id":"20180209-102904_2097212017","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2200","user":"anonymous","dateFinished":"2018-02-13T15:36:00+0000","dateStarted":"2018-02-13T15:35:58+0000"},{"text":"%pyspark\n\n# Save these vectors to disc, so we can just load them later\nscores2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)","dateUpdated":"2018-02-13T15:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518533447137_1835868146","id":"20180209-103007_1930247145","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2201","user":"anonymous","dateFinished":"2018-02-13T17:02:34+0000","dateStarted":"2018-02-13T15:36:02+0000"},{"title":"Load saved scores from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=1024\nscores2 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)\nscores2.show(5)","dateUpdated":"2018-02-13T15:36:19+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|              domain|   topicDistribution|\n+--------------------+--------------------+\n|                null|[0.12615256587902...|\n|1000daysofwriting...|[3.26238961502545...|\n|100unhappydays.bl...|[5.81230792144733...|\n|          10in30.com|[8.42785330446217...|\n|123-free-download...|[6.44166735802645...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518533447137_1835868146","id":"20180209-103529_782306138","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2202","user":"anonymous","dateFinished":"2018-02-13T14:52:07+0000","dateStarted":"2018-02-13T14:51:32+0000"},{"text":"%pyspark\n\n# Aggregate page-scores per Host for now (will be same process for aggregating host-scores to PLD-scores) - first count and sum vectors\nscores3=scores2.rdd.map(lambda x: (x['domain'], (1,x['topicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores4=scores3.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"host\", \"averageTopicDistribution\"])\nscores4.show(5)\n{s.name: type(s.dataType) for s in scores4.schema}\nprint(scores2.count())\nprint(scores4.count())","dateUpdated":"2018-02-13T15:29:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+------------------------+\n|                host|averageTopicDistribution|\n+--------------------+------------------------+\n|gattinawritercram...|    [2.15245888670342...|\n|www.stratifiedaut...|    [6.52478581426803...|\n|      www.edcast.com|    [3.20044544708958...|\n|www.indianyellowp...|    [0.00651007834318...|\n|     www.huskers.com|    [2.66916339495641...|\n+--------------------+------------------------+\nonly showing top 5 rows\n\n{'averageTopicDistribution': <class 'pyspark.ml.linalg.VectorUDT'>, 'host': <class 'pyspark.sql.types.StringType'>}\n"}]},"apps":[],"jobName":"paragraph_1518533447137_1835868146","id":"20180209-104711_1636843522","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2203","user":"anonymous","dateFinished":"2018-02-13T14:52:13+0000","dateStarted":"2018-02-13T14:51:52+0000"},{"text":"%pyspark\n\n# Just playing - code to help understand the different libraries and vector types!\nimport pyspark.mllib.linalg as mllib\nimport pyspark.ml.linalg as ml\ndf = sc.parallelize([\n    (mllib.DenseVector([1, ]), ml.DenseVector([1, ])),\n    (mllib.SparseVector(1, [0, ], [1, ]), ml.SparseVector(1, [0, ], [1, ]))\n]).toDF([\"mllib_v\", \"ml_v\"])\ndf.show()\n{s.name: type(s.dataType) for s in df.schema}","dateUpdated":"2018-02-13T14:52:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+-------------+\n|      mllib_v|         ml_v|\n+-------------+-------------+\n|        [1.0]|        [1.0]|\n|(1,[0],[1.0])|(1,[0],[1.0])|\n+-------------+-------------+\n\n{'ml_v': <class 'pyspark.ml.linalg.VectorUDT'>, 'mllib_v': <class 'pyspark.mllib.linalg.VectorUDT'>}\n"}]},"apps":[],"jobName":"paragraph_1518533447137_1835868146","id":"20180213-131625_601576088","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2204","user":"anonymous","dateFinished":"2018-02-13T14:52:18+0000","dateStarted":"2018-02-13T14:52:16+0000"},{"text":"%pyspark\n\n# Enrich each row with the corresponding PLD (using code from Paul J)\n# TODO: Should just pickle the Bloom filter and load it in!\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-aug-sep-oct/domaingraph/vertices/\"\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()","dateUpdated":"2018-02-13T15:47:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n| ID|  PLD|\n+---+-----+\n|  0|aaa.1|\n|  1|aaa.2|\n|  2|aaa.3|\n+---+-----+\nonly showing top 3 rows\n\nDataFrame[ID: string, PLD: string]\n"}]},"apps":[],"jobName":"paragraph_1518533447138_1837022393","id":"20180209-103948_583472396","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2205","user":"anonymous","dateFinished":"2018-02-13T14:52:23+0000","dateStarted":"2018-02-13T14:52:20+0000"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when counting hosts\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=94000000, error_rate=0.005) # was 91M\n\nfor row in pld_df.rdd.collect(): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\nprint(pld_df.rdd.take(3))\nprint(pld_df.rdd.take(3)[2]['PLD'])\n#pld_bf.add(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false\n","dateUpdated":"2018-02-13T14:52:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518533447138_1837022393","id":"20180213-133625_1860489211","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2206","user":"anonymous","dateFinished":"2018-02-13T15:20:47+0000","dateStarted":"2018-02-13T14:52:25+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(ID=u'0', PLD=u'aaa.1'), Row(ID=u'1', PLD=u'aaa.2'), Row(ID=u'2', PLD=u'aaa.3')]\naaa.3\nTrue\n64\n93110180\nTrue\nFalse\nTrue\nFalse\n"}]}},{"text":"%pyspark\n\nfrom pyspark.sql.functions import udf\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n\nudf_convert_hostname = udf(convert_hostname, StringType())\n\n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","dateUpdated":"2018-02-13T14:52:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518533447139_1836637644","id":"20180213-133744_1577422019","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2207","user":"anonymous","dateFinished":"2018-02-13T15:20:47+0000","dateStarted":"2018-02-13T14:52:29+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]}},{"text":"%pyspark\n\n# Function to reverse hostnames\nfrom pyspark.sql.functions import udf\ndef reverse_domain(domain):\n    try:\n        ret =  '.'.join(reversed(domain.split('.')))\n        return ret\n    except:\n        return \"NODOMAIN\"\nprint(reverse_domain(\"com.facebook\"))\nudf_reverse_domain = udf(reverse_domain, StringType())\n\n# Convert hosts in Topic DF to PLDs using convert_hostname function from Paul 5.\nscores5=scores4.withColumn(\"pld_rev\",udf_reverse_domain(udf_convert_hostname(udf_reverse_domain(\"host\")))) # Reverse the hostnames prior to lookup, then back again!\nscores5.show(10)","dateUpdated":"2018-02-13T15:25:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518533447139_1836637644","id":"20180213-133858_1524477020","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2208","user":"anonymous","dateFinished":"2018-02-13T15:25:46+0000","dateStarted":"2018-02-13T15:25:46+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"facebook.com\n+--------------------+------------------------+--------------------+\n|                host|averageTopicDistribution|             pld_rev|\n+--------------------+------------------------+--------------------+\n|gattinawritercram...|    [2.15245888670342...|        blogspot.com|\n|www.stratifiedaut...|    [6.52478581426803...|  stratifiedauto.com|\n|      www.edcast.com|    [3.20044544708958...|          edcast.com|\n|www.indianyellowp...|    [0.00651007834318...|indianyellowpages...|\n|     www.huskers.com|    [2.66916339495641...|         huskers.com|\n|jeofurry.blogspot...|    [1.68556714565171...|        blogspot.com|\n|www.brickmodeldes...|    [4.93337221161091...|brickmodeldesign.com|\n|      us.petvalu.com|    [2.40757190496072...|         petvalu.com|\n|www.bobgarontrain...|    [3.31587146684675...|bobgarontraining.com|\n|    www.homefocus.ie|    [0.01917307556468...|        homefocus.ie|\n+--------------------+------------------------+--------------------+\nonly showing top 10 rows\n\n"}]}},{"text":"%pyspark\n\n# Now we can aggregate page-scores per PLD, using a map-reduce similar to the host aggregation above - first count and sum vectors\nscores6=scores5.rdd.map(lambda x: (x['pld_rev'], (1,x['averageTopicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores7=scores6.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"pld_rev\", \"averageTopicDistribution\"])\nscores7.show(5)\n{s.name: type(s.dataType) for s in scores7.schema}\nprint(scores5.count())\nprint(scores7.count())","dateUpdated":"2018-02-13T15:29:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518533447139_1836637644","id":"20180213-140037_440180641","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2209","user":"anonymous","dateFinished":"2018-02-13T15:29:22+0000","dateStarted":"2018-02-13T15:29:12+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+------------------------+\n|             pld_rev|averageTopicDistribution|\n+--------------------+------------------------+\n|digitalhorizonson...|    [0.00144483565419...|\n|kitchencollection...|    [0.01798544424072...|\n|            ieee.org|    [3.84272202396751...|\n|        museumca.org|    [1.67996725189244...|\n|thepsoriasisprogr...|    [3.37113604892104...|\n+--------------------+------------------------+\nonly showing top 5 rows\n\n12243\n9141\n"}]}},{"text":"%pyspark\n\n# Save pld topic distributions in parquet format for Tom to play with (and to figure out how to create a PLD topic summary from this).\n# TODO: Maybe a numpy argmax to get the index of the 'top' topic for each PLD with a score.\nscores7.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)","dateUpdated":"2018-02-13T15:31:11+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518533447140_1834713899","id":"20180209-104436_1341725902","dateCreated":"2018-02-13T14:50:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2210","user":"anonymous","dateFinished":"2018-02-13T15:32:11+0000","dateStarted":"2018-02-13T15:31:11+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2018-02-13T15:31:11+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518535871932_-190925595","id":"20180213-153111_1438771373","dateCreated":"2018-02-13T15:31:11+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3772"}],"name":"Tom 2 Wiki Topic Scoring","id":"2D8ZGKG1U","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}