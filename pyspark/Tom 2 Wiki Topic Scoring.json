{"paragraphs":[{"text":"%pyspark\n\n# PySpark CommonCrawl Topic Modelling\n# Tom V / Paul J - 14/2/2018\n\n# SET THE spark.driver.maxResultSize PROPERTY TO 16g\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\nimport pycld2 as cld2\n\n# Latest blog/documentation: http://commoncrawl.org/2018/01/january-2018-crawl-archive-now-available/\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2018-05/wet.paths.gz\") # Jan 2018 Crawl - 80000 files (9.29TB)\n\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x)[0].lang # Maybe we can get away with looking at less characters, or do something less expensive?\n    except Exception as e:\n        return None\n        \ndef detect2(x):\n    try:\n        isReliable, textBytesFound, details = cld2.detect(x)\n        return details[0][1]\n    except Exception as e:\n        print(e)\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file: # Approx 53k web pages per WET file\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                \n                # TODO: Limit number of bytes read per record e.g. read(200000)\n                \n                domain = None if not url else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8') #.limit(100) # TODO: Limit this read to ensure max length (for improving parallizability)\n                lang = detect2(text[:300]) # Use PyCLD2, not langdetect, which was killing performance!\n                yield domain, url, text, lang\n            except Exception as e:\n                yield e\n                \ndef process_wet_simple(id_, iterator):\n    count=0\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                count=count+1\n                # TODO: Output total size of pages, rather than number of pages\n                # Histogram.\n            except Exception as e:\n                pass\n        #print(count)\n        yield count","user":"anonymous","dateUpdated":"2018-03-02T09:31:39+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1519983091963_1179843897","id":"20171020-102243_1718178582","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T09:31:39+0000","dateFinished":"2018-03-02T09:32:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:27092"},{"text":"%pyspark\ndetect2(\"this is a test\")","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091963_1179843897","id":"20171027-134322_549744379","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27093"},{"text":"%pyspark\n\n# PARAMETER - number of input files\nnfiles = 8192 # Total 80000\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles)) #, numSlices=nfiles/32) # TODO: Try numSlices=nfiles/32 etc, or just default!\n\n# Should parallelize\nprint(files.getNumPartitions())\nrdd=files.mapPartitionsWithIndex(process_wet)\n\nprint(str(rdd))\ndocs = rdd.toDF([\"host\", \"url\", \"text\",\"lang\"]) #  \"lang\"\n#docs.cache()\n#docs.count() # Total docs in all languages","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20171020-101220_2022546189","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27094"},{"text":"%pyspark\n\n# Filter for English only\ndocs_en = docs.filter(docs.lang == 'en')","dateUpdated":"2018-03-02T09:31:31+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20171020-110218_1507019685","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27095"},{"title":"Load saved vectors from Wikipedia model (created by python Wikipedia Text Processing.ipynb)","text":"%pyspark\nfrom pyspark.ml import Pipeline,PipelineModel\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\nfrom pyspark.ml.clustering import LocalLDAModel\n\ntextModel = PipelineModel.load('s3://billsdata.net/CommonCrawl/wikipedia/text_model')\nldaModel = LocalLDAModel.load('s3://billsdata.net/CommonCrawl/wikipedia/lda_model')","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20171027-152148_622560673","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27096"},{"text":"%pyspark\n\n# Test the models - for debugging only\nimport numpy as np\nimport pandas as pd\n\nX=ldaModel.topicsMatrix().toArray()\nvocab = np.array(textModel.stages[2].vocabulary)\n\ntopicLabels = [' '.join(vocab[np.argsort(X[:,i])[::-1][:5]]) for i in range(100)]\n\ndef score_topics(text):\n    df = sqlContext.createDataFrame(pd.DataFrame({'text':[text]}))\n    vec = textModel.transform(df)\n    scores = ldaModel.transform(vec).select('topicDistribution').collect()[0].topicDistribution.toArray()\n    return pd.Series(dict(zip(topicLabels, scores)))\n    \n# Try it on an arbitary sentence\nprint(score_topics(\"This is the latest news about North Korea and their involvement in the Winter Olympics in PyongChang\"))","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20180209-102157_2110186523","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27097"},{"text":"%pyspark\n\n# Now score pages from our WET files\ndocs_en.show(5)\nvec = textModel.transform(docs_en)\nvec.show(5)\n","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091965_1177535403","id":"20171020-101912_1610139389","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27098"},{"text":"%pyspark\n\n# Create topic distribution vectors and tidy upp\nscores = ldaModel.transform(vec)\nscores2 = scores.drop(\"url\").drop(\"text\").drop(\"lang\").drop(\"words\").drop(\"filtered\").drop(\"vec\")\nscores2.show(5)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091965_1177535403","id":"20180209-102904_2097212017","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27099"},{"text":"%pyspark\n\n# Save these vectors to disc, so we can just load them later\nscores2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091965_1177535403","id":"20180209-103007_1930247145","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27100"},{"title":"Load saved scores from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=8192\nscores2 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)\nscores2.show(5)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180209-103529_782306138","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27101"},{"text":"%pyspark\n\n# Aggregate page-scores per Host for now (will be same process for aggregating host-scores to PLD-scores) - first count and sum vectors\nscores3=scores2.rdd.map(lambda x: (x['host'], (1,x['topicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores4=scores3.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"host\", \"averageTopicDistribution\"])\nscores4.show(5)\n{s.name: type(s.dataType) for s in scores4.schema}\nprint(scores2.count())\nprint(scores4.count())","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180209-104711_1636843522","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27102"},{"text":"%pyspark\n\n# Just playing - code to help understand the different libraries and vector types!\nimport pyspark.mllib.linalg as mllib\nimport pyspark.ml.linalg as ml\ndf = sc.parallelize([\n    (mllib.DenseVector([1, ]), ml.DenseVector([1, ])),\n    (mllib.SparseVector(1, [0, ], [1, ]), ml.SparseVector(1, [0, ], [1, ]))\n]).toDF([\"mllib_v\", \"ml_v\"])\ndf.show()\n{s.name: type(s.dataType) for s in df.schema}","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180213-131625_601576088","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27103"},{"text":"%pyspark\n\n# Enrich each row with the corresponding PLD (using code from Paul J)\n#saveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-aug-sep-oct/domaingraph/vertices/\"\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-18-nov-dec-jan/domaingraph/vertices/\"\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\npld_df.count() # 70M in latest web graph","user":"anonymous","dateUpdated":"2018-03-02T10:24:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180209-103948_583472396","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T10:24:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27104","dateFinished":"2018-03-02T10:25:19+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+--------+\n| ID|    PLD|NumHosts|\n+---+-------+--------+\n|  0|  aaa.a|       1|\n|  1| aaa.aa|       1|\n|  2|aaa.aaa|       4|\n+---+-------+--------+\nonly showing top 3 rows\n\n70367284\n"}]}},{"text":"%pyspark\n\n# Generate a Bloom Filter of PLDs from the vertex file\nfrom pybloom import BloomFilter\n\n# Attempt to distribute our Bloom Filter so we can build it in parallel, and save it to disk as an RDD\ndef build_partial_bloom(capacity, error_rate):\n    def _build_partial_bloom(record):\n        bloom_filter = BloomFilter(capacity=capacity, error_rate=error_rate)\n        for _,PLD,_ in record: # Just take the PLD field (not the ID or NumHosts)\n            bloom_filter.add(PLD)\n        yield (None, bloom_filter) # returns a Generator (i.e. a kind of Iterator that can only get called once, and doesn't stay in memory)\n    return _build_partial_bloom\n\ndef merge_bloom(bloom_in1, bloom_in2):\n    return bloom_in1.union(bloom_in2) # The reduce function simply becomes a Union\n\n# Our generator function for partial Bloom Filters\ngenerate_bloom = build_partial_bloom(capacity=94000000, error_rate=0.005) # Only 70M in latest web graph?\n\n# Construct the distributed BloomFilter using the PLD dataframe\nbloom_filter_rdd = pld_df.rdd.mapPartitions(generate_bloom).reduceByKey(merge_bloom)\n\n# Save/Load the Bloom Filter RDD - not working\n#saveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-18-nov-dec-jan/domaingraph/vertex_bloom_filter/\"\n#bloom_filter_rdd.saveAsTextFile(saveURI)\n#bloom_filter_rdd=spark.read.load(saveURI)\n\n# Collect and broadcast to cluster nodes\nbloom_filter=bloom_filter_rdd.collect()\nprint(bloom_filter)\npld_bf_distrib = sc.broadcast(bloom_filter[0][1])\n\n# Test contents\nimport sys\nprint(sys.getsizeof(pld_bf_distrib.value))\nprint(len(pld_bf_distrib.value)) # Should match number of items entered\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180214-130159_1231127388","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27105"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import udf\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n\nudf_convert_hostname = udf(convert_hostname, StringType())\n\n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-133744_1577422019","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27106"},{"text":"%pyspark\n\n# Function to reverse hostnames\nfrom pyspark.sql.functions import udf\ndef reverse_domain(domain):\n    try:\n        ret =  '.'.join(reversed(domain.split('.')))\n        return ret\n    except:\n        return \"NODOMAIN\"\nprint(reverse_domain(\"com.facebook\"))\nudf_reverse_domain = udf(reverse_domain, StringType())\n\n# Convert hosts in Topic DF to PLDs using convert_hostname function from Paul 5.\nscores5=scores4.withColumn(\"pld\",udf_reverse_domain(udf_convert_hostname(udf_reverse_domain(\"host\")))) # Reverse the hostnames prior to lookup, then back again!\nscores5.show(10)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-133858_1524477020","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27107"},{"text":"%pyspark\n\n# Now we can aggregate page-scores per PLD, using a map-reduce similar to the host aggregation above - first count and sum vectors\nscores6=scores5.rdd.map(lambda x: (x['pld'], (1,x['averageTopicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores7=scores6.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"pld_rev\", \"averageTopicDistribution\"])\nscores7.show(5)\n{s.name: type(s.dataType) for s in scores7.schema}\nprint(scores5.count())\nprint(scores7.count())","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-140037_440180641","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27108"},{"text":"%pyspark\n\n# Save pld topic distributions in parquet format for Tom to play with (and to figure out how to create a PLD topic summary from this).\n# TODO: Maybe a numpy argmax to get the index of the 'top' topic for each PLD with a score.\nscores7.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180209-104436_1341725902","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27109"},{"title":"Load saved PLD vectors from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=8192\nscores7 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)\nscores7.show(5)","user":"anonymous","dateUpdated":"2018-03-02T09:32:02+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+------------------------+\n|             pld_rev|averageTopicDistribution|\n+--------------------+------------------------+\n|          scrtec.org|    [0.00101137139816...|\n|corporatefitnessc...|    [1.16246367421139...|\n|        1centweb.com|    [1.94489585421988...|\n|   mygaragestory.net|    [2.04009774069444...|\n|     gitedeville.com|    [1.18827049530001...|\n+--------------------+------------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-153111_1438771373","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T09:32:02+0000","dateFinished":"2018-03-02T09:32:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27110"},{"text":"%pyspark\n\n# Next run t-SNE embedding on the vectors to reduce from indlen dimensions to 2\nimport numpy as np\nfrom sklearn.manifold import TSNE\nscores8=scores7.limit(10000)\nprint(scores8.count())\nX=np.array(scores8.select('averageTopicDistribution').collect())\nprint(X)\nnew_X = np.array([i[0] for i in X])\nprint(new_X)\nX_embedded = TSNE(n_components=2).fit_transform(new_X)\nX_embedded.shape","user":"anonymous","dateUpdated":"2018-03-02T10:15:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"10000\n[[[  1.01137140e-03   9.66949978e-04   1.06956744e-03 ...,   9.89253450e-04\n     9.42083646e-04   9.78398843e-04]]\n\n [[  1.16246367e-04   1.11140598e-04   1.22935382e-04 ...,   1.13704145e-04\n     1.08282478e-04   1.12456523e-04]]\n\n [[  1.94489585e-04   1.85947220e-04   2.05680848e-04 ...,   1.90236241e-04\n     1.81165354e-04   1.88148870e-04]]\n\n ..., \n [[  1.80705922e-04   1.72768963e-04   1.91104049e-04 ...,   1.76754016e-04\n     1.68325992e-04   1.74814579e-04]]\n\n [[  2.88131243e-05   2.75475952e-05   3.04710806e-05 ...,   2.81830024e-05\n     2.68391742e-05   2.78737638e-05]]\n\n [[  7.76791115e-04   7.42672922e-04   8.21489002e-04 ...,   7.59803265e-04\n     7.23574156e-04   7.51466306e-04]]]\n[[  1.01137140e-03   9.66949978e-04   1.06956744e-03 ...,   9.89253450e-04\n    9.42083646e-04   9.78398843e-04]\n [  1.16246367e-04   1.11140598e-04   1.22935382e-04 ...,   1.13704145e-04\n    1.08282478e-04   1.12456523e-04]\n [  1.94489585e-04   1.85947220e-04   2.05680848e-04 ...,   1.90236241e-04\n    1.81165354e-04   1.88148870e-04]\n ..., \n [  1.80705922e-04   1.72768963e-04   1.91104049e-04 ...,   1.76754016e-04\n    1.68325992e-04   1.74814579e-04]\n [  2.88131243e-05   2.75475952e-05   3.04710806e-05 ...,   2.81830024e-05\n    2.68391742e-05   2.78737638e-05]\n [  7.76791115e-04   7.42672922e-04   8.21489002e-04 ...,   7.59803265e-04\n    7.23574156e-04   7.51466306e-04]]\n(10000, 2)\n"}]},"apps":[],"jobName":"paragraph_1519983091968_1262564910","id":"20180220-102405_700504264","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T10:15:52+0000","dateFinished":"2018-03-02T10:21:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27111"},{"text":"%pyspark\n\n# Let's try the same thing in UMAP (BEWARE: install umap-learn, not umap!)\nimport umap\nU_embedded = umap.UMAP().fit_transform(new_X)\nU_embedded.shape","user":"anonymous","dateUpdated":"2018-03-02T10:16:06+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(10000, 2)\n"}]},"apps":[],"jobName":"paragraph_1519983091968_1262564910","id":"20180220-110055_1082533544","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T10:16:06+0000","dateFinished":"2018-03-02T10:21:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27112"},{"text":"%pyspark\n\n# Save as CSV for loading by standalone Bokeh demo (tidied up from 2_Create_Summaries_For_Bokeh)\n\n# Add tSNE and umap coordinates as new columns\ntsne_col = sc.parallelize(X_embedded.tolist(), 1)\numap_col = sc.parallelize(U_embedded.tolist(), 1)\nprint(tsne_col.count(),umap_col.count(),scores8.count())\ndef process_tsne(pair):\n   return dict(pair[0].asDict().items() + [(\"tSNE\", pair[1])])\ndef process_umap(pair):\n   return dict(pair[0].asDict().items() + [(\"umap\", pair[1])])\nprint(scores8.count())\nwith_tsne_rdd = (scores8.rdd.coalesce(1).zip(tsne_col).map(process_tsne)) # Add new column for tsne # TODO: Ensure same 10,000 as above!!\nscores_tsne = sqlContext.createDataFrame(with_tsne_rdd) # Rebuild data frame\nprint(scores_tsne.count())\nwith_umap_rdd = (scores_tsne.rdd.coalesce(1).zip(umap_col).map(process_umap)) # Add new column for umap\nscores_tsne_umap = sqlContext.createDataFrame(with_umap_rdd) # Rebuild data frame\n\n# convert tSNE & umap to strings (csv export wouldn't allow them as arrays)\nfrom pyspark.sql.functions import udf\ndef str_func(embedArray):\n    outArray = []\n    for e in embedArray:\n        outArray.append(str(e))\n    return ','.join(outArray)\nstr_udf = udf(str_func, StringType())\nscores8=scores_tsne_umap.withColumn(\"tSNEString\", str_udf(scores_tsne_umap.tSNE)).withColumn(\"umapString\", str_udf(scores_tsne_umap.umap))\nscores8.show(5)","user":"anonymous","dateUpdated":"2018-03-02T10:22:09+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(10000, 10000, 10000)\n10000\n10000\n+------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|averageTopicDistribution|             pld_rev|                tSNE|                umap|          tSNEString|          umapString|\n+------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|    [3.15059422331988...|servprometrocrest...|[-4.4032044410705...|[-5.6035976260947...|-4.40320444107,-7...|-5.60359762609,9....|\n|    [5.27446243610823...|waynestatecollege...|[57.6027946472168...|[-0.9963978965742...|57.6027946472,-7....|-0.996397896574,-...|\n|    [5.18636606421855...|      isrbrevard.com|[2.58689379692077...|[-1.5378591424965...|2.58689379692,-40...|-1.5378591425,4.8...|\n|    [4.41861945505762...|dirttechlandscapi...|[-42.294990539550...|[-3.8410027676229...|-42.2949905396,-9...|-3.84100276762,2....|\n|    [0.00148971332672...|septicpreservatio...|[-21.944774627685...|[-0.3517232434838...|-21.9447746277,-8...|-0.351723243484,0...|\n+------------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1519983139525_-630537596","id":"20180302-093219_846610021","dateCreated":"2018-03-02T09:32:19+0000","dateStarted":"2018-03-02T10:22:09+0000","dateFinished":"2018-03-02T10:22:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27113"},{"text":"%pyspark\n\n# Take reversed PLD and switch back to the right way round\nfrom pyspark.sql.functions import udf, col, when, lit, concat_ws, split\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook.abc\"))\n\nudf_reverse_domain = udf(reverse_domain, StringType())\nscores9=scores8.withColumn(\"actual_pld_rev\",udf_reverse_domain(\"pld_rev\"))\nscores10=scores9.join(pld_df, pld_df.PLD==scores9.actual_pld_rev).withColumnRenamed(\"pld_rev\",\"payLevelDomain\").select(\"payLevelDomain\",\"NumHosts\",\"tSNEString\",\"umapString\")\n#scores9=scores8.join(pld_df,\"PLD\").withColumnRenamed(\"PLD\",\"PLD_rev\").withColumn(\"payLevelDomain\",udf_reverse_domain(\"PLD_rev\")).select(\"payLevelDomain\",\"NumHosts\",\"StatusColour\",\"tSNEString\",\"umapString\")\nscores10.show(3)","user":"anonymous","dateUpdated":"2018-03-02T10:30:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983779708_591942120","id":"20180302-094259_407353261","dateCreated":"2018-03-02T09:42:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27114","dateFinished":"2018-03-02T10:30:54+0000","dateStarted":"2018-03-02T10:30:30+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"abc.facebook.com\n+--------------------+--------+--------------------+--------------------+\n|      payLevelDomain|NumHosts|          tSNEString|          umapString|\n+--------------------+--------+--------------------+--------------------+\n|            cripe.ca|       1|-15.979842186,-36...|-1.1282559629,3.2...|\n|    anythehoorah.com|       2|2.81252336502,26....|0.261848978601,-4...|\n|arizonaduderanche...|       1|-2.5158662796,-79...|-5.56482661887,9....|\n+--------------------+--------+--------------------+--------------------+\nonly showing top 3 rows\n\n"}]}},{"text":"%pyspark\n\n# Export as CSV\ntopicEmbeddingsURI=\"s3://billsdata.net/CommonCrawl/domain_embeddings_topic_10000_2017-18-nov-dec-jan/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\nscores10.coalesce(1).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(topicEmbeddingsURI)","user":"anonymous","dateUpdated":"2018-03-02T10:31:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983815830_-340476320","id":"20180302-094335_509647219","dateCreated":"2018-03-02T09:43:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:27115","dateFinished":"2018-03-02T10:33:13+0000","dateStarted":"2018-03-02T10:31:12+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%pyspark\n\n# Plot the t-SNE embedding using matplotlib in Zeppelin notebook\nimport matplotlib\nimport StringIO\n\n# Turn off interactive mode - we really want this but it leads to dependency errors on AWS linux!\nmatplotlib.use('agg',warn=False, force=True) # Removes Tkinter error (no interactive plots on AWS EMR)\nfrom matplotlib import pyplot as plt\nprint \"Switched to:\",matplotlib.get_backend()\n\nfig, ax = plt.subplots(nrows=1, ncols=2) #, figsize=(40,16))\n#print(X_embedded)\n\n#colors=np.array(pld_codes_with_vec_sc.select('StatusColour').collect())\n#colors2 = np.array([i[0] for i in colors])\n#groups=np.array(pld_codes_with_vec_sc.select('StatusClass').collect())\n#groups2 = np.array([i[0] for i in groups])\n#zip_list = np.array(zip(pld_codes_with_vec_sc.collect(),X_embedded))\n#x_emb = zip_list[:,1]\n#x=[row[0] for row in x_emb]\n#y=[row[1] for row in x_emb]\n\n#ax.scatter(X_embedded[:,0], X_embedded[:,1], s=1.0, c=colors2, edgecolors='face', cmap='rainbow')\n#ax.scatter(x, y, s=1.0, c=colors2, edgecolors='face', cmap='rainbow')\nax[0].set_title(\"t-SNE embedding of PLD topic vectors\")\nax[0].scatter(X_embedded[:,0], X_embedded[:,1], s=1.0)\nax[1].set_title(\"UMAP embedding of PLD topic vectors\")\nax[1].scatter(U_embedded[:,0], U_embedded[:,1], s=1.0)\n\n# plt.show() doesn't seem to work so this is a workaround\ndef show(p):\n    img = StringIO.StringIO()\n    p.savefig(img, format='svg')\n    img.seek(0)\n    print \"%html <div style='width:600px'>\" + img.buf + \"</div>\"\n\n#plt.legend(loc=2)\nshow(plt)","dateUpdated":"2018-03-02T09:42:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091968_1262564910","id":"20180220-104501_1741100148","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27116"},{"text":"%pyspark\n\n\n","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091968_1262564910","id":"20180220-102816_491346592","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27117"}],"name":"Tom 2 Wiki Topic Scoring","id":"2D8RF8QCE","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}