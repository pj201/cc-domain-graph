{"paragraphs":[{"text":"%pyspark\n\n# PySpark CommonCrawl Topic Modelling\n# Tom V / Paul J - 14/2/2018\n\n# SET THE spark.driver.maxResultSize PROPERTY TO 16g\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\nimport pycld2 as cld2\n\n# Latest blog/documentation: http://commoncrawl.org/2018/01/january-2018-crawl-archive-now-available/\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2018-05/wet.paths.gz\") # Jan 2018 Crawl - 80000 files (9.29TB)\n\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x)[0].lang # Maybe we can get away with looking at less characters, or do something less expensive?\n    except Exception as e:\n        return None\n        \ndef detect2(x):\n    try:\n        isReliable, textBytesFound, details = cld2.detect(x)\n        return details[0][1]\n    except Exception as e:\n        print(e)\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file: # Approx 53k web pages per WET file\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                \n                # TODO: Limit number of bytes read per record e.g. read(200000)\n                \n                domain = None if not url else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8') #.limit(100) # TODO: Limit this read to ensure max length (for improving parallizability)\n                lang = detect2(text[:300]) # Use PyCLD2, not langdetect, which was killing performance!\n                yield domain, url, text, lang\n            except Exception as e:\n                yield e\n                \ndef process_wet_simple(id_, iterator):\n    count=0\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                count=count+1\n                # TODO: Output total size of pages, rather than number of pages\n                # Histogram.\n            except Exception as e:\n                pass\n        #print(count)\n        yield count","dateUpdated":"2018-02-20T10:22:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149780_-2029679002","id":"20171020-102243_1718178582","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10073"},{"text":"%pyspark\ndetect2(\"this is a test\")","dateUpdated":"2018-02-20T10:22:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149786_-2030448500","id":"20171027-134322_549744379","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10074"},{"text":"%pyspark\n\n# PARAMETER - number of input files\nnfiles = 8192 # Total 80000\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles)) #, numSlices=nfiles/32) # TODO: Try numSlices=nfiles/32 etc, or just default!\n\n# Should parallelize\nprint(files.getNumPartitions())\nrdd=files.mapPartitionsWithIndex(process_wet)\n\nprint(str(rdd))\ndocs = rdd.toDF([\"host\", \"url\", \"text\",\"lang\"]) #  \"lang\"\n#docs.cache()\n#docs.count() # Total docs in all languages","dateUpdated":"2018-02-20T10:22:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149786_-2030448500","id":"20171020-101220_2022546189","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10075"},{"text":"%pyspark\n\n# Filter for English only\ndocs_en = docs.filter(docs.lang == 'en')","dateUpdated":"2018-02-20T10:22:29+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149787_-2030833249","id":"20171020-110218_1507019685","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10076"},{"title":"Load saved vectors from Wikipedia model (created by python Wikipedia Text Processing.ipynb)","text":"%pyspark\nfrom pyspark.ml import Pipeline,PipelineModel\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\nfrom pyspark.ml.clustering import LocalLDAModel\n\ntextModel = PipelineModel.load('s3://billsdata.net/CommonCrawl/wikipedia/text_model')\nldaModel = LocalLDAModel.load('s3://billsdata.net/CommonCrawl/wikipedia/lda_model')","dateUpdated":"2018-02-20T10:22:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149787_-2030833249","id":"20171027-152148_622560673","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10077"},{"text":"%pyspark\n\n# Test the models - for debugging only\nimport numpy as np\nimport pandas as pd\n\nX=ldaModel.topicsMatrix().toArray()\nvocab = np.array(textModel.stages[2].vocabulary)\n\ntopicLabels = [' '.join(vocab[np.argsort(X[:,i])[::-1][:5]]) for i in range(100)]\n\ndef score_topics(text):\n    df = sqlContext.createDataFrame(pd.DataFrame({'text':[text]}))\n    vec = textModel.transform(df)\n    scores = ldaModel.transform(vec).select('topicDistribution').collect()[0].topicDistribution.toArray()\n    return pd.Series(dict(zip(topicLabels, scores)))\n    \n# Try it on an arbitary sentence\nprint(score_topics(\"This is the latest news about North Korea and their involvement in the Winter Olympics in PyongChang\"))","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149788_-2032756993","id":"20180209-102157_2110186523","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10078"},{"text":"%pyspark\n\n# Now score pages from our WET files\ndocs_en.show(5)\nvec = textModel.transform(docs_en)\nvec.show(5)\n","dateUpdated":"2018-02-20T10:22:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149788_-2032756993","id":"20171020-101912_1610139389","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10079"},{"text":"%pyspark\n\n# Create topic distribution vectors and tidy upp\nscores = ldaModel.transform(vec)\nscores2 = scores.drop(\"url\").drop(\"text\").drop(\"lang\").drop(\"words\").drop(\"filtered\").drop(\"vec\")\nscores2.show(5)","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149789_-2033141742","id":"20180209-102904_2097212017","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10080"},{"text":"%pyspark\n\n# Save these vectors to disc, so we can just load them later\nscores2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149789_-2033141742","id":"20180209-103007_1930247145","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10081"},{"title":"Load saved scores from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=8192\nscores2 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)\nscores2.show(5)","dateUpdated":"2018-02-20T10:23:44+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149790_-2031987495","id":"20180209-103529_782306138","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10082"},{"text":"%pyspark\n\n# Aggregate page-scores per Host for now (will be same process for aggregating host-scores to PLD-scores) - first count and sum vectors\nscores3=scores2.rdd.map(lambda x: (x['host'], (1,x['topicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores4=scores3.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"host\", \"averageTopicDistribution\"])\nscores4.show(5)\n{s.name: type(s.dataType) for s in scores4.schema}\nprint(scores2.count())\nprint(scores4.count())","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149790_-2031987495","id":"20180209-104711_1636843522","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10083"},{"text":"%pyspark\n\n# Just playing - code to help understand the different libraries and vector types!\nimport pyspark.mllib.linalg as mllib\nimport pyspark.ml.linalg as ml\ndf = sc.parallelize([\n    (mllib.DenseVector([1, ]), ml.DenseVector([1, ])),\n    (mllib.SparseVector(1, [0, ], [1, ]), ml.SparseVector(1, [0, ], [1, ]))\n]).toDF([\"mllib_v\", \"ml_v\"])\ndf.show()\n{s.name: type(s.dataType) for s in df.schema}","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149791_-2032372244","id":"20180213-131625_601576088","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10084"},{"text":"%pyspark\n\n# Enrich each row with the corresponding PLD (using code from Paul J)\n#saveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-aug-sep-oct/domaingraph/vertices/\"\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-18-nov-dec-jan/domaingraph/vertices/\"\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\npld_df.count() # 70M in latest web graph","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149791_-2032372244","id":"20180209-103948_583472396","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10085"},{"text":"%pyspark\n\n# Generate a Bloom Filter of PLDs from the vertex file\nfrom pybloom import BloomFilter\n\n# Attempt to distribute our Bloom Filter so we can build it in parallel, and save it to disk as an RDD\ndef build_partial_bloom(capacity, error_rate):\n    def _build_partial_bloom(record):\n        bloom_filter = BloomFilter(capacity=capacity, error_rate=error_rate)\n        for _,PLD,_ in record: # Just take the PLD field (not the ID or NumHosts)\n            bloom_filter.add(PLD)\n        yield (None, bloom_filter) # returns a Generator (i.e. a kind of Iterator that can only get called once, and doesn't stay in memory)\n    return _build_partial_bloom\n\ndef merge_bloom(bloom_in1, bloom_in2):\n    return bloom_in1.union(bloom_in2) # The reduce function simply becomes a Union\n\n# Our generator function for partial Bloom Filters\ngenerate_bloom = build_partial_bloom(capacity=94000000, error_rate=0.005) # Only 70M in latest web graph?\n\n# Construct the distributed BloomFilter using the PLD dataframe\nbloom_filter_rdd = pld_df.rdd.mapPartitions(generate_bloom).reduceByKey(merge_bloom)\n\n# Save/Load the Bloom Filter RDD - not working\n#saveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-18-nov-dec-jan/domaingraph/vertex_bloom_filter/\"\n#bloom_filter_rdd.saveAsTextFile(saveURI)\n#bloom_filter_rdd=spark.read.load(saveURI)\n\n# Collect and broadcast to cluster nodes\nbloom_filter=bloom_filter_rdd.collect()\nprint(bloom_filter)\npld_bf_distrib = sc.broadcast(bloom_filter[0][1])\n\n# Test contents\nimport sys\nprint(sys.getsizeof(pld_bf_distrib.value))\nprint(len(pld_bf_distrib.value)) # Should match number of items entered\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149792_-2046607954","id":"20180214-130159_1231127388","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10086"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import udf\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n\nudf_convert_hostname = udf(convert_hostname, StringType())\n\n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149792_-2046607954","id":"20180213-133744_1577422019","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10087"},{"text":"%pyspark\n\n# Function to reverse hostnames\nfrom pyspark.sql.functions import udf\ndef reverse_domain(domain):\n    try:\n        ret =  '.'.join(reversed(domain.split('.')))\n        return ret\n    except:\n        return \"NODOMAIN\"\nprint(reverse_domain(\"com.facebook\"))\nudf_reverse_domain = udf(reverse_domain, StringType())\n\n# Convert hosts in Topic DF to PLDs using convert_hostname function from Paul 5.\nscores5=scores4.withColumn(\"pld\",udf_reverse_domain(udf_convert_hostname(udf_reverse_domain(\"host\")))) # Reverse the hostnames prior to lookup, then back again!\nscores5.show(10)","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149792_-2046607954","id":"20180213-133858_1524477020","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10088"},{"text":"%pyspark\n\n# Now we can aggregate page-scores per PLD, using a map-reduce similar to the host aggregation above - first count and sum vectors\nscores6=scores5.rdd.map(lambda x: (x['pld'], (1,x['averageTopicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores7=scores6.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"pld_rev\", \"averageTopicDistribution\"])\nscores7.show(5)\n{s.name: type(s.dataType) for s in scores7.schema}\nprint(scores5.count())\nprint(scores7.count())","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149793_-2046992703","id":"20180213-140037_440180641","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10089"},{"text":"%pyspark\n\n# Save pld topic distributions in parquet format for Tom to play with (and to figure out how to create a PLD topic summary from this).\n# TODO: Maybe a numpy argmax to get the index of the 'top' topic for each PLD with a score.\nscores7.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)","dateUpdated":"2018-02-20T10:22:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149793_-2046992703","id":"20180209-104436_1341725902","dateCreated":"2018-02-20T10:22:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10090"},{"title":"Load saved PLD vectors from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=8192\nscores7 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)\nscores7.show(5)","user":"anonymous","dateUpdated":"2018-02-20T11:38:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122149794_-2045838456","id":"20180213-153111_1438771373","dateCreated":"2018-02-20T10:22:29+0000","dateStarted":"2018-02-20T11:38:24+0000","dateFinished":"2018-02-20T11:38:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10091"},{"text":"%pyspark\n\n# Next run t-SNE embedding on the vectors to reduce from indlen dimensions to 2\nimport numpy as np\nfrom sklearn.manifold import TSNE\n#X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1], [1, 1, 0]])\nX=np.array(scores7.select('averageTopicDistribution').take(10000))\nprint(X)\nnew_X = np.array([i[0] for i in X])\nprint(new_X)\nX_embedded = TSNE(n_components=2).fit_transform(new_X)\nX_embedded.shape","user":"anonymous","dateUpdated":"2018-02-20T11:43:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122245512_-186762419","id":"20180220-102405_700504264","dateCreated":"2018-02-20T10:24:05+0000","dateStarted":"2018-02-20T11:43:43+0000","dateFinished":"2018-02-20T11:49:13+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10092"},{"text":"%pyspark\n\n# Let's try the same thing in UMAP (BEWARE: install umap-learn, not umap!)\n#import numpy as np\nimport umap\n\n#X=np.array(scores7.select('averageTopicDistribution').take(1000))\n#print(X)\n#new_X = np.array([i[0] for i in X])\n#print(new_X)\nU_embedded = umap.UMAP().fit_transform(new_X)\nU_embedded.shape","user":"anonymous","dateUpdated":"2018-02-20T11:47:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519124455980_1521511186","id":"20180220-110055_1082533544","dateCreated":"2018-02-20T11:00:55+0000","dateStarted":"2018-02-20T11:47:28+0000","dateFinished":"2018-02-20T11:49:54+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10093"},{"text":"%pyspark\n\n# Now plot the t-SNE embedding using matplotlib\nimport matplotlib\nimport StringIO\n\n# Turn off interactive mode - we really want this but it leads to dependency errors on AWS linux!\nmatplotlib.use('agg',warn=False, force=True) # Removes Tkinter error (no interactive plots on AWS EMR)\nfrom matplotlib import pyplot as plt\nprint \"Switched to:\",matplotlib.get_backend()\n\nfig, ax = plt.subplots(nrows=1, ncols=2) #, figsize=(40,16))\n#print(X_embedded)\n\n#colors=np.array(pld_codes_with_vec_sc.select('StatusColour').collect())\n#colors2 = np.array([i[0] for i in colors])\n#groups=np.array(pld_codes_with_vec_sc.select('StatusClass').collect())\n#groups2 = np.array([i[0] for i in groups])\n#zip_list = np.array(zip(pld_codes_with_vec_sc.collect(),X_embedded))\n#x_emb = zip_list[:,1]\n#x=[row[0] for row in x_emb]\n#y=[row[1] for row in x_emb]\n\n#ax.scatter(X_embedded[:,0], X_embedded[:,1], s=1.0, c=colors2, edgecolors='face', cmap='rainbow')\n#ax.scatter(x, y, s=1.0, c=colors2, edgecolors='face', cmap='rainbow')\nax[0].set_title(\"t-SNE embedding of PLD topic vectors\")\nax[0].scatter(X_embedded[:,0], X_embedded[:,1], s=1.0)\nax[1].set_title(\"UMAP embedding of PLD topic vectors\")\nax[1].scatter(U_embedded[:,0], U_embedded[:,1], s=1.0)\n\n# plt.show() doesn't seem to work so this is a workaround\ndef show(p):\n    img = StringIO.StringIO()\n    p.savefig(img, format='svg')\n    img.seek(0)\n    print \"%html <div style='width:600px'>\" + img.buf + \"</div>\"\n\n#plt.legend(loc=2)\nshow(plt)","user":"anonymous","dateUpdated":"2018-02-20T11:53:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519123501262_657553980","id":"20180220-104501_1741100148","dateCreated":"2018-02-20T10:45:01+0000","dateStarted":"2018-02-20T11:53:05+0000","dateFinished":"2018-02-20T11:53:10+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10094"},{"text":"%pyspark\n\n\n","user":"anonymous","dateUpdated":"2018-02-20T11:02:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519122496493_-1252392111","id":"20180220-102816_491346592","dateCreated":"2018-02-20T10:28:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10095"}],"name":"Tom 2 Wiki Topic Scoring","id":"2D71MA1Z3","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}