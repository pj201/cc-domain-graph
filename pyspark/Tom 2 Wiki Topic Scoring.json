{"paragraphs":[{"text":"%pyspark\n\n# PySpark CommonCrawl Topic Modelling\n# Tom V / Paul J - 9/2/2018\n\n# SET THE spark.driver.maxResultSize PROPERTY TO 3G\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\nimport pycld2 as cld2\n\n#wetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2017-04/wet.paths.gz\") # April(?) 2017 Crawl - 57000 files\n# Latest blog/documentation: http://commoncrawl.org/2017/10/october-2017-crawl-archive-now-available/\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2017-43/wet.paths.gz\") # October 2017 Crawl - 89100 files (10.58TB)\n\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x)[0].lang # Maybe we can get away with looking at less characters, or do something less expensive?\n    except Exception as e:\n        return None\n        \ndef detect2(x):\n    try:\n        isReliable, textBytesFound, details = cld2.detect(x)\n        return details[0][1]\n    except Exception as e:\n        print(e)\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file: # Approx 53k web pages per WET file\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                \n                # TODO: Limit number of bytes read per record e.g. read(200000)\n                \n                domain = None if not url else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8') #.limit(100) # TODO: Limit this read to ensure max length (for improving parallizability)\n                lang = detect2(text[:300]) # Use PyCLD2, not langdetect, which was killing performance!\n                yield domain, url, text, lang\n            except Exception as e:\n                yield e\n                \ndef process_wet_simple(id_, iterator):\n    count=0\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                count=count+1\n                # TODO: Output total size of pages, rather than number of pages\n                # Histogram.\n            except Exception as e:\n                pass\n        #print(count)\n        yield count","user":"anonymous","dateUpdated":"2018-02-09T10:25:24+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518171276528_-1712470079","id":"20171020-102243_1718178582","dateCreated":"2018-02-09T10:14:36+0000","dateStarted":"2018-02-09T10:25:24+0000","dateFinished":"2018-02-09T10:25:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:296"},{"text":"%pyspark\ndetect2(\"this is a test\")","user":"anonymous","dateUpdated":"2018-02-09T10:25:29+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"'en'\n"}]},"apps":[],"jobName":"paragraph_1518171276528_-1712470079","id":"20171027-134322_549744379","dateCreated":"2018-02-09T10:14:36+0000","dateStarted":"2018-02-09T10:25:29+0000","dateFinished":"2018-02-09T10:25:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"%pyspark\n\n# PARAMETER - number of input files\nnfiles = 1 # Total 89100\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles)) #, numSlices=nfiles/32) # TODO: Try numSlices=nfiles/32 etc, or just default!\n\n# Should parallelize\nprint(files.getNumPartitions())\nrdd=files.mapPartitionsWithIndex(process_wet)\n\nprint(str(rdd))\ndocs = rdd.toDF([\"host\", \"url\", \"text\",\"lang\"]) #  \"lang\"\n#docs.cache()\n#docs.count() # Total docs in all languages","user":"anonymous","dateUpdated":"2018-02-09T10:52:24+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"320\nPythonRDD[102] at RDD at PythonRDD.scala:48\n"}]},"apps":[],"jobName":"paragraph_1518171276528_-1712470079","id":"20171020-101220_2022546189","dateCreated":"2018-02-09T10:14:36+0000","dateStarted":"2018-02-09T10:52:24+0000","dateFinished":"2018-02-09T10:52:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"%pyspark\n\n# Filter for English only\ndocs_en = docs.filter(docs.lang == 'en')","user":"anonymous","dateUpdated":"2018-02-09T10:53:14+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518171276528_-1712470079","id":"20171020-110218_1507019685","dateCreated":"2018-02-09T10:14:36+0000","dateStarted":"2018-02-09T10:53:14+0000","dateFinished":"2018-02-09T10:53:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:299"},{"title":"Load saved vectors from Wikipedia model (created by python Wikipedia Text Processing.ipynb)","text":"%pyspark\nfrom pyspark.ml import Pipeline,PipelineModel\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\nfrom pyspark.ml.clustering import LocalLDAModel\n\ntextModel = PipelineModel.load('s3://billsdata.net/CommonCrawl/wikipedia/text_model')\nldaModel = LocalLDAModel.load('s3://billsdata.net/CommonCrawl/wikipedia/lda_model')","user":"anonymous","dateUpdated":"2018-02-09T10:37:40+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518171276529_-1712854828","id":"20171027-152148_622560673","dateCreated":"2018-02-09T10:14:36+0000","dateStarted":"2018-02-09T10:22:17+0000","dateFinished":"2018-02-09T10:22:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:300"},{"text":"%pyspark\n\n# Test the models - for debugging only\nimport numpy as np\nimport pandas as pd\n\nX=ldaModel.topicsMatrix().toArray()\nvocab = np.array(textModel.stages[2].vocabulary)\n\ntopicLabels = [' '.join(vocab[np.argsort(X[:,i])[::-1][:5]]) for i in range(100)]\n\ndef score_topics(text):\n    df = sqlContext.createDataFrame(pd.DataFrame({'text':[text]}))\n    vec = textModel.transform(df)\n    scores = ldaModel.transform(vec).select('topicDistribution').collect()[0].topicDistribution.toArray()\n    return pd.Series(dict(zip(topicLabels, scores)))\n    \n# Try it on an arbitary sentence\nprint(score_topics(\"This is the latest news about North Korea and their involvement in the Winter Olympics in PyongChang\"))","user":"anonymous","dateUpdated":"2018-02-09T10:53:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"africa south african singh harris                    0.001191\nage population census county average                 0.001250\nair flight aircraft airlines airline                 0.001200\naircraft air wing flying first                       0.001206\nairport florida miami international south            0.001217\nal virginia ohio states tennessee                    0.001265\nalbum band released song music                       0.001321\nalso cells cell protein used                         0.001235\narea city town region located                        0.001317\narmy th war military force                           0.001256\nart museum work artist painting                      0.001265\naustralia australian sydney navy south               0.001248\nbishop catholic st italian church                    0.001255\nblue wales white welsh black                         0.001216\nbr plant oil flowers leaves                          0.001264\nbuilding house built historic buildings              0.001295\nc p r g b                                            0.001337\ncanada canadian member assembly ontario              0.001286\ncar race racing class cars                           0.001215\ncentury one book also first                          0.001268\ncentury th castle polish ii                          0.001251\nchurch christian god religious day                   0.001263\ncity county street center community                  0.001298\nclub league football season played                   0.001293\nclub season signed born league                       0.001201\ncode used system number memory                       0.001194\ncollege university school born served                0.001323\ncolorado iowa arizona arkansas utah                  0.001211\ncommune serbian hungarian serbia hungary             0.001254\ncompany business bank million group                  0.001286\n                                                       ...   \nnational united organization states international    0.001272\nnew newspaper editor york norwegian                  0.001259\nnew york city ny jersey                              0.001237\nnuclear kelly allen energy burns                     0.001181\none ball game player players                         0.001210\none time man first also                              0.001295\nparty election elected elections democratic          0.001271\nradio station television channel tv                  0.447599\nriver lake park mountain creek                       0.001316\nroad parish village england centre                   0.001257\nrussian russia moscow soviet alexander               0.001225\nsan el francisco puerto philippines                  0.001245\nschool high schools girls boys                       0.001261\nschool students education university college         0.001276\nseason team first teams cup                          0.001276\nseries book published books novel                    0.001261\nseries show television also episode                  0.001292\nship ships two navy war                              0.001220\nsocial one may also people                           0.001240\nspace earth light solar star                         0.001223\nspecies found also large may                         0.001261\nstation line railway service train                   0.001261\nteam season coach football first                     0.001253\ntom oliver ghost haiti kay                           0.001183\nukrainian ukraine dog dogs stamps                    0.001198\nuniversity research professor published science      0.001323\nwar union soviet communist political                 0.001213\nwater company construction new coal                  0.001240\nworld olympics championships summer women            0.430010\nzealand new grand auckland prix                      0.001216\nLength: 100, dtype: float64\n"}]},"apps":[],"jobName":"paragraph_1518171717009_-1410834043","id":"20180209-102157_2110186523","dateCreated":"2018-02-09T10:21:57+0000","dateStarted":"2018-02-09T10:53:30+0000","dateFinished":"2018-02-09T10:53:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:301"},{"text":"%pyspark\n\n# Now score pages from our WET files\ndocs_en.show(5)\nvec = textModel.transform(docs_en)\nvec.show(5)\n","user":"anonymous","dateUpdated":"2018-02-09T10:54:21+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+----+\n|                host|                 url|                text|lang|\n+--------------------+--------------------+--------------------+----+\n|                null|                null|Software-Info: ia...|  en|\n|1000daysofwriting...|http://1000daysof...|1000 Days of Writ...|  en|\n|100unhappydays.bl...|http://100unhappy...|100 Unhappy Days:...|  en|\n|          10in30.com|http://10in30.com...|LearnOutLoud_300x...|  en|\n|123-free-download...|http://123-free-d...|MusicBoxTool - [3...|  en|\n+--------------------+--------------------+--------------------+----+\nonly showing top 5 rows\n\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n|                host|                 url|                text|lang|               words|            filtered|                 vec|\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n|                null|                null|Software-Info: ia...|  en|[software, info, ...|[software, info, ...|(20000,[88,152,33...|\n|1000daysofwriting...|http://1000daysof...|1000 Days of Writ...|  en|[days, of, writin...|[days, writing, d...|(20000,[0,2,3,5,7...|\n|100unhappydays.bl...|http://100unhappy...|100 Unhappy Days:...|  en|[unhappy, days, d...|[unhappy, days, d...|(20000,[26,48,61,...|\n|          10in30.com|http://10in30.com...|LearnOutLoud_300x...|  en|[learnoutloud, x,...|[learnoutloud, x,...|(20000,[3,22,39,4...|\n|123-free-download...|http://123-free-d...|MusicBoxTool - [3...|  en|[musicboxtool, ho...|[musicboxtool, ho...|(20000,[3,5,22,59...|\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518171276530_-1711700581","id":"20171020-101912_1610139389","dateCreated":"2018-02-09T10:14:36+0000","dateStarted":"2018-02-09T10:54:21+0000","dateFinished":"2018-02-09T10:54:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:302"},{"text":"%pyspark\n\n# Create topic distribution vectors and tidy upp\nscores = ldaModel.transform(vec)\nscores2 = scores.drop(\"url\").drop(\"text\").drop(\"lang\").drop(\"words\").drop(\"filtered\").drop(\"vec\")\nscores2.show(5)","user":"anonymous","dateUpdated":"2018-02-09T10:54:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|                host|   topicDistribution|\n+--------------------+--------------------+\n|                null|[0.13351995547840...|\n|1000daysofwriting...|[3.26238961502545...|\n|100unhappydays.bl...|[5.81230792144732...|\n|          10in30.com|[8.42785330446217...|\n|123-free-download...|[6.44166735802645...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518172144244_2096179541","id":"20180209-102904_2097212017","dateCreated":"2018-02-09T10:29:04+0000","dateStarted":"2018-02-09T10:54:56+0000","dateFinished":"2018-02-09T10:54:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:303"},{"text":"%pyspark\n\n# Save these vectors to disc, so we can just load them later\nscores2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)","user":"anonymous","dateUpdated":"2018-02-09T10:56:53+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518172207118_-1307431823","id":"20180209-103007_1930247145","dateCreated":"2018-02-09T10:30:07+0000","dateStarted":"2018-02-09T10:35:29+0000","dateFinished":"2018-02-09T10:39:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:304"},{"title":"Load saved scores from nfiles of WET files","text":"%pyspark\n\n# Restart here\nscores2 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)\nscores2.show(5)","user":"anonymous","dateUpdated":"2018-02-09T10:55:37+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518172529531_-1557824568","id":"20180209-103529_782306138","dateCreated":"2018-02-09T10:35:29+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:305"},{"text":"%pyspark\n\n# Aggregate page-scores per Host for now (will be same process for aggregating host-scores to PLD-scores) - first count and sum vectors\nscores3=scores2.rdd.map(lambda x: (x['host'], (1,x['topicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and TODO: convert back to a dataframe\nvec_schema=StructType([StructField('host', StringType(), False), StructField('averageTopicDistribution', ArrayType(FloatType()), False)])\nscores4=scores3.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF(vec_schema) # Failed attempt to convert DenseVector to Tuple before toDF\nscores4.show(5)","user":"anonymous","dateUpdated":"2018-02-09T12:08:26+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7074246543696920832.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7074246543696920832.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 4, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 336, in show\n    print(self._jdf.showString(n, 20))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1416.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 106.0 failed 4 times, most recent failure: Lost task 0.3 in stage 106.0 (TID 8033, ip-172-31-46-32.eu-west-1.compute.internal, executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 567, in prepare\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/sql/types.py\", line 1365, in _verify_type\n    _verify_type(v, f.dataType, f.nullable)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/sql/types.py\", line 1329, in _verify_type\n    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\nTypeError: ArrayType(FloatType,true) can not accept object DenseVector([0.0, 0.0, 0.0139, 0.0, 0.0, 0.0, 0.006, 0.0024, 0.0062, 0.0, 0.0, 0.0339, 0.0, 0.0, 0.007, 0.1024, 0.0039, 0.0, 0.0, 0.0, 0.0994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0156, 0.0218, 0.0, 0.0856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0104, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3203, 0.0456, 0.0178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0367, 0.0, 0.039, 0.0156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0254, 0.0, 0.0, 0.0, 0.0659, 0.0, 0.0, 0.0165, 0.0, 0.0]) in type <class 'pyspark.ml.linalg.DenseVector'>\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2861)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 567, in prepare\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/sql/types.py\", line 1365, in _verify_type\n    _verify_type(v, f.dataType, f.nullable)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518167276650_0001/container_1518167276650_0001_01_000011/pyspark.zip/pyspark/sql/types.py\", line 1329, in _verify_type\n    raise TypeError(\"%s can not accept object %r in type %s\" % (dataType, obj, type(obj)))\nTypeError: ArrayType(FloatType,true) can not accept object DenseVector([0.0, 0.0, 0.0139, 0.0, 0.0, 0.0, 0.006, 0.0024, 0.0062, 0.0, 0.0, 0.0339, 0.0, 0.0, 0.007, 0.1024, 0.0039, 0.0, 0.0, 0.0, 0.0994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0156, 0.0218, 0.0, 0.0856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0104, 0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3203, 0.0456, 0.0178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0367, 0.0, 0.039, 0.0156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0254, 0.0, 0.0, 0.0, 0.0659, 0.0, 0.0, 0.0165, 0.0, 0.0]) in type <class 'pyspark.ml.linalg.DenseVector'>\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}]},"apps":[],"jobName":"paragraph_1518173231975_290873639","id":"20180209-104711_1636843522","dateCreated":"2018-02-09T10:47:11+0000","dateStarted":"2018-02-09T12:01:28+0000","dateFinished":"2018-02-09T12:04:29+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:306"},{"text":"%pyspark\n\n# TODO: Enrich each row with the corresponding PLD (using code fropm Paul J, but pickle the Bloom filter and just load in!!!)","user":"anonymous","dateUpdated":"2018-02-09T11:36:10+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7074246543696920832.py\", line 349, in <module>\n    [code.body[-(nhooks + 1)]])\nIndexError: list index out of range\n"}]},"apps":[],"jobName":"paragraph_1518172788220_944934734","id":"20180209-103948_583472396","dateCreated":"2018-02-09T10:39:48+0000","dateStarted":"2018-02-09T11:36:10+0000","dateFinished":"2018-02-09T11:38:53+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"text":"%pyspark\n\n# TODO: Save pld topic distributions in parquet format for Tom to play with (and to figure out how to create a PLD topic summary from this).\n# Maybe a numpy argmax to get the index of the 'top' topic for each PLD with a score.","user":"anonymous","dateUpdated":"2018-02-09T11:12:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518173076518_-1333549897","id":"20180209-104436_1341725902","dateCreated":"2018-02-09T10:44:36+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:308"}],"name":"Tom 2 Wiki Topic Scoring","id":"2D5QYNYNB","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}