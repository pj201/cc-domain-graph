{"paragraphs":[{"text":"%pyspark\n\n# PySpark CommonCrawl Topic Modelling\n# Tom V / Paul J - 2/3/2018\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\nimport pycld2 as cld2\n\n# Latest blog/documentation: http://commoncrawl.org/2018/01/january-2018-crawl-archive-now-available/\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2018-05/wet.paths.gz\") # Jan 2018 Crawl - 80000 files (9.29TB)\n\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x)[0].lang # Maybe we can get away with looking at less characters, or do something less expensive?\n    except Exception as e:\n        return None\n        \ndef detect2(x):\n    try:\n        isReliable, textBytesFound, details = cld2.detect(x)\n        return details[0][1]\n    except Exception as e:\n        print(e)\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file: # Approx 53k web pages per WET file\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                \n                # TODO: Limit number of bytes read per record e.g. read(200000)\n                \n                domain = None if not url else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8') #.limit(100) # TODO: Limit this read to ensure max length (for improving parallizability)\n                lang = detect2(text[:300]) # Use PyCLD2, not langdetect, which was killing performance!\n                yield domain, url, text, lang\n            except Exception as e:\n                yield e\n                \ndef process_wet_simple(id_, iterator):\n    count=0\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                count=count+1\n                # TODO: Output total size of pages, rather than number of pages\n                # Histogram.\n            except Exception as e:\n                pass\n        #print(count)\n        yield count","user":"anonymous","dateUpdated":"2018-03-02T14:09:53+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1519983091963_1179843897","id":"20171020-102243_1718178582","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T09:31:39+0000","dateFinished":"2018-03-02T09:32:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:32170"},{"text":"%pyspark\ndetect2(\"this is a test\")","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091963_1179843897","id":"20171027-134322_549744379","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32171"},{"text":"%pyspark\n\n# PARAMETER - number of input files\nnfiles = 8192 # Total 80000\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles)) #, numSlices=nfiles/32) # TODO: Try numSlices=nfiles/32 etc, or just default!\n\n# Should parallelize\nprint(files.getNumPartitions())\nrdd=files.mapPartitionsWithIndex(process_wet)\n\nprint(str(rdd))\ndocs = rdd.toDF([\"host\", \"url\", \"text\",\"lang\"]) #  \"lang\"\n#docs.cache()\n#docs.count() # Total docs in all languages","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20171020-101220_2022546189","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32172"},{"text":"%pyspark\n\n# Filter for English only\ndocs_en = docs.filter(docs.lang == 'en')","dateUpdated":"2018-03-02T09:31:31+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20171020-110218_1507019685","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32173"},{"title":"Load saved vectors from Wikipedia model (created by python Wikipedia Text Processing.ipynb)","text":"%pyspark\nfrom pyspark.ml import Pipeline,PipelineModel\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\nfrom pyspark.ml.clustering import LocalLDAModel\n\ntextModel = PipelineModel.load('s3://billsdata.net/CommonCrawl/wikipedia/text_model')\nldaModel = LocalLDAModel.load('s3://billsdata.net/CommonCrawl/wikipedia/lda_model')","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20171027-152148_622560673","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32174"},{"text":"%pyspark\n\n# Test the models - for debugging only\nimport numpy as np\nimport pandas as pd\n\nX=ldaModel.topicsMatrix().toArray()\nvocab = np.array(textModel.stages[2].vocabulary)\n\ntopicLabels = [' '.join(vocab[np.argsort(X[:,i])[::-1][:5]]) for i in range(100)]\n\ndef score_topics(text):\n    df = sqlContext.createDataFrame(pd.DataFrame({'text':[text]}))\n    vec = textModel.transform(df)\n    scores = ldaModel.transform(vec).select('topicDistribution').collect()[0].topicDistribution.toArray()\n    return pd.Series(dict(zip(topicLabels, scores)))\n    \n# Try it on an arbitary sentence\nprint(score_topics(\"This is the latest news about North Korea and their involvement in the Winter Olympics in PyongChang\"))","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091964_1177920152","id":"20180209-102157_2110186523","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32175"},{"text":"%pyspark\n\n# Now score pages from our WET files\ndocs_en.show(5)\nvec = textModel.transform(docs_en)\nvec.show(5)\n","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091965_1177535403","id":"20171020-101912_1610139389","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32176"},{"text":"%pyspark\n\n# Create topic distribution vectors and tidy upp\nscores = ldaModel.transform(vec)\nscores2 = scores.drop(\"url\").drop(\"text\").drop(\"lang\").drop(\"words\").drop(\"filtered\").drop(\"vec\")\nscores2.show(5)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091965_1177535403","id":"20180209-102904_2097212017","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32177"},{"text":"%pyspark\n\n# Save these vectors to disc, so we can just load them later\nscores2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091965_1177535403","id":"20180209-103007_1930247145","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32178"},{"title":"Load saved scores from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=8192\nscores2 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)\nscores2.show(5)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180209-103529_782306138","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32179"},{"text":"%pyspark\n\n# Aggregate page-scores per Host for now (will be same process for aggregating host-scores to PLD-scores) - first count and sum vectors\nscores3=scores2.rdd.map(lambda x: (x['host'], (1,x['topicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores4=scores3.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"host\", \"averageTopicDistribution\"])\nscores4.show(5)\n{s.name: type(s.dataType) for s in scores4.schema}\nprint(scores2.count())\nprint(scores4.count())","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180209-104711_1636843522","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32180"},{"text":"%pyspark\n\n# Just playing - code to help understand the different libraries and vector types!\nimport pyspark.mllib.linalg as mllib\nimport pyspark.ml.linalg as ml\ndf = sc.parallelize([\n    (mllib.DenseVector([1, ]), ml.DenseVector([1, ])),\n    (mllib.SparseVector(1, [0, ], [1, ]), ml.SparseVector(1, [0, ], [1, ]))\n]).toDF([\"mllib_v\", \"ml_v\"])\ndf.show()\n{s.name: type(s.dataType) for s in df.schema}","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180213-131625_601576088","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32181"},{"text":"%pyspark\n\n# Enrich each row with the corresponding PLD (using code from Paul J)\n#saveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-aug-sep-oct/domaingraph/vertices/\"\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-18-nov-dec-jan/domaingraph/vertices/\"\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\npld_df.count() # 70M in latest web graph","user":"anonymous","dateUpdated":"2018-03-02T10:24:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+--------+\n| ID|    PLD|NumHosts|\n+---+-------+--------+\n|  0|  aaa.a|       1|\n|  1| aaa.aa|       1|\n|  2|aaa.aaa|       4|\n+---+-------+--------+\nonly showing top 3 rows\n\n70367284\n"}]},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180209-103948_583472396","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T10:24:02+0000","dateFinished":"2018-03-02T10:25:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32182"},{"text":"%pyspark\n\n# Generate a Bloom Filter of PLDs from the vertex file\nfrom pybloom import BloomFilter\n\n# Attempt to distribute our Bloom Filter so we can build it in parallel, and save it to disk as an RDD\ndef build_partial_bloom(capacity, error_rate):\n    def _build_partial_bloom(record):\n        bloom_filter = BloomFilter(capacity=capacity, error_rate=error_rate)\n        for _,PLD,_ in record: # Just take the PLD field (not the ID or NumHosts)\n            bloom_filter.add(PLD)\n        yield (None, bloom_filter) # returns a Generator (i.e. a kind of Iterator that can only get called once, and doesn't stay in memory)\n    return _build_partial_bloom\n\ndef merge_bloom(bloom_in1, bloom_in2):\n    return bloom_in1.union(bloom_in2) # The reduce function simply becomes a Union\n\n# Our generator function for partial Bloom Filters\ngenerate_bloom = build_partial_bloom(capacity=94000000, error_rate=0.005) # Only 70M in latest web graph?\n\n# Construct the distributed BloomFilter using the PLD dataframe\nbloom_filter_rdd = pld_df.rdd.mapPartitions(generate_bloom).reduceByKey(merge_bloom)\n\n# Save/Load the Bloom Filter RDD - not working\n#saveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-18-nov-dec-jan/domaingraph/vertex_bloom_filter/\"\n#bloom_filter_rdd.saveAsTextFile(saveURI)\n#bloom_filter_rdd=spark.read.load(saveURI)\n\n# Collect and broadcast to cluster nodes\nbloom_filter=bloom_filter_rdd.collect()\nprint(bloom_filter)\npld_bf_distrib = sc.broadcast(bloom_filter[0][1])\n\n# Test contents\nimport sys\nprint(sys.getsizeof(pld_bf_distrib.value))\nprint(len(pld_bf_distrib.value)) # Should match number of items entered\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091966_1178689650","id":"20180214-130159_1231127388","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32183"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import udf\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n\nudf_convert_hostname = udf(convert_hostname, StringType())\n\n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-133744_1577422019","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32184"},{"text":"%pyspark\n\n# Function to reverse hostnames\nfrom pyspark.sql.functions import udf\ndef reverse_domain(domain):\n    try:\n        ret =  '.'.join(reversed(domain.split('.')))\n        return ret\n    except:\n        return \"NODOMAIN\"\nprint(reverse_domain(\"com.facebook\"))\nudf_reverse_domain = udf(reverse_domain, StringType())\n\n# Convert hosts in Topic DF to PLDs using convert_hostname function from Paul 5.\nscores5=scores4.withColumn(\"pld\",udf_reverse_domain(udf_convert_hostname(udf_reverse_domain(\"host\")))) # Reverse the hostnames prior to lookup, then back again!\nscores5.show(10)","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-133858_1524477020","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32185"},{"text":"%pyspark\n\n# Now we can aggregate page-scores per PLD, using a map-reduce similar to the host aggregation above - first count and sum vectors\nscores6=scores5.rdd.map(lambda x: (x['pld'], (1,x['averageTopicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores7=scores6.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"pld_rev\", \"averageTopicDistribution\"])\nscores7.show(5)\n{s.name: type(s.dataType) for s in scores7.schema}\nprint(scores5.count())\nprint(scores7.count())","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-140037_440180641","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32186"},{"text":"%pyspark\n\n# Save pld topic distributions in parquet format for Tom to play with (and to figure out how to create a PLD topic summary from this).\nscores7.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)","dateUpdated":"2018-03-02T14:10:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180209-104436_1341725902","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32187"},{"title":"Load saved PLD vectors from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=8192\nscores7 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)\nscores7.show(5)","user":"anonymous","dateUpdated":"2018-03-02T09:32:02+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+------------------------+\n|             pld_rev|averageTopicDistribution|\n+--------------------+------------------------+\n|          scrtec.org|    [0.00101137139816...|\n|corporatefitnessc...|    [1.16246367421139...|\n|        1centweb.com|    [1.94489585421988...|\n|   mygaragestory.net|    [2.04009774069444...|\n|     gitedeville.com|    [1.18827049530001...|\n+--------------------+------------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1519983091967_1178304901","id":"20180213-153111_1438771373","dateCreated":"2018-03-02T09:31:31+0000","dateStarted":"2018-03-02T09:32:02+0000","dateFinished":"2018-03-02T09:32:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32188"},{"text":"%pyspark\n\n# Take a sample of PLDs for plotting in t-SNE and UMAP, and convert to Pandas dataframe\nscores8=scores7.limit(10000)\ndf_sample = scores8.toPandas()\n\n# t-SNE embedding\nfrom sklearn.manifold import TSNE\nimport numpy as np\nX=np.vstack(df_sample.averageTopicDistribution)\nX_embedded = TSNE(n_components=2).fit_transform(X)\nprint(X_embedded.shape)\ndf_sample['x_tsne'] = X_embedded[:,0]\ndf_sample['y_tsne'] = X_embedded[:,1]\n\n# UMAP embedding\nimport umap\nU_embedded = umap.UMAP().fit_transform(X)\ndf_sample['x_umap'] = U_embedded[:,0]\ndf_sample['y_umap'] = U_embedded[:,1]\n\n# Extract primary topic, for use in color scheme\ndf_sample['primary_topic'] = df_sample.averageTopicDistribution.apply(np.argmax)\ndf_sample['primary_topic_score'] = df_sample.averageTopicDistribution.apply(np.max)\n\n# Convert back to Spark dataframe\ndf_sample_spark = sqlContext.createDataFrame(df_sample)\ndf_sample_spark.show()","user":"anonymous","dateUpdated":"2018-03-02T14:12:27+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(10000, 2)\n+--------------------+------------------------+--------------------+--------------------+-------------------+--------------------+-------------+-------------------+\n|             pld_rev|averageTopicDistribution|              x_tsne|              y_tsne|             x_umap|              y_umap|primary_topic|primary_topic_score|\n+--------------------+------------------------+--------------------+--------------------+-------------------+--------------------+-------------+-------------------+\n|          scrtec.org|    [0.00101137139816...|  -63.40200424194336|   44.35719680786133|  -9.20855769204408|   5.353137211795069|           29| 0.9010357886880672|\n|corporatefitnessc...|    [1.16246367421139...|  50.717445373535156|   25.67108154296875| 2.3035335272625375|  2.9489529987426724|           62|0.23111564571532547|\n|        1centweb.com|    [1.94489585421988...|  -57.56715393066406|   25.92642593383789| -5.026943730782058|  2.6341051583784534|           29| 0.7100540119133284|\n|   mygaragestory.net|    [2.04009774069444...|  -51.19037628173828|  0.0740693137049675| -3.402280101604503| -0.4820181151768883|           29|0.26711331698144136|\n|     gitedeville.com|    [1.18827049530001...|  -21.88416862487793| -25.539623260498047|  4.569428845131571|  0.6412596295209377|           46|0.26875609876099643|\n|     annadesogus.com|    [0.01558264822551...|  15.936853408813477|  21.140478134155273|  1.052034302953693| -0.3460609618451271|           29|0.23605239678942952|\n|michael-barrymore...|    [1.00132680529176...|  -17.30196762084961|  -7.373234748840332| -0.432409381629909| -0.0637145346748731|           47|0.31757600891167936|\n| prostarcleaning.com|    [3.77903452976982...| -26.730302810668945| -23.947370529174805| -1.352855649440794|-0.42328961848371544|           41|0.21829410518358922|\n|        bzupages.com|    [1.08151075067157...|  -5.620166301727295|   62.51683044433594|  5.644085827380796|   3.387334899818211|           29| 0.3132924891829208|\n|         denhall.com|    [5.22141585851430...|  -74.50524139404297|  19.671321868896484| -9.048242435888595|  3.4083277862104984|           29| 0.6540574500402434|\n|         ohsfsca.org|    [5.32292848141376...|  52.381595611572266|   7.490338325500488| 3.6725490228377984|  2.4766559059782822|           24| 0.5892815449355147|\n|tridentsportsmgmt...|    [0.00126422413498...|  6.9799885749816895|   45.90101623535156|-0.9666576849196205|  3.7762741399253006|           21| 0.4035271171948168|\n|  hundred-waters.com|    [6.32099708537558...|  -7.125980854034424|  -76.57098388671875| 1.3223285983515394|  -5.290518372692389|            2|0.38072899559808615|\n|    phewinternet.com|    [6.57522654173137...|0.006655278615653515|   -66.3670654296875| 0.8588991408253117| -3.6393276432420514|           79| 0.4381586501630016|\n|  totalappliance.net|    [3.09278509969920...| -30.867233276367188| -11.739279747009277|-1.3119287773615806|   1.377172560989641|           29|0.30169907073285956|\n|        architek.com|    [0.06522385702128...| -33.714576721191406|  -7.778219223022461|-1.9509456720922291|  0.7023720060597143|           29|0.44527668916484775|\n|evanselectricnort...|    [1.14925381277316...| -30.035940170288086|    14.3041410446167|-3.6889912618887184|    1.79165150469912|           29| 0.5633173027256081|\n|qualitycarsolutio...|    [3.20044544708958...|    -72.121337890625|-0.16737070679664612|   -4.6040031183407| -1.4611547304550572|           27|0.48164928778376015|\n|goodsamaritanlodg...|    [9.54096929003881...|   32.42762756347656| -23.465328216552734| 2.1780978957448047|-0.01775710220673...|           45| 0.2648492925915327|\n|noosaorchidsociet...|    [0.19182804449522...| 0.45397379994392395| -18.859155654907227| 0.3068594667715606| -1.3231427702834035|            0| 0.1918280444952256|\n+--------------------+------------------------+--------------------+--------------------+-------------------+--------------------+-------------+-------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1519989262570_1506522475","id":"20180302-111422_237843765","dateCreated":"2018-03-02T11:14:22+0000","dateStarted":"2018-03-02T11:43:57+0000","dateFinished":"2018-03-02T11:49:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32189"},{"text":"%pyspark\n\n# Take reversed PLD and switch back to the right way round\nfrom pyspark.sql.functions import udf, col, when, lit, concat_ws, split\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook.abc\"))\n\n# Tidy up columns\nudf_reverse_domain = udf(reverse_domain, StringType())\nscores9=df_sample_spark.withColumn(\"actual_pld_rev\",udf_reverse_domain(\"pld_rev\"))\nscores10=scores9.join(pld_df, pld_df.PLD==scores9.actual_pld_rev).withColumnRenamed(\"pld_rev\",\"payLevelDomain\").select(\"payLevelDomain\",\"NumHosts\",\"x_tsne\",\"y_tsne\",\"x_umap\",\"y_umap\",\"primary_topic\",\"primary_topic_score\")\nscores10.show(3)","user":"anonymous","dateUpdated":"2018-03-02T14:13:18+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"abc.facebook.com\n+--------------------+--------+-----------------+-------------------+-------------------+--------------------+-------------+-------------------+\n|      payLevelDomain|NumHosts|           x_tsne|             y_tsne|             x_umap|              y_umap|primary_topic|primary_topic_score|\n+--------------------+--------+-----------------+-------------------+-------------------+--------------------+-------------+-------------------+\n|            2bap.com|       1|-1.01448392868042|-3.0176773071289062| 0.4180089070827317|  0.2568085759153541|           29| 0.2239727598434192|\n|addictionrehabaus...|       1| 69.7011947631836| 10.383376121520996| -6.795308385975276|0.017603301001099064|           18| 0.4195210474391418|\n|            ccfo.com|       1|7.305209159851074| 39.560585021972656|-0.8117916602499007|    3.05639909875979|           21| 0.3325174922165379|\n+--------------------+--------+-----------------+-------------------+-------------------+--------------------+-------------+-------------------+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1519983779708_591942120","id":"20180302-094259_407353261","dateCreated":"2018-03-02T09:42:59+0000","dateStarted":"2018-03-02T11:51:33+0000","dateFinished":"2018-03-02T11:51:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32193"},{"text":"%pyspark\n\n# Export as CSV\ntopicEmbeddingsURI=\"s3://billsdata.net/CommonCrawl/domain_embeddings_topic_10000_2017-18-nov-dec-jan/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\nscores10.coalesce(1).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(topicEmbeddingsURI)","user":"anonymous","dateUpdated":"2018-03-02T11:52:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1519983815830_-340476320","id":"20180302-094335_509647219","dateCreated":"2018-03-02T09:43:35+0000","dateStarted":"2018-03-02T11:52:41+0000","dateFinished":"2018-03-02T11:54:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32194"},{"text":"%pyspark\n\n# Plot the t-SNE embedding using matplotlib in Zeppelin notebook\nimport matplotlib\nimport StringIO\n\n# Turn off interactive mode - we really want this but it leads to dependency errors on AWS linux!\nmatplotlib.use('agg',warn=False, force=True) # Removes Tkinter error (no interactive plots on AWS EMR)\nfrom matplotlib import pyplot as plt\nprint \"Switched to:\",matplotlib.get_backend()\n\nfig, ax = plt.subplots(nrows=1, ncols=2) #, figsize=(40,16))\n#print(X_embedded)\n\n#colors=np.array(pld_codes_with_vec_sc.select('StatusColour').collect())\n#colors2 = np.array([i[0] for i in colors])\n#groups=np.array(pld_codes_with_vec_sc.select('StatusClass').collect())\n#groups2 = np.array([i[0] for i in groups])\n#zip_list = np.array(zip(pld_codes_with_vec_sc.collect(),X_embedded))\n#x_emb = zip_list[:,1]\n#x=[row[0] for row in x_emb]\n#y=[row[1] for row in x_emb]\n\n#ax.scatter(X_embedded[:,0], X_embedded[:,1], s=1.0, c=colors2, edgecolors='face', cmap='rainbow')\n#ax.scatter(x, y, s=1.0, c=colors2, edgecolors='face', cmap='rainbow')\nax[0].set_title(\"t-SNE embedding of PLD topic vectors\")\nax[0].scatter(X_embedded[:,0], X_embedded[:,1], s=1.0)\nax[1].set_title(\"UMAP embedding of PLD topic vectors\")\nax[1].scatter(U_embedded[:,0], U_embedded[:,1], s=1.0)\n\n# plt.show() doesn't seem to work so this is a workaround\ndef show(p):\n    img = StringIO.StringIO()\n    p.savefig(img, format='svg')\n    img.seek(0)\n    print \"%html <div style='width:600px'>\" + img.buf + \"</div>\"\n\n#plt.legend(loc=2)\nshow(plt)","dateUpdated":"2018-03-02T09:42:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091968_1262564910","id":"20180220-104501_1741100148","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32195"},{"text":"%pyspark\n\n\n","dateUpdated":"2018-03-02T09:31:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519983091968_1262564910","id":"20180220-102816_491346592","dateCreated":"2018-03-02T09:31:31+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32196"}],"name":"Tom 2 Wiki Topic Scoring","id":"2D8RF8QCE","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}