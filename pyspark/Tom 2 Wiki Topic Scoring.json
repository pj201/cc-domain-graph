{"paragraphs":[{"text":"%pyspark\n\n# PySpark CommonCrawl Topic Modelling\n# Tom V / Paul J - 14/2/2018\n\n# SET THE spark.driver.maxResultSize PROPERTY TO 16g\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\nimport pycld2 as cld2\n\n# Latest blog/documentation: http://commoncrawl.org/2018/01/january-2018-crawl-archive-now-available/\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2018-05/wet.paths.gz\") # Jan 2018 Crawl - 80000 files (9.29TB)\n\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x)[0].lang # Maybe we can get away with looking at less characters, or do something less expensive?\n    except Exception as e:\n        return None\n        \ndef detect2(x):\n    try:\n        isReliable, textBytesFound, details = cld2.detect(x)\n        return details[0][1]\n    except Exception as e:\n        print(e)\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file: # Approx 53k web pages per WET file\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                \n                # TODO: Limit number of bytes read per record e.g. read(200000)\n                \n                domain = None if not url else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8') #.limit(100) # TODO: Limit this read to ensure max length (for improving parallizability)\n                lang = detect2(text[:300]) # Use PyCLD2, not langdetect, which was killing performance!\n                yield domain, url, text, lang\n            except Exception as e:\n                yield e\n                \ndef process_wet_simple(id_, iterator):\n    count=0\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                count=count+1\n                # TODO: Output total size of pages, rather than number of pages\n                # Histogram.\n            except Exception as e:\n                pass\n        #print(count)\n        yield count","user":"anonymous","dateUpdated":"2018-02-14T10:13:06+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518599492215_808642713","id":"20171020-102243_1718178582","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:13:06+0000","dateFinished":"2018-02-14T10:13:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3420"},{"text":"%pyspark\ndetect2(\"this is a test\")","user":"anonymous","dateUpdated":"2018-02-14T10:13:11+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"'en'\n"}]},"apps":[],"jobName":"paragraph_1518599492220_805179972","id":"20171027-134322_549744379","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:13:11+0000","dateFinished":"2018-02-14T10:13:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3421"},{"text":"%pyspark\n\n# PARAMETER - number of input files\nnfiles = 8192 # Total 80000\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles)) #, numSlices=nfiles/32) # TODO: Try numSlices=nfiles/32 etc, or just default!\n\n# Should parallelize\nprint(files.getNumPartitions())\nrdd=files.mapPartitionsWithIndex(process_wet)\n\nprint(str(rdd))\ndocs = rdd.toDF([\"host\", \"url\", \"text\",\"lang\"]) #  \"lang\"\n#docs.cache()\n#docs.count() # Total docs in all languages","user":"anonymous","dateUpdated":"2018-02-14T10:13:21+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"640\nPythonRDD[114] at RDD at PythonRDD.scala:48\n"}]},"apps":[],"jobName":"paragraph_1518599492220_805179972","id":"20171020-101220_2022546189","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:13:21+0000","dateFinished":"2018-02-14T10:13:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3422"},{"text":"%pyspark\n\n# Filter for English only\ndocs_en = docs.filter(docs.lang == 'en')","user":"anonymous","dateUpdated":"2018-02-14T10:14:01+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518599492220_805179972","id":"20171020-110218_1507019685","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:14:01+0000","dateFinished":"2018-02-14T10:14:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3423"},{"title":"Load saved vectors from Wikipedia model (created by python Wikipedia Text Processing.ipynb)","text":"%pyspark\nfrom pyspark.ml import Pipeline,PipelineModel\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\nfrom pyspark.ml.clustering import LocalLDAModel\n\ntextModel = PipelineModel.load('s3://billsdata.net/CommonCrawl/wikipedia/text_model')\nldaModel = LocalLDAModel.load('s3://billsdata.net/CommonCrawl/wikipedia/lda_model')","user":"anonymous","dateUpdated":"2018-02-14T10:14:04+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518599492220_805179972","id":"20171027-152148_622560673","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:14:04+0000","dateFinished":"2018-02-14T10:14:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3424"},{"text":"%pyspark\n\n# Test the models - for debugging only\nimport numpy as np\nimport pandas as pd\n\nX=ldaModel.topicsMatrix().toArray()\nvocab = np.array(textModel.stages[2].vocabulary)\n\ntopicLabels = [' '.join(vocab[np.argsort(X[:,i])[::-1][:5]]) for i in range(100)]\n\ndef score_topics(text):\n    df = sqlContext.createDataFrame(pd.DataFrame({'text':[text]}))\n    vec = textModel.transform(df)\n    scores = ldaModel.transform(vec).select('topicDistribution').collect()[0].topicDistribution.toArray()\n    return pd.Series(dict(zip(topicLabels, scores)))\n    \n# Try it on an arbitary sentence\nprint(score_topics(\"This is the latest news about North Korea and their involvement in the Winter Olympics in PyongChang\"))","user":"anonymous","dateUpdated":"2018-02-14T10:14:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"africa south african singh harris                    0.001191\nage population census county average                 0.001250\nair flight aircraft airlines airline                 0.001200\naircraft air wing flying first                       0.001206\nairport florida miami international south            0.001217\nal virginia ohio states tennessee                    0.001265\nalbum band released song music                       0.001321\nalso cells cell protein used                         0.001235\narea city town region located                        0.001317\narmy th war military force                           0.001256\nart museum work artist painting                      0.001265\naustralia australian sydney navy south               0.001248\nbishop catholic st italian church                    0.001255\nblue wales white welsh black                         0.001216\nbr plant oil flowers leaves                          0.001264\nbuilding house built historic buildings              0.001295\nc p r g b                                            0.001337\ncanada canadian member assembly ontario              0.001286\ncar race racing class cars                           0.001215\ncentury one book also first                          0.001268\ncentury th castle polish ii                          0.001251\nchurch christian god religious day                   0.001263\ncity county street center community                  0.001298\nclub league football season played                   0.001293\nclub season signed born league                       0.001201\ncode used system number memory                       0.001194\ncollege university school born served                0.001323\ncolorado iowa arizona arkansas utah                  0.001211\ncommune serbian hungarian serbia hungary             0.001254\ncompany business bank million group                  0.001286\n                                                       ...   \nnational united organization states international    0.001272\nnew newspaper editor york norwegian                  0.001259\nnew york city ny jersey                              0.001237\nnuclear kelly allen energy burns                     0.001181\none ball game player players                         0.001210\none time man first also                              0.001295\nparty election elected elections democratic          0.001271\nradio station television channel tv                  0.447151\nriver lake park mountain creek                       0.001316\nroad parish village england centre                   0.001257\nrussian russia moscow soviet alexander               0.001225\nsan el francisco puerto philippines                  0.001245\nschool high schools girls boys                       0.001261\nschool students education university college         0.001276\nseason team first teams cup                          0.001276\nseries book published books novel                    0.001261\nseries show television also episode                  0.001292\nship ships two navy war                              0.001220\nsocial one may also people                           0.001240\nspace earth light solar star                         0.001223\nspecies found also large may                         0.001261\nstation line railway service train                   0.001261\nteam season coach football first                     0.001253\ntom oliver ghost haiti kay                           0.001183\nukrainian ukraine dog dogs stamps                    0.001198\nuniversity research professor published science      0.001323\nwar union soviet communist political                 0.001213\nwater company construction new coal                  0.001240\nworld olympics championships summer women            0.430457\nzealand new grand auckland prix                      0.001216\nLength: 100, dtype: float64\n"}]},"apps":[],"jobName":"paragraph_1518599492220_805179972","id":"20180209-102157_2110186523","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:14:08+0000","dateFinished":"2018-02-14T10:14:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3425"},{"text":"%pyspark\n\n# Now score pages from our WET files\ndocs_en.show(5)\nvec = textModel.transform(docs_en)\nvec.show(5)\n","user":"anonymous","dateUpdated":"2018-02-14T10:14:14+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+----+\n|                host|                 url|                text|lang|\n+--------------------+--------------------+--------------------+----+\n|                null|                null|Software-Info: ia...|  en|\n|0ncemorewithfeeli...|http://0ncemorewi...|Once More, With F...|  en|\n|        100share.com|http://100share.c...|Saitek Eclipse Ba...|  en|\n|101bestandroidapp...|http://101bestand...|fabien | 101 Best...|  en|\n|        1045espn.com|http://1045espn.c...|Mickey Joseph on ...|  en|\n+--------------------+--------------------+--------------------+----+\nonly showing top 5 rows\n\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n|                host|                 url|                text|lang|               words|            filtered|                 vec|\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\n|                null|                null|Software-Info: ia...|  en|[software, info, ...|[software, info, ...|(20000,[84,152,33...|\n|0ncemorewithfeeli...|http://0ncemorewi...|Once More, With F...|  en|[once, more, with...|[feeling, christm...|(20000,[2,3,5,7,1...|\n|        100share.com|http://100share.c...|Saitek Eclipse Ba...|  en|[saitek, eclipse,...|[saitek, eclipse,...|(20000,[0,1,2,3,4...|\n|101bestandroidapp...|http://101bestand...|fabien | 101 Best...|  en|[fabien, best, an...|[fabien, best, an...|(20000,[0,1,2,3,7...|\n|        1045espn.com|http://1045espn.c...|Mickey Joseph on ...|  en|[mickey, joseph, ...|[mickey, joseph, ...|(20000,[3,6,40,72...|\n+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518599492221_804795224","id":"20171020-101912_1610139389","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:14:14+0000","dateFinished":"2018-02-14T10:14:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3426"},{"text":"%pyspark\n\n# Create topic distribution vectors and tidy upp\nscores = ldaModel.transform(vec)\nscores2 = scores.drop(\"url\").drop(\"text\").drop(\"lang\").drop(\"words\").drop(\"filtered\").drop(\"vec\")\nscores2.show(5)","user":"anonymous","dateUpdated":"2018-02-14T10:14:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|                host|   topicDistribution|\n+--------------------+--------------------+\n|                null|[4.59706427634550...|\n|0ncemorewithfeeli...|[3.78779380156022...|\n|        100share.com|[8.1625505686031E...|\n|101bestandroidapp...|[1.28997469852102...|\n|        1045espn.com|[6.65356474668816...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518599492221_804795224","id":"20180209-102904_2097212017","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:14:19+0000","dateFinished":"2018-02-14T10:14:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3427"},{"text":"%pyspark\n\n# Save these vectors to disc, so we can just load them later\nscores2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)","user":"anonymous","dateUpdated":"2018-02-14T10:14:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518599492221_804795224","id":"20180209-103007_1930247145","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:14:27+0000","dateFinished":"2018-02-14T17:08:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3428"},{"title":"Load saved scores from nfiles of WET files","text":"%pyspark\n\n# Restart here\nnfiles=8192\nscores2 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_page_wiki_topic_scores' % nfiles)\nscores2.show(5)","user":"anonymous","dateUpdated":"2018-02-14T10:14:43+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|                host|   topicDistribution|\n+--------------------+--------------------+\n|                null|[0.13020043279709...|\n|1000daysofwriting...|[3.26238961502545...|\n|100unhappydays.bl...|[5.81230792144733...|\n|          10in30.com|[8.42785330446217...|\n|123-free-download...|[6.44166735802645...|\n+--------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518599492222_805949470","id":"20180209-103529_782306138","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T09:13:58+0000","dateFinished":"2018-02-14T09:14:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3429"},{"text":"%pyspark\n\n# Aggregate page-scores per Host for now (will be same process for aggregating host-scores to PLD-scores) - first count and sum vectors\nscores3=scores2.rdd.map(lambda x: (x['host'], (1,x['topicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores4=scores3.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"host\", \"averageTopicDistribution\"])\nscores4.show(5)\n{s.name: type(s.dataType) for s in scores4.schema}\nprint(scores2.count())\nprint(scores4.count())","user":"anonymous","dateUpdated":"2018-02-14T16:46:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"+--------------------+------------------------+\n|                host|averageTopicDistribution|\n+--------------------+------------------------+\n|www.wolfwoodrefug...|    [1.21819079661946...|\n|gameschat.reociti...|    [0.00274348231459...|\n| www.redmapleinn.com|    [4.10790514555528...|\n|   dallasal-anon.org|    [4.43570678815298...|\n|      cappelleria.eu|    [0.00534788675680...|\n+--------------------+------------------------+\nonly showing top 5 rows\n\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-4055167835508644674.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-4055167835508644674.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 5, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 427, in count\n    return int(self._jdf.count())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o576.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 275 in stage 80.0 failed 4 times, most recent failure: Lost task 275.3 in stage 80.0 (TID 9277, ip-172-31-21-168.eu-west-1.compute.internal, executor 7): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518599246324_0001/container_1518599246324_0001_01_000014/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518599246324_0001/container_1518599246324_0001_01_000014/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518599246324_0001/container_1518599246324_0001_01_000014/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 32, in process_wet\n  File \"<stdin>\", line 13, in unpack\n  File \"/usr/local/lib/python2.7/site-packages/boto/__init__.py\", line 141, in connect_s3\n    return S3Connection(aws_access_key_id, aws_secret_access_key, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/boto/s3/connection.py\", line 191, in __init__\n    validate_certs=validate_certs, profile_name=profile_name)\n  File \"/usr/local/lib/python2.7/site-packages/boto/connection.py\", line 555, in __init__\n    profile_name)\n  File \"/usr/local/lib/python2.7/site-packages/boto/provider.py\", line 200, in __init__\n    self.get_credentials(access_key, secret_key, security_token, profile_name)\n  File \"/usr/local/lib/python2.7/site-packages/boto/provider.py\", line 376, in get_credentials\n    self._populate_keys_from_metadata_server()\n  File \"/usr/local/lib/python2.7/site-packages/boto/provider.py\", line 394, in _populate_keys_from_metadata_server\n    security = list(metadata.values())[0]\n  File \"/usr/local/lib/python2.7/site-packages/boto/utils.py\", line 339, in values\n    self._materialize()\n  File \"/usr/local/lib/python2.7/site-packages/boto/utils.py\", line 263, in _materialize\n    self[key]\n  File \"/usr/local/lib/python2.7/site-packages/boto/utils.py\", line 323, in __getitem__\n    raise last_exception\nJSONDecodeError: Unterminated string starting at: line 8 column 18 (char 870)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1708)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1696)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1695)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1695)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1867)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2435)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2434)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2842)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2841)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2434)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518599246324_0001/container_1518599246324_0001_01_000014/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518599246324_0001/container_1518599246324_0001_01_000014/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/zeppelin/appcache/application_1518599246324_0001/container_1518599246324_0001_01_000014/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<stdin>\", line 32, in process_wet\n  File \"<stdin>\", line 13, in unpack\n  File \"/usr/local/lib/python2.7/site-packages/boto/__init__.py\", line 141, in connect_s3\n    return S3Connection(aws_access_key_id, aws_secret_access_key, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/boto/s3/connection.py\", line 191, in __init__\n    validate_certs=validate_certs, profile_name=profile_name)\n  File \"/usr/local/lib/python2.7/site-packages/boto/connection.py\", line 555, in __init__\n    profile_name)\n  File \"/usr/local/lib/python2.7/site-packages/boto/provider.py\", line 200, in __init__\n    self.get_credentials(access_key, secret_key, security_token, profile_name)\n  File \"/usr/local/lib/python2.7/site-packages/boto/provider.py\", line 376, in get_credentials\n    self._populate_keys_from_metadata_server()\n  File \"/usr/local/lib/python2.7/site-packages/boto/provider.py\", line 394, in _populate_keys_from_metadata_server\n    security = list(metadata.values())[0]\n  File \"/usr/local/lib/python2.7/site-packages/boto/utils.py\", line 339, in values\n    self._materialize()\n  File \"/usr/local/lib/python2.7/site-packages/boto/utils.py\", line 263, in _materialize\n    self[key]\n  File \"/usr/local/lib/python2.7/site-packages/boto/utils.py\", line 323, in __getitem__\n    raise last_exception\nJSONDecodeError: Unterminated string starting at: line 8 column 18 (char 870)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}]},"apps":[],"jobName":"paragraph_1518599492222_805949470","id":"20180209-104711_1636843522","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T17:08:13+0000","dateFinished":"2018-02-14T23:18:32+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:3430"},{"text":"%pyspark\n\n# Just playing - code to help understand the different libraries and vector types!\nimport pyspark.mllib.linalg as mllib\nimport pyspark.ml.linalg as ml\ndf = sc.parallelize([\n    (mllib.DenseVector([1, ]), ml.DenseVector([1, ])),\n    (mllib.SparseVector(1, [0, ], [1, ]), ml.SparseVector(1, [0, ], [1, ]))\n]).toDF([\"mllib_v\", \"ml_v\"])\ndf.show()\n{s.name: type(s.dataType) for s in df.schema}","user":"anonymous","dateUpdated":"2018-02-15T10:06:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+-------------+\n|      mllib_v|         ml_v|\n+-------------+-------------+\n|        [1.0]|        [1.0]|\n|(1,[0],[1.0])|(1,[0],[1.0])|\n+-------------+-------------+\n\n{'ml_v': <class 'pyspark.ml.linalg.VectorUDT'>, 'mllib_v': <class 'pyspark.mllib.linalg.VectorUDT'>}\n"}]},"apps":[],"jobName":"paragraph_1518599492222_805949470","id":"20180213-131625_601576088","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-15T10:06:15+0000","dateFinished":"2018-02-15T10:06:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3431"},{"text":"%pyspark\n\n# Enrich each row with the corresponding PLD (using code from Paul J)\n# TODO: Should just pickle the Bloom filter and load it in, or use above distributed Bloom Filter code!\n#saveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-aug-sep-oct/domaingraph/vertices/\"\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-18-nov-dec-jan/domaingraph/vertices/\"\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\npld_df.count() # 70M in latest web graph","user":"anonymous","dateUpdated":"2018-02-15T10:07:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+--------+\n| ID|    PLD|NumHosts|\n+---+-------+--------+\n|  0|  aaa.a|       1|\n|  1| aaa.aa|       1|\n|  2|aaa.aaa|       4|\n+---+-------+--------+\nonly showing top 3 rows\n\n70367284\n"}]},"apps":[],"jobName":"paragraph_1518599492222_805949470","id":"20180209-103948_583472396","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-15T10:06:25+0000","dateFinished":"2018-02-15T10:06:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3432"},{"text":"%pyspark\n\nfrom pybloom import BloomFilter\n\n# Attempt to distribute our Bloom Filter so we can build it in parallel, and save it to disk as an RDD\ndef build_partial_bloom(capacity, error_rate):\n    def _build_partial_bloom(record):\n        bloom_filter = BloomFilter(capacity=capacity, error_rate=error_rate)\n        for _,PLD,_ in record: # Just take the PLD field (not the ID or NumHosts)\n            bloom_filter.add(PLD)\n        yield (None, bloom_filter) # returns a Generator (i.e. a kind of Iterator that can only get called once, and doesn't stay in memory)\n    return _build_partial_bloom\n\ndef merge_bloom(bloom_in1, bloom_in2):\n    return bloom_in1.union(bloom_in2) # The reduce function simply becomes a Union\n\n# Our generator function for partial Bloom Filters\ngenerate_bloom = build_partial_bloom(capacity=94000000, error_rate=0.005) # Only 70M in latest web graph?\n\n# Construct the distributed BloomFilter using the PLD dataframe\nbloom_filter_rdd = pld_df.rdd.mapPartitions(generate_bloom).reduceByKey(merge_bloom).collect()\nprint(bloom_filter_rdd)\n\n# Broadcast bloom object\nbloom_filter_broadcast = sc.broadcast(bloom_filter_rdd[0][1])\n","user":"anonymous","dateUpdated":"2018-02-15T11:28:36+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[(None, <pybloom.pybloom.BloomFilter object at 0x7fc7d02add50>)]\n"}]},"apps":[],"jobName":"paragraph_1518613319936_288462282","id":"20180214-130159_1231127388","dateCreated":"2018-02-14T13:01:59+0000","dateStarted":"2018-02-15T11:28:36+0000","dateFinished":"2018-02-15T11:36:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3433"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when counting hosts\n# Create a bloom filter using a pure python package (might be a little slow)\npld_bf = BloomFilter(capacity=94000000, error_rate=0.005) # was 91M\n\nfor row in pld_df.rdd.collect(): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\nprint(pld_df.rdd.take(3))\nprint(pld_df.rdd.take(3)[2]['PLD'])\n#pld_bf.add(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false\n","user":"anonymous","dateUpdated":"2018-02-14T13:02:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(ID=u'0', PLD=u'aaa.1'), Row(ID=u'1', PLD=u'aaa.2'), Row(ID=u'2', PLD=u'aaa.3')]\naaa.3\nTrue\n64\n93110180\nTrue\nFalse\nTrue\nFalse\n"}]},"apps":[],"jobName":"paragraph_1518599492223_805564721","id":"20180213-133625_1860489211","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T09:17:30+0000","dateFinished":"2018-02-14T09:46:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3434"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import udf\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n\nudf_convert_hostname = udf(convert_hostname, StringType())\n\n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","user":"anonymous","dateUpdated":"2018-02-14T09:49:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]},"apps":[],"jobName":"paragraph_1518599492223_805564721","id":"20180213-133744_1577422019","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T09:49:56+0000","dateFinished":"2018-02-14T09:49:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3435"},{"text":"%pyspark\n\n# Function to reverse hostnames\nfrom pyspark.sql.functions import udf\ndef reverse_domain(domain):\n    try:\n        ret =  '.'.join(reversed(domain.split('.')))\n        return ret\n    except:\n        return \"NODOMAIN\"\nprint(reverse_domain(\"com.facebook\"))\nudf_reverse_domain = udf(reverse_domain, StringType())\n\n# Convert hosts in Topic DF to PLDs using convert_hostname function from Paul 5.\nscores5=scores4.withColumn(\"pld\",udf_reverse_domain(udf_convert_hostname(udf_reverse_domain(\"host\")))) # Reverse the hostnames prior to lookup, then back again!\nscores5.show(10)","user":"anonymous","dateUpdated":"2018-02-14T10:01:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"facebook.com\n+--------------------+------------------------+--------------------+\n|                host|averageTopicDistribution|                 pld|\n+--------------------+------------------------+--------------------+\n|     www.huskers.com|    [3.21845819846659...|         huskers.com|\n|www.thedallassoci...|    [6.56541334847065...|thedallassocials.com|\n|www.teamlivelonge...|    [0.05599575752450...|  teamlivelonger.com|\n|   natashasattic.com|    [9.10570835348067...|   natashasattic.com|\n|puertorican-femal...|    [0.02448912169981...|     yesalbania.info|\n| thejunglesafari.com|    [1.80690128796215...| thejunglesafari.com|\n|theamateurathlete...|    [0.00288833454319...|       wordpress.com|\n|wellbeingenhanced...|    [1.73198422950055...|        blogspot.com|\n|scholarsmine.mst.edu|    [0.00493865345810...|             mst.edu|\n|  hivdb.stanford.edu|    [5.55176978154368...|        stanford.edu|\n+--------------------+------------------------+--------------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1518599492223_889824730","id":"20180213-133858_1524477020","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:01:31+0000","dateFinished":"2018-02-14T10:01:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3436"},{"text":"%pyspark\n\n# Now we can aggregate page-scores per PLD, using a map-reduce similar to the host aggregation above - first count and sum vectors\nscores6=scores5.rdd.map(lambda x: (x['pld'], (1,x['averageTopicDistribution']))).reduceByKey(lambda acc, val: (acc[0]+val[0], acc[1]+val[1]))\n\n# Next, divide by the total to create averaged vectors, and convert back to a dataframe\nscores7=scores6.map(lambda x: (x[0], (x[1][1]/x[1][0]))).toDF([\"pld_rev\", \"averageTopicDistribution\"])\nscores7.show(5)\n{s.name: type(s.dataType) for s in scores7.schema}\nprint(scores5.count())\nprint(scores7.count())","user":"anonymous","dateUpdated":"2018-02-14T10:01:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+------------------------+\n|             pld_rev|averageTopicDistribution|\n+--------------------+------------------------+\n|            wect.com|    [0.00392042707324...|\n|silverspringsacad...|    [3.58793849595704...|\n|     demandforce.com|    [0.00126409763409...|\n|worldvintagefashi...|    [5.30052293861408...|\n|       ticketfoo.com|    [9.13519657478704...|\n+--------------------+------------------------+\nonly showing top 5 rows\n\n487486\n304685\n"}]},"apps":[],"jobName":"paragraph_1518599492224_889824730","id":"20180213-140037_440180641","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:01:44+0000","dateFinished":"2018-02-14T10:02:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3437"},{"text":"%pyspark\n\n# Save pld topic distributions in parquet format for Tom to play with (and to figure out how to create a PLD topic summary from this).\n# TODO: Maybe a numpy argmax to get the index of the 'top' topic for each PLD with a score.\nscores7.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_pld_wiki_topic_scores' % nfiles)","user":"anonymous","dateUpdated":"2018-02-14T10:02:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1518599492224_889824730","id":"20180209-104436_1341725902","dateCreated":"2018-02-14T09:11:32+0000","dateStarted":"2018-02-14T10:02:36+0000","dateFinished":"2018-02-14T10:05:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3438"},{"text":"%pyspark\n","dateUpdated":"2018-02-14T09:11:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518599492224_889824730","id":"20180213-153111_1438771373","dateCreated":"2018-02-14T09:11:32+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3439"}],"name":"Tom 2 Wiki Topic Scoring","id":"2D958XKC5","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}