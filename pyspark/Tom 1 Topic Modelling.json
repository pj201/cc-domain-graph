{"paragraphs":[{"text":"%pyspark\n\n# PySpark CommonCrawl Topic Modelling\n# Tom V / Paul J - 14/11/2017\n\n# SET THE spark.driver.maxResultSize PROPERTY TO 3G\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\nimport pycld2 as cld2\n\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2017-04/wet.paths.gz\")\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x)[0].lang # Maybe we can get away with looking at less characters, or do something less expensive?\n    except Exception as e:\n        return None\n        \ndef detect2(x):\n    try:\n        isReliable, textBytesFound, details = cld2.detect(x)\n        return details[0][1]\n    except Exception as e:\n        print(e)\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file: # Approx 53k web pages per WET file\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                domain = None if not url else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8') #.limit(100) # TODO: Limit this read to ensure max length (for improving parallizability)\n                lang = detect2(text[:300]) # Use PyCLD2, not langdetect, which was killing performance!\n                yield domain, url, text, lang\n            except Exception as e:\n                yield e\n                \ndef process_wet_simple(id_, iterator):\n    count=0\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                count=count+1\n                # TODO: Output total size of pages, rather than number of pages\n                # Histogram.\n            except Exception as e:\n                pass\n        #print(count)\n        yield count","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1510650498202_-470249385","id":"20171020-102243_1718178582","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T09:25:39+0000","dateFinished":"2017-11-14T09:25:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:302"},{"text":"%pyspark\ndetect2(\"this is a test\")","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"'en'\n"}]},"apps":[],"jobName":"paragraph_1510650498205_-472942627","id":"20171027-134322_549744379","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T09:25:39+0000","dateFinished":"2017-11-14T09:25:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:303"},{"text":"%pyspark\n\n# PARAMETER - number of input files\nnfiles = 4096\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles), numSlices=nfiles)\n\n# TODO: Make this use more than one CPU!\nprint(files.getNumPartitions())\n#files.mapPartitionsWithIndex(process_wet_simple).collect()\nrdd=files.mapPartitionsWithIndex(process_wet)\n\nprint(str(rdd))\ndocs = rdd.toDF([\"domain\", \"url\", \"text\",\"lang\"]) #  \"lang\"\n#docs.cache()\ndocs.count()","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"4096\nPythonRDD[22] at RDD at PythonRDD.scala:48\n220058414\n"}]},"apps":[],"jobName":"paragraph_1510650498205_-472942627","id":"20171020-101220_2022546189","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T09:25:39+0000","dateFinished":"2017-11-14T09:39:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:304"},{"text":"%pyspark\n\ndocs_en = docs.filter(docs.lang == 'en')\n#docs_en = docs\n\n# PARAMETER - possibly set partitions?\n#docs_en = docs_en.repartition(nfiles)\n#docs_en.rdd.getNumPartitions()\n#docs_en.sample(True,0.01).groupBy('lang').count().toPandas()\ndocs_en.count() # 87842898 for 4096 files","user":"anonymous","dateUpdated":"2017-11-14T11:05:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"tableHide":false,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"87842898\n"}]},"apps":[],"jobName":"paragraph_1510650498206_-471788380","id":"20171020-110218_1507019685","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T09:25:39+0000","dateFinished":"2017-11-14T09:53:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:305"},{"text":"%pyspark\n\nstopwords_english = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'com', 'nbsp']\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\n\n# PARAMETER - regex tokenization\ntokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\p{L}{3,}\", gaps=False)\nstopwordRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\",stopWords=stopwords_english, caseSensitive=False)\n\n# PARAMETER - vocab size, min and max doc frequency\ncv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vec\",vocabSize=50000, minDF=50) # reduced vocabSize from 100k to 20k to save memory\n\n# Run the model fitting and extract vectors\npipeline = Pipeline(stages=[tokenizer, stopwordRemover, cv])\nmodel = pipeline.fit(docs_en)\nvecs = model.transform(docs_en).drop('text').drop('words').drop('filtered')\nvecs.cache()\n#vecs.count()\n\n# Took 42 min 51 sec on m4.16xlarge with 1024 files, approx 2 hours on 5xr4.8xlarge with 4096 files ","user":"anonymous","dateUpdated":"2017-11-14T12:16:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false,"title":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"87842898\n"}]},"apps":[],"jobName":"paragraph_1510650498206_-471788380","id":"20171020-101427_882065123","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T09:39:36+0000","dateFinished":"2017-11-14T12:15:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:306"},{"text":"%pyspark\nsc.getConf().get('spark.driver.maxResultSize')","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"u'16g'\n"}]},"apps":[],"jobName":"paragraph_1510650498207_-472173129","id":"20171027-153123_778277811","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T09:53:27+0000","dateFinished":"2017-11-14T12:15:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"text":"%pyspark\nmodel.save('s3://billsdata.net/CommonCrawl/topic_model_%d_files/textmodel' % nfiles)\nvecs.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_english_page_tf_vectors' % nfiles)","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1510650498207_-472173129","id":"20171027-145239_579746754","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T12:15:29+0000","dateFinished":"2017-11-14T13:17:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:308"},{"title":"Skip to here to restart from saved vectors","text":"%pyspark\nfrom pyspark.ml.pipeline import PipelineModel\n\nnfiles = 4096\n\nmodel2 = PipelineModel.load('s3://billsdata.net/CommonCrawl/topic_model_%d_files/textmodel' % nfiles)\nvecs2 = spark.read.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_english_page_tf_vectors' % nfiles)\n\nvecs2.show(10)","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+----+--------------------+\n|              domain|                 url|lang|                 vec|\n+--------------------+--------------------+----+--------------------+\n|                null|                null|  en|(50000,[59,79,91,...|\n|1.cricwaves-hrd.a...|http://1.cricwave...|  en|(50000,[0,1,2,4,6...|\n|       1015store.com|http://1015store....|  en|(50000,[0,5,6,22,...|\n|101bestandroidapp...|http://101bestand...|  en|(50000,[0,3,4,5,6...|\n|101bestandroidapp...|http://101bestand...|  en|(50000,[0,3,4,6,7...|\n|1027jackfm.iheart...|http://1027jackfm...|  en|(50000,[0,3,4,5,8...|\n|     1057thehawk.com|http://1057thehaw...|  en|(50000,[0,1,2,4,5...|\n|     1063thebuzz.com|http://1063thebuz...|  en|(50000,[0,2,4,5,6...|\n|     1063thebuzz.com|http://1063thebuz...|  en|(50000,[0,2,4,5,8...|\n|  1190kex.iheart.com|http://1190kex.ih...|  en|(50000,[0,3,4,5,6...|\n+--------------------+--------------------+----+--------------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1510650498208_-486408839","id":"20171027-152148_622560673","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T12:15:29+0000","dateFinished":"2017-11-14T13:17:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:309"},{"text":"%pyspark\nvecs2.cache()","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DataFrame[domain: string, url: string, lang: string, vec: vector]\n"}]},"apps":[],"jobName":"paragraph_1510650498209_-486793587","id":"20171027-152858_769393097","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T13:17:04+0000","dateFinished":"2017-11-14T13:17:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:310"},{"text":"%pyspark\n\n# Run the topic modelling\n\nfrom pyspark.ml.clustering import LDA\n#inputCol=\"vec\", outputCol=\"ldaVec\", k=3, optimizer=\"online\"\n\n# Fix java memory errors, perhaps using:\n# spark.driver.memory 256g - DIDN'T WORK\n# or by reducing vocabSize from 100k to 20k - WORKS!\n\n#With 128:\n#    Py4JJavaError: An error occurred while calling o375.fit.\n#: org.apache.spark.SparkException: Job 49 cancelled because SparkContext was shut down\n\nlda = LDA(k=100, maxIter=10, featuresCol=\"vec\") # Reduced maxIter from 100 to 50\nldaModel = lda.fit(vecs2)","user":"anonymous","dateUpdated":"2017-11-14T10:50:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1510650498209_-486793587","id":"20171020-101446_892258643","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T13:17:09+0000","dateFinished":"2017-11-14T20:47:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:311"},{"text":"%pyspark\n\n# Save the models\nldaModel.save('s3://billsdata.net/CommonCrawl/topic_model_%d_files/ldamodel' % nfiles)\n#pipeline.save('s3://billsdata.net/CommonCrawl/topic_model_%d_files/textpipeline' % nfiles)","user":"anonymous","dateUpdated":"2017-11-14T10:50:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1510650498209_-486793587","id":"20171020-124001_1609701105","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T13:17:10+0000","dateFinished":"2017-11-14T20:48:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:312"},{"text":"%pyspark\n\n# Get topic vectors for index pages (estimate of topic vec per domain)\nvecs_index = vecs2.filter(\"url LIKE '%index.html'\")\nresults = ldaModel.transform(vecs_index)\n\n# Drop text cols\nresults2=results.drop('text').drop('words').drop('filtered')\n\n# Save domain topic vecs\nresults2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_index_page_topics' % nfiles)","user":"anonymous","dateUpdated":"2017-11-14T10:50:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1510650498210_-485639341","id":"20171020-124102_343731757","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T20:47:57+0000","dateFinished":"2017-11-14T20:52:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:313"},{"text":"%pyspark\n\n# Create a dataset containing just the host, url and top 3 topic labels & scores\nimport pandas as pd\nimport numpy as np\ntopicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5).collect()\nvocab = model2.stages[2].vocabulary\n\ntopic_labels = []\nfor i, (topic, terms, termWeights) in enumerate(topicIndices):\n    topwords = pd.Series(dict(zip([vocab[t] for t in terms], termWeights))).sort_values(ascending=False)\n    topic_labels.append('_'.join(topwords.index.values))\n\ntopic_labels = np.array(topic_labels)\n\ndef topTopics(x):\n    labels = topic_labels[np.argsort(x.topicDistribution)[::-1][:3]]\n    scores = np.sort(x.topicDistribution)[::-1][:3]\n    return (x.domain, x.url, str(labels[0]), float(scores[0]), str(labels[1]), float(scores[1]), str(labels[2]), float(scores[2]))\n\nresults3 = results2.rdd.map(topTopics)\nresults3 = results3.toDF([\"host\", \"url\", \"topic1\", \"score1\", \"topic2\", \"score2\", \"topic3\", \"score3\"])\n\nresults3.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_%d_files/cc_index_page_topic_labels' % nfiles)\nresults3.show()","user":"anonymous","dateUpdated":"2017-11-14T10:50:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+\n|                host|                 url|              topic1|             score1|              topic2|             score2|              topic3|              score3|\n+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+\n|    arabiansites.com|http://arabiansit...|horse_magazine_ho...|0.32690709992629885|property_estate_h...|0.19472739545485596|love_new_book_bes...| 0.19126870899727655|\n|      boysgayxxx.com|http://boysgayxxx...|porn_tube_sex_big...| 0.8037020611999924|views_care_tags_a...|0.18620647574523513|games_wiki_game_c...|0.005722520944758405|\n|      boysgayxxx.com|http://boysgayxxx...|porn_tube_sex_big...|   0.76103367641799|views_care_tags_a...| 0.1975227238501043|sex_videos_rape_v...|0.037113952819980756|\n|        cfbstats.com|http://cfbstats.c...|sports_basketball...| 0.8205514209918837|news_perez_new_ga...|0.16291537586398164|one_like_would_ti...|1.732563448797539...|\n|     dailysudoku.com|http://dailysudok...| jan_dec_nov_mar_oct| 0.5246290222286816|january_december_...|0.25241842372341156|dog_pet_boat_cat_...| 0.14782379383242267|\n|  distro.ibiblio.org|http://distro.ibi...|data_software_use...| 0.8943977514118331|english_photo_spa...| 0.0792418771457871|one_like_would_ti...|2.762865990589873...|\n|     docs.oracle.com|http://docs.oracl...|data_software_use...| 0.8310203852340442|buy_search_domain...|0.09232621235673148|name_see_address_...| 0.06874196279839279|\n|     docs.oracle.com|http://docs.oracl...|data_software_use...| 0.5279230230434105|php_quot_line_htm...| 0.2879934462864938|name_see_address_...|  0.0713479696271631|\n|     docs.oracle.com|http://docs.oracl...|data_software_use...|  0.788282001498816|agent_cloud_new_c...|0.09986350477647545|php_quot_line_htm...|0.042235529660738506|\n|     docs.oracle.com|http://docs.oracl...|amp_java_org_apac...| 0.4150390629448446|data_software_use...|  0.379045388829628|php_quot_line_htm...|  0.1983733931437911|\n|     docs.oracle.com|http://docs.oracl...|data_software_use...|  0.441599050371447|figure_skating_fo...|0.24191240835021305|function_return_v...| 0.17318573024212544|\n|     docs.oracle.com|http://docs.oracl...|data_software_use...| 0.5861787940032585|php_quot_line_htm...|0.24787892656019953|agent_cloud_new_c...|  0.1244802880173212|\n|     edition.cnn.com|http://edition.cn...|said_news_report_...| 0.9635777837592386|share_new_music_f...|0.03310475547720189|one_like_would_ti...|3.476693671937279E-5|\n|        erotica7.com|http://erotica7.c...|porn_tube_sex_big...| 0.5960637601868957|sex_videos_rape_v...| 0.3662782821588147|art_county_societ...|0.028230217476013036|\n|  faculty.ccbcmd.edu|http://faculty.cc...|data_software_use...| 0.7354488577291799|gene_database_che...|0.11068836745669192|products_inc_ener...|  0.0771061301646233|\n|galleries.payserv...|http://galleries....|porn_tube_sex_big...| 0.5480084155524777|sex_videos_rape_v...| 0.3534834793881945|property_estate_h...|0.052527662049537474|\n|galleries.payserv...|http://galleries....|sex_videos_rape_v...|  0.776732477408968|data_software_use...|0.14335291457896607|pandora_photo_sub...|  0.0714472419514846|\n|galleries.payserv...|http://galleries....|sex_videos_rape_v...| 0.7761245598307572|data_software_use...|0.14343267315433433|pandora_photo_sub...| 0.07197540095432717|\n|galleries.payserv...|http://galleries....|sex_videos_rape_v...| 0.6549328573747005|love_new_book_bes...|0.26322466497914604|http_www_tutoring...| 0.06490451278206634|\n|galleries.payserv...|http://galleries....|sex_videos_rape_v...| 0.8493269413048462|pandora_photo_sub...|0.11002331075501805|one_like_would_ti...|4.259708855302345...|\n+--------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1510650498210_-485639341","id":"20171020-131815_2006247238","dateCreated":"2017-11-14T09:08:18+0000","dateStarted":"2017-11-14T20:48:03+0000","dateFinished":"2017-11-14T20:56:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:314"},{"text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2017-11-14T10:50:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510650498211_-486024090","id":"20171020-101912_1610139389","dateCreated":"2017-11-14T09:08:18+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:315"}],"name":"Tom 1 Topic Modelling","id":"2D15AREKK","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}