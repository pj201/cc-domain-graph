{"paragraphs":[{"text":"%pyspark\n\nimport boto\nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom pyspark.sql.types import *\nimport warc\nimport ujson as json\nfrom urlparse import urlparse\nfrom langdetect import detect_langs\n\nwetlist = sc.textFile(\"s3://commoncrawl/crawl-data/CC-MAIN-2017-04/wet.paths.gz\")\nwetlist.cache()\n\ndef unpack(uri):\n    conn = boto.connect_s3(anon=True, host='s3.amazonaws.com')\n    bucket = conn.get_bucket('commoncrawl')\n    key_ = Key(bucket, uri)\n    file_ = warc.WARCFile(fileobj=GzipStreamFile(key_))\n    return file_\n\ndef detect(x):\n    try:\n        return detect_langs(x[:300])[0].lang\n    except Exception as e:\n        return None\n\ndef process_wet(id_, iterator):\n    for uri in iterator:\n        file = unpack(uri)\n        for record in file:\n            try:\n                #url = record.rec_headers.get_header('WARC-Target-URI')\n                #yield record, record.content_stream().read().decode('utf-8')\n                url = record.url\n                domain = None if not url  else urlparse(url).netloc\n                text = record.payload.read().decode('utf-8')\n                lang = detect(text)\n                yield domain, url, lang, text\n            except Exception as e:\n                yield e","dateUpdated":"2017-10-20T14:10:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1508508356420_-1781193322","id":"20171020-102243_1718178582","dateCreated":"2017-10-20T14:05:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:174","user":"anonymous","dateFinished":"2017-10-20T14:10:19+0000","dateStarted":"2017-10-20T14:10:19+0000"},{"text":"%pyspark\n# PARAMETER - number of input files\nnfiles = 64\n\n# PARAMETER - slices / partitions of input\nfiles = sc.parallelize(wetlist.take(nfiles), numSlices=64)\n\n# TODO: Make this use more than one CPU!\n\nprint(files.getNumPartitions())\n\nrdd = files.mapPartitionsWithIndex(process_wet)\n\ndocs = rdd.toDF([\"domain\", \"url\", \"lang\", \"text\"])\ndocs.cache()\n\ndocs.show()","dateUpdated":"2017-10-20T14:42:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"64\n+--------------------+--------------------+-----+--------------------+\n|              domain|                 url| lang|                text|\n+--------------------+--------------------+-----+--------------------+\n|                null|                null|   en|Software-Info: ia...|\n|        03online.com|http://03online.c...|   ru|Вопросы пользоват...|\n|        03online.com|http://03online.c...|   ru|Почему синеют губ...|\n|        03online.com|http://03online.c...|   ru|Темные круги под ...|\n|          05sese.com|http://05sese.com...|zh-cn|手淫的妹妹-\n图片专区 自拍偷拍亚...|\n|            08.od.ua|http://08.od.ua/g...|   ru|Тепло-холод в Оде...|\n|            08.od.ua|http://08.od.ua/t...|   ru|4В компания, ооо ...|\n|             0lik.ru|http://0lik.ru/te...|   ru|Free Arthur Radle...|\n|             0lik.ru|http://0lik.ru/te...|   ru|Новогодняя рамка ...|\n|             0lik.ru|http://0lik.ru/te...|   ru|Коллекция из 3 св...|\n|           1.163.com|http://1.163.com/...|   ko|【平安金】平安银行 富贵平安金碗筷...|\n|           1.163.com|http://1.163.com/...|zh-cn|夺宝记录 - 网易1元夺宝\n欢迎来...|\n|         100.ufc.com|http://100.ufc.co...|   en|UFC 100 Official ...|\n|         10000km.com|http://10000km.co...|   ja|海外「この炊飯器のどこに9万円の価...|\n|      1000designs.ru|http://1000design...|   ru|Экзотический скра...|\n|      1000designs.ru|http://1000design...|   ru|Летняя рамка на д...|\n|           1000ff.de|http://1000ff.de/...|   en|Neues Spielen?\n10...|\n|         1000form.ru|http://1000form.r...|   ru|Коллективный дого...|\n|1000fragrances.bl...|http://1000fragra...|   en|1000fragrances: A...|\n|           1000mg.jp|http://1000mg.jp/...|   ja|ニワトリの長すぎるロングトーンｗ　...|\n+--------------------+--------------------+-----+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1508508356424_-1782732318","id":"20171020-101220_2022546189","dateCreated":"2017-10-20T14:05:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:175","user":"anonymous","dateFinished":"2017-10-20T14:18:11+0000","dateStarted":"2017-10-20T14:10:19+0000"},{"text":"%pyspark\n\ndocs_en = docs.filter(docs.lang == 'en')\n\n# PARAMETER - possibly set partitions?\n\ndocs_en = docs_en.repartition(256)","dateUpdated":"2017-10-20T14:19:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1508508356424_-1782732318","id":"20171020-110218_1507019685","dateCreated":"2017-10-20T14:05:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:176","user":"anonymous","dateFinished":"2017-10-20T14:19:14+0000","dateStarted":"2017-10-20T14:19:14+0000"},{"text":"%pyspark\n\nstopwords_english = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import RegexTokenizer, CountVectorizer, StopWordsRemover\n\n# PARAMETER - regex tokenization\ntokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\p{L}{3,}\", gaps=False)\nstopwordRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\",stopWords=stopwords_english, caseSensitive=False)\n\n# PARAMETER - vocab size, min and max doc frequency\ncv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vec\",vocabSize=100000, minDF=50)\n\npipeline = Pipeline(stages=[tokenizer, stopwordRemover, cv])\n\nmodel = pipeline.fit(docs_en)\n\nvecs = model.transform(docs_en)\n\nvecs.show()","dateUpdated":"2017-10-20T14:19:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+\n|              domain|                 url|lang|                text|               words|            filtered|                 vec|\n+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+\n|        777parts.net|http://777parts.n...|  en|3069380 INJECTOR ...|[injector, group,...|[injector, group,...|(100000,[0,2,3,5,...|\n|     android-zone.ws|http://android-zo...|  en|File Commander - ...|[file, commander,...|[file, commander,...|(100000,[0,1,3,4,...|\n|babynames.allpare...|http://babynames....|  en|Meaning of Spence...|[meaning, spencer...|[meaning, spencer...|(100000,[1,4,5,6,...|\n| blog.jarofjuice.com|http://blog.jarof...|  en|riyadh-sandstorm-...|[riyadh, sandstor...|[riyadh, sandstor...|(100000,[0,4,18,4...|\n|bumpersuperstore.com|http://bumpersupe...|  en|N-Fab C11115CC Ne...|[fab, nerf, step,...|[fab, nerf, step,...|(100000,[0,1,2,4,...|\n|  chaturbateplus.com|http://chaturbate...|  en|Watch kissenpussy...|[watch, kissenpus...|[watch, kissenpus...|(100000,[1,3,6,33...|\n|content.usatoday.com|http://content.us...|  en|USATODAY.com\nYour...|[usatoday, com, y...|[usatoday, com, b...|(100000,[1,98,477...|\n|datazone.birdlife...|http://datazone.b...|  en|Hill Partridge (A...|[hill, partridge,...|[hill, partridge,...|(100000,[2,5,10,1...|\n|dlkcollection.blo...|http://dlkcollect...|  en|DLK COLLECTION: A...|[dlk, collection,...|[dlk, collection,...|(100000,[0,1,2,3,...|\n|     en-kw.sivvi.com|http://en-kw.sivv...|  en|Pastel Neon Maxi ...|[pastel, neon, ma...|[pastel, neon, ma...|(100000,[0,1,2,3,...|\n|fathersfuckdaught...|http://fathersfuc...|  en|Fathers Fuck Daug...|[fathers, fuck, d...|[fathers, fuck, d...|(100000,[1,6,13,1...|\n|forums.macrumors.com|http://forums.mac...|  en|Does apple use wi...|[does, apple, use...|[apple, use, wind...|(100000,[0,1,3,4,...|\n|          gfxhome.co|http://gfxhome.co...|  en|﻿\nThe Big Issue -...|[the, big, issue,...|[big, issue, may,...|(100000,[0,1,2,3,...|\n|healthlibrary.neb...|http://healthlibr...|  en|Acoustic Neuroma ...|[acoustic, neurom...|[acoustic, neurom...|(100000,[0,3,5,8,...|\n|           icecat.us|http://icecat.us/...|  en|Product data Micr...|[product, data, m...|[product, data, m...|(100000,[0,2,5,6,...|\n|       italiensee.de|http://italiensee...|  en|christian loubout...|[christian, loubo...|[christian, loubo...|(100000,[0,3,7,9,...|\n|         keyxnxx.com|http://keyxnxx.co...|  en|College games lov...|[college, games, ...|[college, games, ...|(100000,[1,6,7,14...|\n|        liherald.com|http://liherald.c...|  en|Rockville Centre ...|[rockville, centr...|[rockville, centr...|(100000,[1,3,5,6,...|\n|        mangable.com|http://mangable.c...|  en|Sekainohate de Ai...|[sekainohate, aim...|[sekainohate, aim...|(100000,[1,3,7,9,...|\n|        mobile17.com|http://mobile17.c...|  en|Ringtones | Bojan...|[ringtones, bojan...|[ringtones, bojan...|(100000,[7,10,11,...|\n+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1508508356425_-1783117067","id":"20171020-101427_882065123","dateCreated":"2017-10-20T14:05:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:177","user":"anonymous","dateFinished":"2017-10-20T14:39:51+0000","dateStarted":"2017-10-20T14:19:18+0000"},{"text":"%pyspark\n\n#Run the topic modelling\n\nfrom pyspark.ml.clustering import LDA\n#inputCol=\"vec\", outputCol=\"ldaVec\", k=3, optimizer=\"online\"\n\nlda = LDA(k=300, maxIter=100, featuresCol=\"vec\")\nldaModel = lda.fit(vecs)\nprint(lda.isDistributed())\n","dateUpdated":"2017-10-20T15:02:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 3, in <module>\n  File \"/usr/lib/spark/python/pyspark/ml/base.py\", line 64, in fit\n    return self._fit(dataset)\n  File \"/usr/lib/spark/python/pyspark/ml/wrapper.py\", line 265, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/usr/lib/spark/python/pyspark/ml/wrapper.py\", line 262, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o354.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 5 tasks (1150.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1690)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1678)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1677)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1677)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1905)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1849)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.submitMiniBatch(LDAOptimizer.scala:479)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.next(LDAOptimizer.scala:449)\n\tat org.apache.spark.mllib.clustering.OnlineLDAOptimizer.next(LDAOptimizer.scala:262)\n\tat org.apache.spark.mllib.clustering.LDA.run(LDA.scala:336)\n\tat org.apache.spark.ml.clustering.LDA.fit(LDA.scala:912)\n\tat org.apache.spark.ml.clustering.LDA.fit(LDA.scala:814)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]},"apps":[],"jobName":"paragraph_1508508356425_-1783117067","id":"20171020-101446_892258643","dateCreated":"2017-10-20T14:05:56+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:178","user":"anonymous","dateFinished":"2017-10-20T14:56:08+0000","dateStarted":"2017-10-20T14:40:32+0000"},{"text":"%pyspark\n\n#Save the models\n\nldaModel.save('s3://billsdata.net/CommonCrawl/topic_model_64files/ldamodel')\npipeline.save('s3://billsdata.net/CommonCrawl/topic_model_64files/textpipeline')","dateUpdated":"2017-10-20T14:10:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1508508356426_-1781962820","id":"20171020-124001_1609701105","dateCreated":"2017-10-20T14:05:56+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:179","user":"anonymous","dateFinished":"2017-10-20T14:19:09+0000","dateStarted":"2017-10-20T14:19:09+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'ldaModel' is not defined\n\n"}]}},{"text":"%pyspark\n\n# Get topic vectors for index pages (estimate of topic vec per domain)\n\nvecs_index = vecs.filter(\"url LIKE '%index.html'\")\nresults = ldaModel.transform(vecs_index)\n\n# Drop text cols\nresults2=results.drop('text').drop('words').drop('filtered')\n\n# Save domain topic vecs\nresults2.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_64files/cc_index_page_topics.parquet')","dateUpdated":"2017-10-20T14:10:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'vecs' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1508508356427_-1782347569","id":"20171020-124102_343731757","dateCreated":"2017-10-20T14:05:56+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:180","user":"anonymous","dateFinished":"2017-10-20T14:19:09+0000","dateStarted":"2017-10-20T14:19:09+0000"},{"text":"%pyspark\n\n# Create a dataset containing just the host, url and top 3 topic labels & scores\n\nimport pandas as pd\ntopicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5).collect()\nvocab = model.stages[2].vocabulary\n\ntopic_labels = []\nfor i, (topic, terms, termWeights) in enumerate(topicIndices):\n    topwords = pd.Series(dict(zip([vocab[t] for t in terms], termWeights))).sort_values(ascending=False)\n    topic_labels.append('_'.join(topwords.index.values))\n\ntopic_labels = np.array(topic_labels)\n\ndef topTopics(x):\n    labels = topic_labels[np.argsort(x.topicDistribution)[::-1][:3]]\n    scores = np.sort(x.topicDistribution)[::-1][:3]\n    return (x.domain, x.url, str(labels[0]), float(scores[0]), str(labels[1]), float(scores[1]), str(labels[2]), float(scores[2]))\n\nresults3 = results2.rdd.map(topTopics)\nresults3 = results3.toDF([\"host\", \"url\", \"topic1\", \"score1\", \"topic2\", \"score2\", \"topic3\", \"score3\"])\n\nresults3.write.parquet('s3://billsdata.net/CommonCrawl/topic_model_64files/cc_index_page_topic_labels.parquet')\nresults3.show()","dateUpdated":"2017-10-20T14:10:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2850149162578674029.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named pandas\n\n"}]},"apps":[],"jobName":"paragraph_1508508356427_-1782347569","id":"20171020-131815_2006247238","dateCreated":"2017-10-20T14:05:56+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:181","user":"anonymous","dateFinished":"2017-10-20T14:19:09+0000","dateStarted":"2017-10-20T14:19:09+0000"},{"text":"%pyspark\n\n","dateUpdated":"2017-10-20T14:08:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1508508356427_-1782347569","id":"20171020-101912_1610139389","dateCreated":"2017-10-20T14:05:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:182","user":"anonymous"}],"name":"Tom 1 Topic Modelling","id":"2CWT73WX3","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}