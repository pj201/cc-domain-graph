{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to demonstrate evaluation of CommonCrawl-derived domain vectors by using them to\n# classify domains according to high-level topic in the DMOZ dataset. Currently configured to load\n# Bill's domain hex feature vectors from the 'Bill 6' notebook, and to use only Pyspark APIs and spark.ML.\n# All cells should complete in less than a few minutes on an m4.2xlarge cluster.\n# End-to-end run-time: approx 30 mins, with nfiles=128.\n# NOTE: Should we really be trying to predict domain links instead? Or predicting bad domains from a known dictionary?\n# PJ - 20 Sept 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Import the DMOZ domain category dataset as an RDD\n# (downloaded from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OMV93V)\n\ndmoz_labels=sc.textFile('s3://billsdata.net/CommonCrawl/DMOZ/dmoz_domain_category.csv')\nheader = dmoz_labels.first() # extract header\ndmoz_labels = dmoz_labels.filter(lambda row: row != header).map(lambda row: row.replace('\"','').split(',',1)) # remove header row, quotes and split on (only the first) comma\ndmoz_labels.take(3)","user":"anonymous","dateUpdated":"2017-09-20T15:45:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[[u'sdcastroverde.com', u'Top/World/Galego/regional/Galicia/Lugo/municipalities/Castroverde'], [u'www.232analyzer.com', u'Top/Computers/Hardware/Test_Equipment/Analyzers'], [u'zschachwitz-tischtennis.de', u'Top/World/Deutsch/Sport/ball_Sports/table_tennis/Teams/Germany/Saxony']]\n"}]},"apps":[],"jobName":"paragraph_1505897986189_1695561000","id":"20170908-135610_63929491","dateCreated":"2017-09-20T08:59:46+0000","dateStarted":"2017-09-20T14:51:02+0000","dateFinished":"2017-09-20T14:51:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:697"},{"text":"%pyspark\n\n# Convert our labels RDD into a Spark DataFrame with a schema - neither column can be Null\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"categories\", StringType(), False)])\ndmoz_labels_df=spark.createDataFrame(dmoz_labels,schema)\ndmoz_labels_df.printSchema()\nprint(dmoz_labels_df.count())\ndmoz_labels_df.show(1)\ndmoz_labels_df.cache()","user":"anonymous","dateUpdated":"2017-09-20T14:51:19+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- domain: string (nullable = false)\n |-- categories: string (nullable = false)\n\n2488259\n+-----------------+--------------------+\n|           domain|          categories|\n+-----------------+--------------------+\n|sdcastroverde.com|Top/World/Galego/...|\n+-----------------+--------------------+\nonly showing top 1 row\n\nDataFrame[domain: string, categories: string]\n"}]},"apps":[],"jobName":"paragraph_1505898275676_697318205","id":"20170920-090435_466600213","dateCreated":"2017-09-20T09:04:35+0000","dateStarted":"2017-09-20T14:51:19+0000","dateFinished":"2017-09-20T14:51:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:698"},{"text":"%pyspark\n\n# Make a dictionary of short domains (removing www. prefix) to top-level category label, as per this page: http://dmoztools.net\nprefix=\"www.\"\ndmoz_labels_clean=dmoz_labels_df.rdd.map(lambda row: ((row['domain'][len(prefix):] if row['domain'].startswith(prefix) else row['domain']),\\\n                                                       row['categories'].split(\"/\")[1].split(\"|\")[0]))\ndmoz_labels_df.unpersist()\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"category\", StringType(), False)])\ndmoz_labels_clean_df=spark.createDataFrame(dmoz_labels_clean,schema)\ndmoz_labels_clean_df.show(2)\ndmoz_labels_clean_df.cache()","user":"anonymous","dateUpdated":"2017-09-20T14:55:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------+---------+\n|           domain| category|\n+-----------------+---------+\n|sdcastroverde.com|    World|\n|  232analyzer.com|Computers|\n+-----------------+---------+\nonly showing top 2 rows\n\nDataFrame[domain: string, category: string]\n"}]},"apps":[],"jobName":"paragraph_1505897986194_1707488216","id":"20170908-144404_840192591","dateCreated":"2017-09-20T08:59:46+0000","dateStarted":"2017-09-20T14:55:57+0000","dateFinished":"2017-09-20T14:55:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:699"},{"text":"%pyspark\n\n# Summarize categories in the DMOZ data\ndmoz_labels_clean_df.groupBy('category').count().show()","user":"anonymous","dateUpdated":"2017-09-20T14:53:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-------+\n|  category|  count|\n+----------+-------+\n|Recreation|  46095|\n|     World|1273970|\n|   Science|  28138|\n|      Home|   6952|\n| Computers|  45194|\n|    Sports|  34890|\n|    Health|  24218|\n|   Society|  82079|\n|  Shopping|  54062|\n| Reference|  21663|\n|     Games|  10246|\n|      Arts|  66721|\n|  Business| 148144|\n|  Regional| 642176|\n|      News|   3711|\n+----------+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1505897986195_1707103467","id":"20170908-150338_1273147128","dateCreated":"2017-09-20T08:59:46+0000","dateStarted":"2017-09-20T14:53:57+0000","dateFinished":"2017-09-20T14:54:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:700"},{"text":"%pyspark\n\n# Load Bill's domain feature vectors from s3, in the following format:\n# (u'www.angelinajolin.com', [4.30406509320417, 0.02702702702702703, 0.0, 0.13513513513513514, 0.0, 0.06756756756756757, 0.0])\n\nnfiles=128 # (takes about 5 mins for 128 files)\n\n# Load feature vectors from WAT files (from 'Bill 6' notebook) as an RDD:\ninputURI = \"s3://billsdata.net/CommonCrawl/domain_hex_feature_vectors_from_%d_WAT_files\" % nfiles\nfeatures_rdd = sc.textFile(inputURI).map(eval)\nimport pyspark.sql.types as typ\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"vector\", ArrayType(DoubleType(), False))])\nfeatures_df=spark.createDataFrame(features_rdd,schema)\nfeatures_df.cache()\nprint(\"Nr domains:\", features_df.count())\nprint(features_df.show(1))\nfeatures_df.printSchema()","user":"anonymous","dateUpdated":"2017-09-20T14:56:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('Nr domains:', 2626203)\n+-----------+--------------------+\n|     domain|              vector|\n+-----------+--------------------+\n|www.iggl.de|[3.63758615972638...|\n+-----------+--------------------+\nonly showing top 1 row\n\nNone\nroot\n |-- domain: string (nullable = false)\n |-- vector: array (nullable = true)\n |    |-- element: double (containsNull = false)\n\n"}]},"apps":[],"jobName":"paragraph_1505897986196_1705179723","id":"20170914-093439_1131502776","dateCreated":"2017-09-20T08:59:46+0000","dateStarted":"2017-09-20T14:56:07+0000","dateFinished":"2017-09-20T15:01:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:701"},{"text":"%pyspark\n\n# Spark.ML classifiers require VectorUDF type, rather than Array, so we need to convert\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nvectorize=udf(lambda vs: Vectors.dense(vs), VectorUDT())\nfeatures_df = features_df.withColumn(\"vec\", vectorize(features_df['vector'])).drop('vector')\nprint(features_df.show(1))\nfeatures_df.printSchema()\n","user":"anonymous","dateUpdated":"2017-09-20T15:06:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505915160429_829693877","id":"20170920-134600_1938001702","dateCreated":"2017-09-20T13:46:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3125","dateFinished":"2017-09-20T15:08:49+0000","dateStarted":"2017-09-20T15:06:48+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+--------------------+\n|     domain|                 vec|\n+-----------+--------------------+\n|www.iggl.de|[3.63758615972638...|\n+-----------+--------------------+\nonly showing top 1 row\n\nNone\nroot\n |-- domain: string (nullable = false)\n |-- vec: vector (nullable = true)\n\n"}]}},{"text":"%pyspark\n\n# Filter embeddings for only those vectors that have entries in the DMOZ dictionary (i.e. ground truth labels)\n#common_domains_df= features_df.join(dmoz_labels_clean_df, features_df.domain == dmoz_labels_clean_df.domain, how='inner') \ncommon_domains_df=features_df.join(dmoz_labels_clean_df, [\"domain\"]) # doesn't create extra column\ncommon_domains_df.cache()\nfeatures_df.unpersist()\ndmoz_labels_clean_df.unpersist()\nprint(\"Number of labelled domains = \" + str(common_domains_df.count()))\ncommon_domains_df.show(3)\ncommon_domains_df.printSchema()","user":"anonymous","dateUpdated":"2017-09-20T15:09:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Number of labelled domains = 57479\n+---------+--------------------+--------+\n|   domain|                 vec|category|\n+---------+--------------------+--------+\n|   1by.by|[4.74493212836325...|   World|\n|   360.ch|[5.42934562895444...|   World|\n|631la.com|[3.46573590279972...|   World|\n+---------+--------------------+--------+\nonly showing top 3 rows\n\nroot\n |-- domain: string (nullable = false)\n |-- vec: vector (nullable = true)\n |-- category: string (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1505897986198_1705949220","id":"20170914-122753_1677347656","dateCreated":"2017-09-20T08:59:46+0000","dateStarted":"2017-09-20T15:09:14+0000","dateFinished":"2017-09-20T15:16:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:703"},{"text":"%pyspark\n\n# Create numeric indexes for our classes\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nlabelIndexer = StringIndexer(inputCol=\"category\", outputCol=\"indexedCategory\").fit(common_domains_df)\n\n# Split into training and test sets using spark.ML API\ndomains_train, domains_test = common_domains_df.randomSplit([0.7,0.3],seed=42)\ndomains_test.groupBy('category').count().show()","user":"anonymous","dateUpdated":"2017-09-20T15:32:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+\n|  category|count|\n+----------+-----+\n|Recreation|  419|\n|     World| 6469|\n|   Science|  564|\n|      Home|  211|\n| Computers| 1168|\n|    Sports|  400|\n|    Health|  233|\n|   Society| 1016|\n|  Shopping|  373|\n| Reference|  857|\n|     Games|  209|\n|      Arts|  928|\n|  Business|  795|\n|  Regional| 3460|\n|      News|  180|\n+----------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1505897986198_1705949220","id":"20170914-141248_1548737052","dateCreated":"2017-09-20T08:59:46+0000","dateStarted":"2017-09-20T15:32:13+0000","dateFinished":"2017-09-20T15:34:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:704"},{"text":"%pyspark\n\n# Create a pipeline and fit a RandomForest Classifier using spark.ml\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Build our RF classifier\nrf = RandomForestClassifier(labelCol=\"indexedCategory\", featuresCol=\"vec\", numTrees=10)\n\n# Convert indexed labels back to original labels\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedCategory\", labels=labelIndexer.labels)\n\n# Define and run the full pipeline to train the model and make predictions                               \npipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])\nmodel=pipeline.fit(domains_train)\npredictions=model.transform(domains_test)\nprint(predictions.take(1))\npredictions.select(\"predictedCategory\", \"category\", \"vec\").show(5)\n\n# FYI, Equivalent code in sklearn\n#from sklearn.ensemble import RandomForestClassifier\n#rf = RandomForestClassifier(max_depth=2, random_state=0)\n#rf.fit(X_train, y_train)\n#print(classification_report(y_test, rf.predict(X_test)))","user":"anonymous","dateUpdated":"2017-09-20T15:34:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(domain=u'360.ch', vec=DenseVector([5.4293, 0.5307, 0.0, 0.0, 0.0044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0131, 0.0, 0.0262, 0.0262, 0.0873, 0.0655, 0.0131, 0.0437, 0.1659, 0.0742, 0.0131, 0.0131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0044, 0.0, 0.0087, 0.0306, 0.0218, 0.0524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), category=u'World', indexedCategory=0.0, rawPrediction=DenseVector([3.9293, 1.8593, 0.6566, 0.6234, 0.5674, 0.4469, 0.4028, 0.3395, 0.2174, 0.236, 0.2129, 0.1511, 0.1064, 0.1276, 0.1233]), probability=DenseVector([0.3929, 0.1859, 0.0657, 0.0623, 0.0567, 0.0447, 0.0403, 0.034, 0.0217, 0.0236, 0.0213, 0.0151, 0.0106, 0.0128, 0.0123]), prediction=0.0, predictedCategory=u'World')]\n+-----------------+--------+--------------------+\n|predictedCategory|category|                 vec|\n+-----------------+--------+--------------------+\n|            World|   World|[5.42934562895444...|\n|            World|   World|[3.46573590279972...|\n|            World|   World|[4.49980967033026...|\n|            World| Science|[5.78689738136670...|\n|            World|   World|[6.03548143252475...|\n+-----------------+--------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1505897986200_1703640727","id":"20170914-141640_1522668331","dateCreated":"2017-09-20T08:59:46+0000","dateStarted":"2017-09-20T15:34:27+0000","dateFinished":"2017-09-20T15:39:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:706"},{"text":"%pyspark\n\n# Select (prediction, true label) and compute test error\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator1 = MulticlassClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator2 = MulticlassClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"f1\")\n\naccuracy = evaluator1.evaluate(predictions)\nf1=evaluator2.evaluate(predictions)\n\nprint(\"Accuracy=%g, F1=%g\" % (accuracy, f1))","user":"anonymous","dateUpdated":"2017-09-20T15:40:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505911768282_-1486907658","id":"20170920-124928_1782886854","dateCreated":"2017-09-20T12:49:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1657","dateFinished":"2017-09-20T15:48:24+0000","dateStarted":"2017-09-20T15:40:24+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Accuracy=0.37432, F1=0.203905\n"}]}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-09-20T11:06:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505897986201_1703255978","id":"20170914-141713_903756751","dateCreated":"2017-09-20T08:59:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:708"}],"name":"201709 evaluate CC vectors","id":"2CW37U4P7","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}