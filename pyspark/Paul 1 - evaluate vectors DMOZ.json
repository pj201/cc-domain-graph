{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to demonstrate evaluation of CommonCrawl-derived domain vectors by using them to\n# classify domains according to high-level topic in the DMOZ dataset. Currently configured to load\n# Bill's domain hex feature vectors from the 'Bill 6' notebook, and to use only Pyspark APIs and spark.ML.\n# All cells should complete in less than a few minutes on an m4.2xlarge cluster.\n# End-to-end run-time: approx 30 mins, with nfiles=128.\n# NOTE: Should we really be trying to predict domain links instead? Or predicting bad domains from a known dictionary?\n# PJ - 20 Sept 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Import the DMOZ domain category dataset as an RDD\n# (downloaded from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OMV93V)\n\ndmoz_labels=sc.textFile('s3://billsdata.net/CommonCrawl/DMOZ/dmoz_domain_category.csv')\nheader = dmoz_labels.first() # extract header\ndmoz_labels = dmoz_labels.filter(lambda row: row != header).map(lambda row: row.replace('\"','').split(',',1)) # remove header row, quotes and split on (only the first) comma\ndmoz_labels.take(3)","user":"anonymous","dateUpdated":"2017-10-13T15:19:12+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[[u'sdcastroverde.com', u'Top/World/Galego/regional/Galicia/Lugo/municipalities/Castroverde'], [u'www.232analyzer.com', u'Top/Computers/Hardware/Test_Equipment/Analyzers'], [u'zschachwitz-tischtennis.de', u'Top/World/Deutsch/Sport/ball_Sports/table_tennis/Teams/Germany/Saxony']]\n"}]},"apps":[],"jobName":"paragraph_1507907936115_-826158500","id":"20170908-135610_63929491","dateCreated":"2017-10-13T15:18:56+0000","dateStarted":"2017-10-13T15:19:12+0000","dateFinished":"2017-10-13T15:19:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4247"},{"text":"%pyspark\n\n# Convert our labels RDD into a Spark DataFrame with a schema - neither column can be Null\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"categories\", StringType(), False)])\ndmoz_labels_df=spark.createDataFrame(dmoz_labels,schema)\ndmoz_labels_df.printSchema()\nprint(dmoz_labels_df.count())\ndmoz_labels_df.show(1)\ndmoz_labels_df.cache()","user":"anonymous","dateUpdated":"2017-10-13T15:19:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- domain: string (nullable = false)\n |-- categories: string (nullable = false)\n\n2488259\n+-----------------+--------------------+\n|           domain|          categories|\n+-----------------+--------------------+\n|sdcastroverde.com|Top/World/Galego/...|\n+-----------------+--------------------+\nonly showing top 1 row\n\nDataFrame[domain: string, categories: string]\n"}]},"apps":[],"jobName":"paragraph_1507907936116_-828082245","id":"20170920-090435_466600213","dateCreated":"2017-10-13T15:18:56+0000","dateStarted":"2017-10-13T15:19:17+0000","dateFinished":"2017-10-13T15:19:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4248"},{"text":"%pyspark\n\n# Make a dictionary of short domains (removing www. prefix) to top-level category label, as per this page: http://dmoztools.net\nprefix=\"www.\"\ndmoz_labels_clean=dmoz_labels_df.rdd.map(lambda row: ((row['domain'][len(prefix):] if row['domain'].startswith(prefix) else row['domain']),\\\n                                                       row['categories'].split(\"/\")[1].split(\"|\")[0]))\ndmoz_labels_df.unpersist()\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"category\", StringType(), False)])\ndmoz_labels_clean_df=spark.createDataFrame(dmoz_labels_clean,schema)\ndmoz_labels_clean_df.show(2)\ndmoz_labels_clean_df.cache()","user":"anonymous","dateUpdated":"2017-10-13T15:19:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------+---------+\n|           domain| category|\n+-----------------+---------+\n|sdcastroverde.com|    World|\n|  232analyzer.com|Computers|\n+-----------------+---------+\nonly showing top 2 rows\n\nDataFrame[domain: string, category: string]\n"}]},"apps":[],"jobName":"paragraph_1507907936116_-828082245","id":"20170908-144404_840192591","dateCreated":"2017-10-13T15:18:56+0000","dateStarted":"2017-10-13T15:19:20+0000","dateFinished":"2017-10-13T15:19:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4249"},{"text":"%pyspark\n\n# Summarize categories in the DMOZ data\ndmoz_labels_clean_df.groupBy('category').count().show()","user":"anonymous","dateUpdated":"2017-10-13T15:19:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-------+\n|  category|  count|\n+----------+-------+\n|Recreation|  46095|\n|     World|1273970|\n|   Science|  28138|\n|      Home|   6952|\n| Computers|  45194|\n|    Sports|  34890|\n|    Health|  24218|\n|   Society|  82079|\n|  Shopping|  54062|\n| Reference|  21663|\n|     Games|  10246|\n|      Arts|  66721|\n|  Business| 148144|\n|  Regional| 642176|\n|      News|   3711|\n+----------+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1507907936116_-828082245","id":"20170908-150338_1273147128","dateCreated":"2017-10-13T15:18:56+0000","dateStarted":"2017-10-13T15:19:27+0000","dateFinished":"2017-10-13T15:19:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4250"},{"text":"%pyspark\n\n# Load domain feature vectors from s3, in the following format:\n# (u'www.angelinajolin.com', [4.30406509320417, 0.02702702702702703, 0.0, 0.13513513513513514, 0.0, 0.06756756756756757, 0.0])\n\n# Load feature vectors from WAT files (from 'Bill 6' notebook) as an RDD:\n#nfiles=128 # (takes about 5 mins for 128 files)\n#inputURI = \"s3://billsdata.net/CommonCrawl/domain_hex_feature_vectors_from_%d_WAT_files\" % nfiles\n\n# Load Tom V's LDA topic-based vectors\ninputURI = \"s3://billsdata.net/CommonCrawl/lda_topic_vectors/tom_lda_vecs_sample3.txt\"\n\nfeatures_rdd = sc.textFile(inputURI).map(eval)\nfeatures_rdd.take(3)\nimport pyspark.sql.types as typ\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"vector\", ArrayType(DoubleType(), False))])\nfeatures_df=spark.createDataFrame(features_rdd,schema)\nfeatures_df.cache()\nprint(\"Nr domains:\", features_df.count())\nprint(features_df.show(1))\nfeatures_df.printSchema()","user":"anonymous","dateUpdated":"2017-10-13T15:33:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('Nr domains:', 9763)\n+--------------------+--------------------+\n|              domain|              vector|\n+--------------------+--------------------+\n|10kidsin2010.blog...|[6.80862914E-5, 0...|\n+--------------------+--------------------+\nonly showing top 1 row\n\nNone\nroot\n |-- domain: string (nullable = false)\n |-- vector: array (nullable = true)\n |    |-- element: double (containsNull = false)\n\n"}]},"apps":[],"jobName":"paragraph_1507907936117_-828466994","id":"20170914-093439_1131502776","dateCreated":"2017-10-13T15:18:56+0000","dateStarted":"2017-10-13T15:33:31+0000","dateFinished":"2017-10-13T15:33:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4251"},{"text":"%pyspark\n\n# Spark.ML classifiers require VectorUDF type, rather than Array, so we need to convert\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import udf\n\nvectorize=udf(lambda vs: Vectors.dense(vs), VectorUDT())\nfeatures_df = features_df.withColumn(\"vec\", vectorize(features_df['vector'])).drop('vector')\nprint(features_df.show(1))\nfeatures_df.printSchema()\n","user":"anonymous","dateUpdated":"2017-10-13T15:34:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|              domain|                 vec|\n+--------------------+--------------------+\n|10kidsin2010.blog...|[6.80862914E-5,0....|\n+--------------------+--------------------+\nonly showing top 1 row\n\nNone\nroot\n |-- domain: string (nullable = false)\n |-- vec: vector (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1507907936117_-828466994","id":"20170920-134600_1938001702","dateCreated":"2017-10-13T15:18:56+0000","dateStarted":"2017-10-13T15:34:51+0000","dateFinished":"2017-10-13T15:34:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4252"},{"text":"%pyspark\n\n# Filter embeddings for only those vectors that have entries in the DMOZ dictionary (i.e. ground truth labels)\n#common_domains_df= features_df.join(dmoz_labels_clean_df, features_df.domain == dmoz_labels_clean_df.domain, how='inner') \ncommon_domains_df=features_df.join(dmoz_labels_clean_df, [\"domain\"]) # doesn't create extra column\ncommon_domains_df.cache()\nfeatures_df.unpersist()\ndmoz_labels_clean_df.unpersist()\nprint(\"Number of labelled domains = \" + str(common_domains_df.count()))\ncommon_domains_df.show(3)\ncommon_domains_df.printSchema()","dateUpdated":"2017-10-13T15:34:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Number of labelled domains = 647\n+-----------------+--------------------+---------+\n|           domain|                 vec| category|\n+-----------------+--------------------+---------+\n|roanokeisland.com|[0.0599172335,4.5...| Regional|\n|       vietbao.vn|[1.04846403E-4,8....|    World|\n|     tv.adobe.com|[0.472358937,4.30...|Computers|\n+-----------------+--------------------+---------+\nonly showing top 3 rows\n\nroot\n |-- domain: string (nullable = false)\n |-- vec: vector (nullable = true)\n |-- category: string (nullable = false)\n\n"}]},"apps":[],"jobName":"paragraph_1507907936117_-828466994","id":"20170914-122753_1677347656","dateCreated":"2017-10-13T15:18:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4253","user":"anonymous","dateFinished":"2017-10-13T15:35:15+0000","dateStarted":"2017-10-13T15:34:59+0000"},{"text":"%pyspark\n\n# Create numeric indexes for our classes\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nlabelIndexer = StringIndexer(inputCol=\"category\", outputCol=\"indexedCategory\").fit(common_domains_df)\n\n# Split into training and test sets using spark.ML API\ndomains_train, domains_test = common_domains_df.randomSplit([0.7,0.3],seed=42)\ndomains_test.groupBy('category').count().show()","dateUpdated":"2017-10-13T15:35:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+\n|  category|count|\n+----------+-----+\n|Recreation|    7|\n|     World|   12|\n|   Science|    9|\n|      Home|    5|\n| Computers|   24|\n|    Sports|   11|\n|    Health|    5|\n|   Society|   16|\n|  Shopping|    6|\n| Reference|   13|\n|     Games|    5|\n|      Arts|   20|\n|  Business|   10|\n|  Regional|   35|\n|      News|    9|\n+----------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1507907936118_-827312747","id":"20170914-141248_1548737052","dateCreated":"2017-10-13T15:18:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4254","user":"anonymous","dateFinished":"2017-10-13T15:35:51+0000","dateStarted":"2017-10-13T15:35:22+0000"},{"text":"%pyspark\n\n# Create a pipeline and fit a RandomForest Classifier using spark.ml\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Build our RF classifier\nrf = RandomForestClassifier(labelCol=\"indexedCategory\", featuresCol=\"vec\", numTrees=10)\n\n# Convert indexed labels back to original labels\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedCategory\", labels=labelIndexer.labels)\n\n# Define and run the full pipeline to train the model and make predictions                               \npipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])\nmodel=pipeline.fit(domains_train)\npredictions=model.transform(domains_test)\nprint(predictions.take(1))\npredictions.select(\"predictedCategory\", \"category\", \"vec\").show(5)\n\n# FYI, Equivalent code in sklearn\n#from sklearn.ensemble import RandomForestClassifier\n#rf = RandomForestClassifier(max_depth=2, random_state=0)\n#rf.fit(X_train, y_train)\n#print(classification_report(y_test, rf.predict(X_test)))","dateUpdated":"2017-10-13T15:37:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(domain=u'411.info', vec=DenseVector([0.7023, 0.001, 0.0011, 0.001, 0.001, 0.0011, 0.2896, 0.001, 0.0009, 0.001]), category=u'Reference', indexedCategory=5.0, rawPrediction=DenseVector([3.442, 0.2423, 1.3674, 1.9328, 0.23, 1.3183, 0.1013, 0.3289, 0.1688, 0.3291, 0.1651, 0.144, 0.0455, 0.0911, 0.0935]), probability=DenseVector([0.3442, 0.0242, 0.1367, 0.1933, 0.023, 0.1318, 0.0101, 0.0329, 0.0169, 0.0329, 0.0165, 0.0144, 0.0045, 0.0091, 0.0094]), prediction=0.0, predictedCategory=u'Regional')]\n+-----------------+---------+--------------------+\n|predictedCategory| category|                 vec|\n+-----------------+---------+--------------------+\n|         Regional|Reference|[0.70233998,0.001...|\n|         Regional|     Arts|[0.167306443,1.85...|\n|         Regional| Regional|[0.868170457,1.52...|\n|         Regional|Reference|[0.241388187,2.36...|\n|         Regional| Business|[0.417959319,0.04...|\n+-----------------+---------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1507907936118_-827312747","id":"20170914-141640_1522668331","dateCreated":"2017-10-13T15:18:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4255","user":"anonymous","dateFinished":"2017-10-13T15:38:42+0000","dateStarted":"2017-10-13T15:37:04+0000"},{"text":"%pyspark\n\n# Select (prediction, true label) and compute test error\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator1 = MulticlassClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator2 = MulticlassClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"f1\")\n\naccuracy = evaluator1.evaluate(predictions)\nf1=evaluator2.evaluate(predictions)\n\nprint(\"Accuracy=%g, F1=%g\" % (accuracy, f1))","dateUpdated":"2017-10-13T15:38:48+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Accuracy=0.213904, F1=0.131569\n"}]},"apps":[],"jobName":"paragraph_1507907936118_-827312747","id":"20170920-124928_1782886854","dateCreated":"2017-10-13T15:18:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4256","user":"anonymous","dateFinished":"2017-10-13T15:40:00+0000","dateStarted":"2017-10-13T15:38:48+0000"},{"text":"%pyspark\n","dateUpdated":"2017-10-13T15:18:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507907936118_-827312747","id":"20170914-141713_903756751","dateCreated":"2017-10-13T15:18:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4257"}],"name":"201709 evaluate CC vectors","id":"2CWN2SWTF","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}