{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to enrich domain summaries (from Paul 5) with examples (from Paul 6)\n# and topic metadata (from Tom 1)\n# PJ - 27 November 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\nsummary_df.unpersist()\n# Load Domain Summaries DF in Gzip files (from Paul 5)\nloadURL=\"s3://billsdata.net/CommonCrawl/domain_summaries_withexamples4/\" # Split into 20 files (as opposed to domain_summaries7, which is 10)\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\nsummary_df=spark.read.format('com.databricks.spark.csv').options(header='true', codec=codec).load(loadURL)\nsummary_df.show(3)\nsummary_df.cache()\nsummary_df.rdd.getNumPartitions()","user":"anonymous","dateUpdated":"2017-11-27T09:53:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------+--------+-------------+----------+-----------+--------------+------+------+---------------+--------------------+------------------+\n| payLevelDomain|numHosts|pldIsHostFlag|pldLinksIn|pldLinksOut|wasCrawledFlag|hcRank|prRank|   exampleHosts|   exampleInLinkPlds|exampleOutLinkPlds|\n+---------------+--------+-------------+----------+-----------+--------------+------+------+---------------+--------------------+------------------+\n|      0-0-2.biz|       1|         true|         2|       null|         false|38.302|17.903|      0-0-2.biz|beyondwhois.com, ...|              None|\n|0-0-24.xn--p1ai|       1|         true|         1|       null|         false|40.316|21.320|0-0-24.xn--p1ai|             yapl.ru|              None|\n|       0-0-8.eu|       1|         true|         1|       null|         false|32.967|17.884|       0-0-8.eu|     beyondwhois.com|              None|\n+---------------+--------+-------------+----------+-----------+--------------+------+------+---------------+--------------------+------------------+\nonly showing top 3 rows\n\n40\n"}]},"apps":[],"jobName":"paragraph_1511773425536_440775927","id":"20171023-080924_1586412256","dateCreated":"2017-11-27T09:03:45+0000","dateStarted":"2017-11-27T09:53:58+0000","dateFinished":"2017-11-27T09:53:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3938"},{"text":"%pyspark\n\n# Load examples dataframe (from Paul 6)\nexamplesURI=\"s3://billsdata.net/CommonCrawl/domain_examples4/\"\nexample_df=spark.read.load(examplesURI)\nexample_df.show(10)\nexample_df.cache()\nexample_df.rdd.getNumPartitions()","dateUpdated":"2017-11-27T09:03:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+--------------------+\n|                 PLD|        exampleHosts|   exampleInLinkPlds|  exampleOutLinkPlds|\n+--------------------+--------------------+--------------------+--------------------+\n|0----------------...|[0---------------...|         [nomina.ru]|                null|\n|           0-0la.com|         [0-0la.com]|[jessicawilson.co...|[wordpress.org, g...|\n|           0-3-6.com|         [0-3-6.com]|[3d114.com, menok...|                null|\n|           0-3ani.ro|         [0-3ani.ro]|[cere.ro, adedir....|                null|\n|           0-5-1.com|         [0-5-1.com]|    [allthecom.info]|                null|\n|       0-60times.net|     [0-60times.net]|[lodekka.com, kev...|                null|\n|            0-744.cn|          [0-744.cn]|     [wordpress.com]|                null|\n|0-ads-free-web-pa...|[0-ads-free-web-p...|[list-of-domains....|                null|\n|       0-artlove.net|     [0-artlove.net]|[list-of-domains....|                null|\n|  0-clubpenguin-0.tk|[0-clubpenguin-0.tk]|  [similarsites.com]|                null|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n256\n"}]},"apps":[],"jobName":"paragraph_1511773425539_441160676","id":"20171103-135448_580425054","dateCreated":"2017-11-27T09:03:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3939"},{"text":"%pyspark\n\n# Remove square brackets from list output and remove Nulls\nfrom pyspark.sql.functions import udf\ndef remove_brackets(egs):\n    return str(egs).replace('[', '').replace(']', '')\nprint(remove_brackets(\"[bla, bla]\"))\nudf_remove_brackets = udf(remove_brackets, StringType())\n\nexample_df2=example_df.filter(example_df.PLD.isNotNull()).withColumn(\"tmp\", udf_remove_brackets(\"exampleOutLinkPlds\")).drop(\"exampleOutLinkPlds\").withColumnRenamed(\"tmp\",\"exampleOutLinkPlds\")\nexample_df3=example_df2.withColumn(\"tmp\", udf_remove_brackets(\"exampleInLinkPlds\")).drop(\"exampleInLinkPlds\").withColumnRenamed(\"tmp\",\"exampleInLinkPlds\")\nexample_df4=example_df3.withColumn(\"tmp\", udf_remove_brackets(\"exampleHosts\")).drop(\"exampleHosts\").withColumnRenamed(\"tmp\",\"exampleHosts\").select(\"PLD\",\"exampleHosts\",\"exampleInLinkPlds\",\"exampleOutLinkPlds\")\nexample_df4.show(10)\nexample_df4.count()\n","dateUpdated":"2017-11-27T09:03:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"bla, bla\n+--------------------+--------------------+--------------------+--------------------+\n|                 PLD|        exampleHosts|   exampleInLinkPlds|  exampleOutLinkPlds|\n+--------------------+--------------------+--------------------+--------------------+\n|0----------------...|0----------------...|           nomina.ru|                None|\n|           0-0la.com|           0-0la.com|jessicawilson.co....|wordpress.org, gm...|\n|           0-3-6.com|           0-3-6.com|3d114.com, menok....|                None|\n|           0-3ani.ro|           0-3ani.ro|cere.ro, adedir.info|                None|\n|           0-5-1.com|           0-5-1.com|      allthecom.info|                None|\n|       0-60times.net|       0-60times.net|lodekka.com, kevi...|                None|\n|            0-744.cn|            0-744.cn|       wordpress.com|                None|\n|0-ads-free-web-pa...|0-ads-free-web-pa...| list-of-domains.org|                None|\n|       0-artlove.net|       0-artlove.net| list-of-domains.org|                None|\n|  0-clubpenguin-0.tk|  0-clubpenguin-0.tk|    similarsites.com|                None|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n90839924\n"}]},"apps":[],"jobName":"paragraph_1511773425539_441160676","id":"20171103-141703_260872707","dateCreated":"2017-11-27T09:03:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3940"},{"text":"%pyspark\n\n# Join with Original summaries\n#example_df.unpersist()\n#example_summary_df=summary_df.join(example_df4, summary_df.payLevelDomain==example_df4.PLD).drop(\"PLD\")\n#summary_df.unpersist()\n#example_summary_df.dropDuplicates().sort(\"numHosts\",ascending=False).show(100)\nsummary_df.count()","user":"anonymous","dateUpdated":"2017-11-27T09:16:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3093619791748570289.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3093619791748570289.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 427, in count\n    return int(self._jdf.count())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o78.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 115 in stage 3.0 failed 4 times, most recent failure: Lost task 115.3 in stage 3.0 (TID 211, ip-172-31-38-66.eu-west-1.compute.internal, executor 3): java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(UnivocityParser.scala:194)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.parse(UnivocityParser.scala:191)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$5.apply(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$5.apply(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:312)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:312)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1690)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1678)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1677)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1677)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1905)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1849)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:278)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2430)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2429)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2429)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(UnivocityParser.scala:194)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.parse(UnivocityParser.scala:191)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$5.apply(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$5.apply(UnivocityParser.scala:308)\n\tat org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:312)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:312)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:100)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:92)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n"}]},"apps":[],"jobName":"paragraph_1511773425540_439236932","id":"20171103-135636_1283074493","dateCreated":"2017-11-27T09:03:45+0000","dateStarted":"2017-11-27T09:16:04+0000","dateFinished":"2017-11-27T09:17:12+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:3941"},{"text":"%pyspark\n\n# Save Example Summaries as GZIP files, approx 100MB each.\n#outURI=\"s3://billsdata.net/CommonCrawl/domain_summaries_withexamples4/\"\n#codec=\"org.apache.hadoop.io.compress.GzipCodec\"\n#example_summary_df.coalesce(40).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outURI) # Creates 94x23.8MB files by default\nprint(\"SAVE HERE FOR ONLY EXAMPLE SUMMARIES\")","dateUpdated":"2017-11-27T09:03:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"SAVE HERE FOR ONLY EXAMPLE SUMMARIES\n"}]},"apps":[],"jobName":"paragraph_1511773425540_439236932","id":"20171103-140215_1756777634","dateCreated":"2017-11-27T09:03:45+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3942"},{"text":"%pyspark\n\n# Load topic labels (from Tom 1)\nloadURL=\"s3://billsdata.net/CommonCrawl/topic_model_32768_files/cc_index_page_topic_labels/\" # TODO - complete with all files!\ntopic_df=spark.read.load(loadURL)\ntopic_df.show(3)\nprint(topic_df.count())\ntopic_df.cache()\ntopic_df.rdd.getNumPartitions()","dateUpdated":"2017-11-27T09:17:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n|             host|                 url|              topic1|            score1|              topic2|              score2|              topic3|              score3|\n+-----------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n|   allsizeweb.com|http://allsizeweb...|home_jul_new_cest...|0.3334087361447539|home_cushion_golf...|  0.2305628527052053|church_school_dir...|  0.1897973651904288|\n|ancestorguide.com|http://ancestorgu...|search_genealogy_...|0.9961374426189594|quotes_magazines_...|3.906793596073824E-5|one_new_like_time...|3.906692863996136...|\n|  archive.cor.org|http://archive.co...|one_new_like_time...|0.6437347327901176|information_cance...|  0.1712867757182201|one_home_like_lit...| 0.11463936819013947|\n+-----------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 3 rows\n\n945260\n250\n"}]},"apps":[],"jobName":"paragraph_1511773425541_438852183","id":"20171023-081217_1250969967","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3943","user":"anonymous","dateFinished":"2017-11-27T09:18:07+0000","dateStarted":"2017-11-27T09:17:56+0000"},{"text":"%pyspark\n\n# From Paul 5.\n\n# Load in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\n#pld_df.coalesce(64).write.save(saveURI) # Use all default options\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\n#print(pld_df.count()) # Should have 91M domains","dateUpdated":"2017-11-27T09:18:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\nDataFrame[ID: string, PLD: string]\n"}]},"apps":[],"jobName":"paragraph_1511773425541_438852183","id":"20171108-100011_1754193907","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3944","user":"anonymous","dateFinished":"2017-11-27T09:18:26+0000","dateStarted":"2017-11-27T09:18:22+0000"},{"text":"%pyspark\n\n# From Paul 5.\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when counting hosts\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.rdd.collect(): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\nprint(pld_df.rdd.take(3))\nprint(pld_df.rdd.take(3)[2]['PLD'])\n#pld_bf.add(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","dateUpdated":"2017-11-27T09:18:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(ID=u'0', PLD=u'aaa.a'), Row(ID=u'1', PLD=u'aaa.aa'), Row(ID=u'2', PLD=u'aaa.aaa')]\naaa.aaa\nTrue\n64\n90751305\nTrue\nFalse\nTrue\nFalse\n"}]},"apps":[],"jobName":"paragraph_1511773425541_438852183","id":"20171108-095927_1270326887","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3945","user":"anonymous","dateFinished":"2017-11-27T09:44:36+0000","dateStarted":"2017-11-27T09:18:34+0000"},{"text":"%pyspark\n\n# From Paul 5.\n\nfrom pyspark.sql.functions import udf\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n\nudf_convert_hostname = udf(convert_hostname, StringType())\n\n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","dateUpdated":"2017-11-27T09:47:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]},"apps":[],"jobName":"paragraph_1511773425541_438852183","id":"20171108-095830_940370411","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3946","user":"anonymous","dateFinished":"2017-11-27T09:47:15+0000","dateStarted":"2017-11-27T09:47:15+0000"},{"text":"%pyspark\n\n# Function to reverse hostnames\nfrom pyspark.sql.functions import udf\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook\"))\nudf_reverse_domain = udf(reverse_domain, StringType())\n\n# Convert hosts in Topic DF to PLDs using convert_hostname function from Paul 5.\ntopic_df2=topic_df.withColumn(\"pld\",udf_reverse_domain(udf_convert_hostname(udf_reverse_domain(\"host\")))) # Reverse the hostnames prior to lookup, then back again!\ntopic_df2.show(10)","dateUpdated":"2017-11-27T09:47:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"facebook.com\n+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|                host|                 url|              topic1|            score1|              topic2|              score2|              topic3|              score3|                 pld|\n+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|      allsizeweb.com|http://allsizeweb...|home_jul_new_cest...|0.3334087361447539|home_cushion_golf...|  0.2305628527052053|church_school_dir...|  0.1897973651904288|      allsizeweb.com|\n|   ancestorguide.com|http://ancestorgu...|search_genealogy_...|0.9961374426189594|quotes_magazines_...|3.906793596073824E-5|one_new_like_time...|3.906692863996136...|   ancestorguide.com|\n|     archive.cor.org|http://archive.co...|one_new_like_time...|0.6437347327901176|information_cance...|  0.1712867757182201|one_home_like_lit...| 0.11463936819013947|             cor.org|\n|         belmont.edu|http://belmont.ed...|health_research_s...|0.6416289147432386|abstract_universi...| 0.18741124677241397|home_http_decembe...| 0.16830550303785022|         belmont.edu|\n|     bunnyhollow.org|http://bunnyhollo...|one_new_like_time...|0.6567567002030557|photos_card_peopl...|  0.1852641544451361|tigers_new_one_ho...| 0.15577726475682582|     bunnyhollow.org|\n| carletonbaptist.org|http://carletonba...|one_new_like_time...|0.6338257675386012|church_school_dir...|  0.2723894038191887|porn_teen_sex_fre...| 0.04366310910537767| carletonbaptist.org|\n|creatinglandscape...|http://creatingla...|one_new_like_time...|0.7011104095872868|viagra_buy_new_ha...| 0.14546600688309536|new_news_video_ho...| 0.07889037170640825|creatinglandscape...|\n|curriculumatocee....|http://curriculum...|one_new_like_time...|0.5657433039479814|viagra_buy_new_ha...| 0.42007094067818834|quotes_magazines_...|1.449495487316212E-4|          weebly.com|\n|dispatch.typepad.com|http://dispatch.t...|sex_november_free...| 0.714359099198136|new_skf_one_time_...| 0.12030658888601956|viagra_buy_new_ha...| 0.10112400773348668|         typepad.com|\n|      flyinmyear.com|http://flyinmyear...|one_new_like_time...|0.5129297365481966|one_may_comments_...| 0.38350403029976443|xen_devel_names_d...| 0.06831560972575888|      flyinmyear.com|\n+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1511773425542_440006429","id":"20171023-081653_2145141554","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3947","user":"anonymous","dateFinished":"2017-11-27T09:47:50+0000","dateStarted":"2017-11-27T09:47:46+0000"},{"text":"%pyspark\n\n# If multiple rows per PLD, only use the one with the shortest URL.\ndef url_len(url):\n    return len(url)\nprint(url_len(\"com.facebook.bla\"))\nudf_url_len = udf(url_len, IntegerType())\nfrom pyspark.sql.functions import col, desc\ntopic_df3=topic_df2.withColumn(\"url_len\", udf_url_len(\"url\"))\n#topic_df3.show(30)\n\n# Use window functions to filter to only the row with the shortest URL for each PLD\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import rank, col\nwindow = Window.partitionBy(topic_df3['pld']).orderBy(topic_df3['url_len']) # Ascending order of URL length\ntopic_df4=topic_df3.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 1)\ntopic_df4.show(20)\n\n# TODO: Consider putting the three topic columns into a single column.","dateUpdated":"2017-11-27T09:48:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"16\n+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------+----+\n|                host|                 url|              topic1|             score1|              topic2|              score2|              topic3|              score3|                 pld|url_len|rank|\n+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------+----+\n|www.108divyadesam...|http://www.108div...|quotes_magazines_...| 0.5960511970669616|one_new_like_time...|   0.238302918414496|home_jul_new_cest...| 0.16046506000003882|   108divyadesam.org|     39|   1|\n|       1worldart.com|http://1worldart....|one_new_like_time...| 0.5816464041506926|milf_big_sex_vide...|  0.3275255789264762|linux_news_story_...| 0.07592288987113473|       1worldart.com|     34|   1|\n|       clfc.2itb.com|http://clfc.2itb....|linux_news_story_...| 0.5691525223795646|viagra_buy_new_ha...| 0.29267240173364856|file_new_pantyhos...| 0.09837755286729974|            2itb.com|     31|   1|\n| 3dstudioofdance.com|http://3dstudioof...|one_new_like_time...|0.45167691211960154|one_may_comments_...| 0.22249829419519343|quotes_magazines_...|   0.170511377764314| 3dstudioofdance.com|     37|   1|\n|      www.aatgwa.com|http://www.aatgwa...|die_der_messe_new...| 0.5275884734569005|case_university_n...|  0.4506593196120721|quotes_magazines_...|2.222580395038882E-4|          aatgwa.com|     32|   1|\n|     abglazing.co.uk|http://abglazing....|information_cance...|0.31015144515757487|linux_news_story_...| 0.20177094843447896|new_skf_one_time_...|  0.1655601360249711|     abglazing.co.uk|     33|   1|\n|        abject.co.uk|http://abject.co....|viagra_buy_new_ha...| 0.9883668807103371|quotes_magazines_...|1.176645419917233E-4|one_new_like_time...|1.176615081499055...|        abject.co.uk|     30|   1|\n|   aboproperties.com|http://abopropert...|quotes_magazines_...| 0.9951765727958441|one_new_like_time...|4.878607518554005E-5|viagra_buy_new_ha...|4.876597509981281E-5|   aboproperties.com|     35|   1|\n|    www.aboutflr.com|http://www.aboutf...|viagra_buy_new_ha...| 0.5124091181717997|one_new_like_time...|  0.4725232527676937|domain_dropping_n...|0.013908768061069145|        aboutflr.com|     40|   1|\n|www.aboutscotland...|http://www.abouts...|home_jul_new_cest...| 0.5877686439022313|one_time_law_like...|   0.226249538284614|one_home_like_lit...| 0.18388022093388207|   aboutscotland.com|     51|   1|\n|www.aboutscotland...|http://www.abouts...|home_jul_new_cest...| 0.5764491270179075|one_time_law_like...|   0.230016924487006|quotes_magazines_...| 0.19016997313568884|   aboutscotland.com|     51|   1|\n|accentartandframe...|http://accentarta...|one_may_comments_...| 0.7869842904335046|linux_news_story_...| 0.15543443063262327|quotes_magazines_...|5.883559871697778E-4|accentartandframe...|     49|   1|\n|     acecars4u.co.uk|http://acecars4u....|quotes_magazines_...| 0.4333941366754434|escorts_london_ce...| 0.29663188689870323|movies_defloratio...| 0.21800126989877586|     acecars4u.co.uk|     33|   1|\n|      aclcollect.com|http://aclcollect...|abstract_universi...| 0.5218417681176085|one_may_comments_...| 0.41728039102235587|escorts_london_ce...| 0.05190714230946218|      aclcollect.com|     32|   1|\n|      actdragons.org|http://actdragons...|one_new_like_time...| 0.5328770787493172|information_cance...|  0.4486544371933419|quotes_magazines_...|1.887088953919878...|      actdragons.org|     32|   1|\n|    adhikresorts.com|http://adhikresor...|information_cance...|0.47682611215723714|quotes_magazines_...| 0.20521581953367596|linux_news_story_...|  0.1282732776941214|    adhikresorts.com|     34|   1|\n|advance-engineeri...|http://advance-en...|information_cance...| 0.5398981430696999|skf_new_republic_...|   0.268708855572846|driver_research_o...| 0.18050711041742787|advance-engineeri...|     43|   1|\n|  advancedsilver.com|http://advancedsi...|var_white_functio...| 0.4033786946330825|health_research_s...| 0.21030902255517644|one_may_comments_...| 0.12083940533642275|  advancedsilver.com|     36|   1|\n| www.advcablebus.com|http://www.advcab...|abstract_universi...| 0.4476663404306233|viagra_buy_new_ha...|  0.2924060442954655|new_skf_one_time_...| 0.24839398242346133|     advcablebus.com|     37|   1|\n|            afda.org|http://afda.org/i...|linux_news_story_...|0.45134895717570506|new_said_would_ne...| 0.43514952752064395|search_genealogy_...| 0.10752104172328104|            afda.org|     26|   1|\n+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------+----+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1511773425542_440006429","id":"20171109-100957_801598180","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3948","user":"anonymous","dateFinished":"2017-11-27T09:48:40+0000","dateStarted":"2017-11-27T09:48:21+0000"},{"text":"%pyspark\n\n# Join on host/PLD\npld_df.unpersist()\ntopic_df2.unpersist()\ntopic_df3.unpersist()\n# PJ - 27/11 - previously was using example_summary_df\nenrich_summary_df=summary_df.join(topic_df4, summary_df.payLevelDomain==topic_df4.pld, \"leftOuter\")\\\n                            .drop(\"host\").drop(\"url\").drop(\"score1\").drop(\"score2\").drop(\"score3\").drop(\"url_len\").drop(\"rank\").drop(\"topic2\").drop(\"topic3\").drop(\"pld\")\\\n                            .withColumnRenamed(\"topic1\",\"exampleIndexPageTopics\").dropDuplicates()\nenrich_summary_df.sort(\"prRank\",ascending=False).show()\nprint(enrich_summary_df.count())","dateUpdated":"2017-11-27T10:03:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"+--------------------+--------+-------------+----------+-----------+--------------+-------+------+--------------------+--------------------+--------------------+----------------------+\n|      payLevelDomain|numHosts|pldIsHostFlag|pldLinksIn|pldLinksOut|wasCrawledFlag| hcRank|prRank|        exampleHosts|   exampleInLinkPlds|  exampleOutLinkPlds|exampleIndexPageTopics|\n+--------------------+--------+-------------+----------+-----------+--------------+-------+------+--------------------+--------------------+--------------------+----------------------+\n|           zwire.com|      21|         true|      4315|       null|         false| 99.999|99.999|zwire.com, banner...|blogspot.com, car...|                None|                  null|\n|technologyreview.com|      50|         true|     30701|      12951|          true| 99.999|99.999|technologyreview....|macworld.com.au, ...|unimelb.edu.au, u...|                  null|\n|         wrenkin.net|       2|         true|        48|        147|          true| 98.800|99.999|wrenkin.net, moot...|blog.cz, crushing...|news1130.com, vid...|                  null|\n|         eczanet.com|       2|         true|         6|          6|          true| 99.316|99.999|eczanet.com, indi...|googleapis.com, r...|googleapis.com, y...|                  null|\n|   the-tournament.jp|       2|         true|        18|         14|          true| 99.371|99.999|the-tournament.jp...|saitama-soccer.jp...|notsobad.jp, twit...|                  null|\n|freecsstemplates.org|       1|         true|     17668|       null|         false| 99.999|99.999|freecsstemplates.org|uczymypolskiego.a...|                None|                  null|\n|      scalescale.com|       1|         true|        68|       null|         false| 98.486|99.999|      scalescale.com|damianschwyrz.de,...|                None|                  null|\n|         taka.com.vn|       4|         true|        50|         34|          true| 99.908|99.999|taka.com.vn, new....|blogspot.com, imo...|seomoz.org, actio...|                  null|\n|            acco.org|       5|         true|       966|        302|          true| 99.997|99.999|acco.org, give.ac...|baptistcancercent...|emergingmed.com, ...|                  null|\n|     boogenstein.com|       1|         true|        22|       null|         false| 98.788|99.999|     boogenstein.com|andybudd.com, apo...|                None|                  null|\n|     wigsforkids.org|       1|         true|       627|         23|          true| 99.998|99.999|     wigsforkids.org|yahoo.com, hudson...|bfrb.org, paypal....|                  null|\n|       savegrace.com|       1|         true|        11|          1|          true| 99.945|99.999|       savegrace.com|blogspot.com, tom...|         thegocf.org|                  null|\n|       carepages.com|       9|         true|      2696|       2465|          true| 99.998|99.999|carepages.com, as...|jbxstones.com.br,...|adventurecastlesa...|                  null|\n|            wpfr.net|       1|         true|     19658|       3915|          true| 98.351|99.999|            wpfr.net|autisme-en-action...|kriesi.at, issuep...|                  null|\n|         webnode.com|   82595|         true|     51261|      58312|          true| 99.999|99.999|webnode.com, 007g...|htl-klu.at, pfarr...|santacruzuno.com....|                  null|\n|          redhat.com|     863|         true|     14747|       7330|          true|100.000|99.999|redhat.com, 108.r...|perito.inf.br, uc...|unimelb.edu.au, t...|  abstract_universi...|\n|       adcouncil.org|      46|         true|      1887|        476|          true| 99.994|99.999|adcouncil.org, 2m...|blogspot.com, car...|lwr.org, lungforc...|                  null|\n|     basseyworld.com|       1|         true|        26|          1|          true| 99.947|99.999|     basseyworld.com|typepad.com, blog...|       pikavippim.fi|                  null|\n|        etracker.com|       8|         true|     16931|        112|          true| 99.910|99.999|etracker.com, app...|getraenke-welt.at...|embis.de, alkim.d...|                  null|\n|          redhat.com|     863|         true|     12802|      10785|          true|100.000|99.999|redhat.com, 108.r...|perito.inf.br, uc...|unimelb.edu.au, t...|  abstract_universi...|\n+--------------------+--------+-------------+----------+-----------+--------------+-------+------+--------------------+--------------------+--------------------+----------------------+\nonly showing top 20 rows\n\n91032617\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3093619791748570289.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-3093619791748570289.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 9, in <module>\nNameError: name 'exampleIndexPageTopics' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1511773425542_440006429","id":"20171108-114205_1373138614","dateCreated":"2017-11-27T09:03:45+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:3949","user":"anonymous","dateFinished":"2017-11-27T10:02:32+0000","dateStarted":"2017-11-27T10:00:05+0000"},{"text":"%pyspark\n\n# Save Example Summaries as GZIP files, approx 100MB each.\noutURI=\"s3://billsdata.net/CommonCrawl/domain_summaries_withtopics4/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\nenrich_summary_df.coalesce(40).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outURI) # coalesce(40) creates 94x23.8MB files by default","dateUpdated":"2017-11-27T10:05:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511773425542_440006429","id":"20171023-082228_1219608625","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3950","user":"anonymous","dateFinished":"2017-11-27T10:08:40+0000","dateStarted":"2017-11-27T10:05:13+0000"},{"text":"%pyspark\n\nenrich_summary_df.filter(enrich_summary_df.exampleIndexPageTopics.isNotNull()).count()\n#help(enrich_summary_df.filter)","dateUpdated":"2017-11-27T10:03:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511773425542_440006429","id":"20171109-105109_2094006765","dateCreated":"2017-11-27T09:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3951","user":"anonymous","dateFinished":"2017-11-27T10:03:42+0000","dateStarted":"2017-11-27T10:03:24+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"155853\n"}]}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-11-27T09:58:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511776728471_-67971630","id":"20171127-095848_511383996","dateCreated":"2017-11-27T09:58:48+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5370"}],"name":"Paul 7 - enrich domain summary","id":"2D2PKAKPX","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}