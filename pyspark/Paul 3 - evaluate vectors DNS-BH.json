{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to evaluate CC domain feature vectors against the DNS-BH Malware\n# Domain Blocklist from this site: http://mirror1.malwaredomains.com \n# Specifically, the 'justdomains' file, which currently contains 31k 'bad' domains.\n# We train using some of these domains, and try to predict the rest from amongst all \n# the domains for which we have feature vectors.\n# PJ - 21 Sept 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Import the DNS-BH domain list as a DataFrame\nbh_schema=StructType([StructField(\"domain\", StringType(), False)])\ndns_bh=spark.read.csv('s3://billsdata.net/CommonCrawl/DNS-BH/justdomains.dms', header=False, schema=bh_schema)\ndns_bh.show(3)\nprint(\"Bad domains in DNS-BH: \" + str(dns_bh.count()))\ndns_bh.cache()","user":"anonymous","dateUpdated":"2017-09-21T12:19:26+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|              domain|\n+--------------------+\n|amazon.co.uk.secu...|\n|autosegurancabras...|\n|christianmensfell...|\n+--------------------+\nonly showing top 3 rows\n\nBad domains in DNS-BH: 31877\nDataFrame[domain: string]\n"}]},"apps":[],"jobName":"paragraph_1505995851538_82160413","id":"20170921-121051_780451163","dateCreated":"2017-09-21T12:10:51+0000","dateStarted":"2017-09-21T12:19:26+0000","dateFinished":"2017-09-21T12:26:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:29856"},{"text":"%pyspark\n\n# Load Bill's domain feature vectors from s3, in the following format:\n# (u'www.angelinajolin.com', [4.30406509320417, 0.02702702702702703, 0.0, 0.13513513513513514, 0.0, 0.06756756756756757, 0.0])\n\nnfiles=128 # (takes about 5 mins for 128 files)\n\n# Load feature vectors from WAT files (from 'Bill 6' notebook) as an RDD:\ninputURI = \"s3://billsdata.net/CommonCrawl/domain_hex_feature_vectors_from_%d_WAT_files\" % nfiles\nfeatures_rdd = sc.textFile(inputURI).map(eval)\nimport pyspark.sql.types as typ\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"vector\", ArrayType(DoubleType(), False))])\nfeatures_df=spark.createDataFrame(features_rdd,schema)\nfeatures_df.cache()\nprint(\"Nr domains:\", features_df.count())\nprint(features_df.show(1))\nfeatures_df.printSchema()\n","user":"anonymous","dateUpdated":"2017-09-21T14:47:19+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('Nr domains:', 2626203)\n+-----------+--------------------+\n|     domain|              vector|\n+-----------+--------------------+\n|www.iggl.de|[3.63758615972638...|\n+-----------+--------------------+\nonly showing top 1 row\n\nNone\nroot\n |-- domain: string (nullable = false)\n |-- vector: array (nullable = true)\n |    |-- element: double (containsNull = false)\n\n"}]},"apps":[],"jobName":"paragraph_1505996045266_1166660293","id":"20170921-121405_178585430","dateCreated":"2017-09-21T12:14:05+0000","dateStarted":"2017-09-21T14:47:19+0000","dateFinished":"2017-09-21T14:52:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29857"},{"text":"%pyspark\n\n# Spark.ML classifiers require VectorUDF type, rather than Array, so we need to convert\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import UserDefinedFunction\nvectorize=UserDefinedFunction(lambda vs: Vectors.dense(vs), VectorUDT())\nfeatures_df = features_df.withColumn(\"vec\", vectorize(features_df['vector'])).drop('vector')\nfeatures_df.show(1)\nfeatures_df.printSchema()\n","user":"anonymous","dateUpdated":"2017-09-21T14:52:59+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+--------------------+\n|     domain|                 vec|\n+-----------+--------------------+\n|www.iggl.de|[3.63758615972638...|\n+-----------+--------------------+\nonly showing top 1 row\n\nroot\n |-- domain: string (nullable = false)\n |-- vec: vector (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1505996090611_880635329","id":"20170921-121450_847363480","dateCreated":"2017-09-21T12:14:50+0000","dateStarted":"2017-09-21T14:52:59+0000","dateFinished":"2017-09-21T14:53:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29858"},{"text":"%pyspark\n\n# Remove www. prefix\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import StringType\n\nname='domain'\nprefix=\"www.\"\nudf = UserDefinedFunction(lambda x: (x[len(prefix):] if (x.startswith(prefix) if x else False) else x), StringType())\n\ndns_bh2 = dns_bh.select(*[udf(column).alias(name) if column == name else column for column in dns_bh.columns])\ndns_bh2.show(3)","user":"anonymous","dateUpdated":"2017-09-21T14:53:46+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|              domain|\n+--------------------+\n|amazon.co.uk.secu...|\n|autosegurancabras...|\n|christianmensfell...|\n+--------------------+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1506002545573_980911344","id":"20170921-140225_605497342","dateCreated":"2017-09-21T14:02:25+0000","dateStarted":"2017-09-21T14:53:46+0000","dateFinished":"2017-09-21T14:53:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29859"},{"text":"%pyspark\n\n# Filter feature vectors for only those vectors that have entries in the DNS-BH list dictionary (i.e. ground truth labels)\ncommon_domains_df=features_df.join(dns_bh2, [\"domain\"]) # doesn't create extra column\ncommon_domains_df.cache()\nfeatures_df.unpersist()\ndns_bh.unpersist()\nprint(\"Number of labelled domains = \" + str(common_domains_df.count()))\ncommon_domains_df.show(3)\ncommon_domains_df.printSchema()\n\n# Damn, we appear to only have 57 bad domains in the 128 WAT files currently processed by Bill.. this may not be enough to train a model.\n# TODO: Get more/better data, both for CC feature vectors, and known bad domains!","user":"anonymous","dateUpdated":"2017-09-21T14:18:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Number of labelled domains = 57\n+--------------------+--------------------+\n|              domain|              vector|\n+--------------------+--------------------+\n|         tsjyoti.com|[4.57471097850338...|\n|     jur-science.com|[6.52356230614951...|\n|fernandovillamorj...|[4.26267987704131...|\n+--------------------+--------------------+\nonly showing top 3 rows\n\nroot\n |-- domain: string (nullable = false)\n |-- vector: array (nullable = true)\n |    |-- element: double (containsNull = false)\n\n"}]},"apps":[],"jobName":"paragraph_1505996125084_1163672602","id":"20170921-121525_2032796778","dateCreated":"2017-09-21T12:15:25+0000","dateStarted":"2017-09-21T14:03:19+0000","dateFinished":"2017-09-21T14:15:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29860"},{"text":"%pyspark\n\n# Create numeric indexes for our classes\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nlabelIndexer = StringIndexer(inputCol=\"category\", outputCol=\"indexedCategory\").fit(common_domains_df)\n\n# Split into training and test sets using spark.ML API\ndomains_train, domains_test = common_domains_df.randomSplit([0.7,0.3],seed=42)\n\n# TODO: Need to add a Good class, and bring in domains from Bill's data\n#       Then add 'Good' and 'Bad' labels to each record for training and test.\n\n#domains_test.groupBy('category').count().show()","user":"anonymous","dateUpdated":"2017-09-21T14:19:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1849060203705397415.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1849060203705397415.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/lib/spark/python/pyspark/ml/base.py\", line 64, in fit\n    return self._fit(dataset)\n  File \"/usr/lib/spark/python/pyspark/ml/wrapper.py\", line 265, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/usr/lib/spark/python/pyspark/ml/wrapper.py\", line 262, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'Field \"category\" does not exist.'\n\n"}]},"apps":[],"jobName":"paragraph_1505996210608_-1643516670","id":"20170921-121650_838064234","dateCreated":"2017-09-21T12:16:50+0000","dateStarted":"2017-09-21T14:19:13+0000","dateFinished":"2017-09-21T14:19:13+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:29861"},{"text":"%pyspark\n\n# Create a pipeline and fit a RandomForest Classifier using spark.ml\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Build our RF classifier\nrf = RandomForestClassifier(labelCol=\"indexedCategory\", featuresCol=\"vec\", numTrees=10)\n\n# Convert indexed labels back to original labels\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedCategory\", labels=labelIndexer.labels)\n\n# Define and run the full pipeline to train the model and make predictions                               \npipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])\nmodel=pipeline.fit(domains_train)\npredictions=model.transform(domains_test)\nprint(predictions.take(1))\npredictions.select(\"predictedCategory\", \"category\", \"vec\").show(5)\n","user":"anonymous","dateUpdated":"2017-09-21T14:18:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1849060203705397415.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1849060203705397415.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 4, in <module>\nNameError: name 'labelIndexer' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1505996263309_8487838","id":"20170921-121743_1948672338","dateCreated":"2017-09-21T12:17:43+0000","dateStarted":"2017-09-21T14:18:47+0000","dateFinished":"2017-09-21T14:18:47+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:29862"},{"text":"%pyspark\n\n# Select (prediction, true label) and compute test error\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator1 = BinaryClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator2 = BinaryClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"f1\")\n\naccuracy = evaluator1.evaluate(predictions)\nf1=evaluator2.evaluate(predictions)\n\nprint(\"Accuracy=%g, F1=%g\" % (accuracy, f1))","user":"anonymous","dateUpdated":"2017-09-21T14:19:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Exception AttributeError: \"'BinaryClassificationEvaluator' object has no attribute '_java_obj'\" in <object repr() failed> ignored\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1849060203705397415.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1849060203705397415.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/lib/spark/python/pyspark/__init__.py\", line 104, in wrapper\n    return func(self, **kwargs)\nTypeError: __init__() got an unexpected keyword argument 'predictionCol'\n\n"}]},"apps":[],"jobName":"paragraph_1505996286008_-418168044","id":"20170921-121806_1465677517","dateCreated":"2017-09-21T12:18:06+0000","dateStarted":"2017-09-21T14:19:55+0000","dateFinished":"2017-09-21T14:19:55+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:29863"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-09-21T14:19:21+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506003561383_-1615547210","id":"20170921-141921_2024402912","dateCreated":"2017-09-21T14:19:21+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:29864"}],"name":"Paul 3 - evaluate CC vectors on DNS-BH","id":"2CV1GZX46","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}