{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to evaluate CC domain feature vectors against the DNS-BH Malware\n# Domain Blocklist from this site: http://mirror1.malwaredomains.com \n# Specifically, the 'justdomains' file, which currently contains 31k 'bad' domains.\n# We train using some of these domains, and try to predict the rest from amongst all \n# the domains for which we have feature vectors.\n# PJ - 25 Sept 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Import the DNS-BH domain list as a DataFrame\nbh_schema=StructType([StructField(\"domain\", StringType(), False)])\ndns_bh=spark.read.csv('s3://billsdata.net/CommonCrawl/DNS-BH/justdomains.dms', header=False, schema=bh_schema)\ndns_bh.show(3)\nprint(\"Bad domains in DNS-BH: \" + str(dns_bh.count()))\ndns_bh.cache()","dateUpdated":"2017-09-25T11:32:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|              domain|\n+--------------------+\n|amazon.co.uk.secu...|\n|autosegurancabras...|\n|christianmensfell...|\n+--------------------+\nonly showing top 3 rows\n\nBad domains in DNS-BH: 31877\nDataFrame[domain: string]\n"}]},"apps":[],"jobName":"paragraph_1506333778374_439863166","id":"20170921-121051_780451163","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:109","user":"anonymous","dateFinished":"2017-09-25T11:32:05+0000","dateStarted":"2017-09-25T11:32:01+0000"},{"text":"%pyspark\n\n# Load Bill's domain feature vectors from s3, in the following format:\n# (u'www.angelinajolin.com', [4.30406509320417, 0.02702702702702703, 0.0, 0.13513513513513514, 0.0, 0.06756756756756757, 0.0])\n\nnfiles=128 # (takes about 5 mins for 128 files)\n\n# Load feature vectors from WAT files (from 'Bill 6' notebook) as an RDD:\ninputURI = \"s3://billsdata.net/CommonCrawl/domain_hex_feature_vectors_from_%d_WAT_files\" % nfiles\nfeatures_rdd = sc.textFile(inputURI).map(eval)\nimport pyspark.sql.types as typ\nschema=StructType([StructField(\"domain\", StringType(), False), StructField(\"vector\", ArrayType(DoubleType(), False))])\nfeatures_df=spark.createDataFrame(features_rdd,schema)\nfeatures_df.cache()\nprint(\"Nr domains:\", features_df.count())\nprint(features_df.show(1))\nfeatures_df.printSchema()\n","dateUpdated":"2017-09-25T11:32:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('Nr domains:', 2626203)\n+-----------+--------------------+\n|     domain|              vector|\n+-----------+--------------------+\n|www.iggl.de|[3.63758615972638...|\n+-----------+--------------------+\nonly showing top 1 row\n\nNone\nroot\n |-- domain: string (nullable = false)\n |-- vector: array (nullable = true)\n |    |-- element: double (containsNull = false)\n\n"}]},"apps":[],"jobName":"paragraph_1506333778377_437169924","id":"20170921-121405_178585430","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:110","user":"anonymous","dateFinished":"2017-09-25T11:34:00+0000","dateStarted":"2017-09-25T11:32:01+0000"},{"text":"%pyspark\n\n# Spark.ML classifiers require VectorUDF type, rather than Array, so we need to convert\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import UserDefinedFunction\nvectorize=UserDefinedFunction(lambda vs: Vectors.dense(vs), VectorUDT())\nfeatures_df = features_df.withColumn(\"vec\", vectorize(features_df['vector'])).drop('vector')\nfeatures_df.show(1)\nfeatures_df.printSchema()\n","dateUpdated":"2017-09-25T11:32:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+--------------------+\n|     domain|                 vec|\n+-----------+--------------------+\n|www.iggl.de|[3.63758615972638...|\n+-----------+--------------------+\nonly showing top 1 row\n\nroot\n |-- domain: string (nullable = false)\n |-- vec: vector (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1506333778378_438324170","id":"20170921-121450_847363480","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:111","user":"anonymous","dateFinished":"2017-09-25T11:34:18+0000","dateStarted":"2017-09-25T11:32:06+0000"},{"text":"%pyspark\n\n# Remove www. prefix from both DNS_BH and Bill's vectors\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import StringType\n\nname='domain'\nprefix=\"www.\"\nudf = UserDefinedFunction(lambda x: (x[len(prefix):] if (x.startswith(prefix) if x else False) else x), StringType())\n\ndns_bh2 = dns_bh.select(*[udf(column).alias(name) if column == name else column for column in dns_bh.columns])\ndns_bh2.show(3)\n\nfeatures_df2 = features_df.select(*[udf(column).alias(name) if column == name else column for column in features_df.columns])\nfeatures_df2.show(3)","dateUpdated":"2017-09-25T11:32:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|              domain|\n+--------------------+\n|amazon.co.uk.secu...|\n|autosegurancabras...|\n|christianmensfell...|\n+--------------------+\nonly showing top 3 rows\n\n+----------------+--------------------+\n|          domain|                 vec|\n+----------------+--------------------+\n|         iggl.de|[3.63758615972638...|\n|     bmskirov.ru|[3.91202300542814...|\n|education.nh.gov|[4.00733318523247...|\n+----------------+--------------------+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1506333778378_438324170","id":"20170921-140225_605497342","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:112","user":"anonymous","dateFinished":"2017-09-25T11:34:39+0000","dateStarted":"2017-09-25T11:34:00+0000"},{"text":"%pyspark\n\n# Filter feature vectors for only those vectors that have entries in the DNS-BH list dictionary (i.e. ground truth labels)\ncommon_domains_df=features_df2.join(dns_bh2, [\"domain\"]) # doesn't create extra column\ncommon_domains_df.cache()\nfeatures_df.unpersist()\ndns_bh.unpersist()\nprint(\"Number of labelled domains = \" + str(common_domains_df.count()))\ncommon_domains_df.show(3)\ncommon_domains_df.printSchema()\n\n# We appear to only have 149 bad domains in the 128 WAT files currently processed by Bill.. this may not be enough to train a good model but let's try anyway.\n# TODO: Get more/better data, both for CC feature vectors, and known bad domains!","dateUpdated":"2017-09-25T11:32:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Number of labelled domains = 149\n+---------------+--------------------+\n|         domain|                 vec|\n+---------------+--------------------+\n|    tsjyoti.com|[4.57471097850338...|\n|jur-science.com|[6.52356230614951...|\n|      simbio.ru|[5.05624580534830...|\n+---------------+--------------------+\nonly showing top 3 rows\n\nroot\n |-- domain: string (nullable = true)\n |-- vec: vector (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1506333778379_437939421","id":"20170921-121525_2032796778","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:113","user":"anonymous","dateFinished":"2017-09-25T11:35:05+0000","dateStarted":"2017-09-25T11:34:19+0000"},{"text":"%pyspark\n\n# Add a category column to our dataframe, and label all these as bad\nfrom pyspark.sql.functions import lit\ncommon_domains_df=common_domains_df.withColumn(\"category\", lit(\"bad\"))\ncommon_domains_df.show(3)\n#common_domains_df.printSchema()\n\n# Now create a 'good' class made up of a random sample of other domains in Bill's data\nnumber_of_samples=common_domains_df.count()\nfraction=float(number_of_samples)/features_df2.count()\nprint(\"--> Finding \" + str(number_of_samples) + \" samples of good domains, fraction of \" + str(fraction))\nfeatures_df2_sample=features_df2.sample(False, fraction, 42) # create roughly balanced classes\nfeatures_df2_sample=features_df2_sample.withColumn(\"category\", lit(\"good\"))\nfeatures_df2_sample.show(3)\n#features_df2_sample.printSchema()\n\n# Concatenate the two dataframes together using union, and summarize\nunion_df=common_domains_df.union(features_df2_sample)\nunion_df.groupBy('category').count().show()","user":"anonymous","dateUpdated":"2017-09-25T12:10:20+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506334663038_-1250278576","id":"20170925-101743_818218521","dateCreated":"2017-09-25T10:17:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1044","dateFinished":"2017-09-25T12:12:42+0000","dateStarted":"2017-09-25T12:10:20+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------+--------------------+--------+\n|         domain|                 vec|category|\n+---------------+--------------------+--------+\n|    tsjyoti.com|[4.57471097850338...|     bad|\n|jur-science.com|[6.52356230614951...|     bad|\n|      simbio.ru|[5.05624580534830...|     bad|\n+---------------+--------------------+--------+\nonly showing top 3 rows\n\n--> Finding 149 samples of good domains, fraction of 5.67359035078e-05\n+--------------------+--------------------+--------+\n|              domain|                 vec|category|\n+--------------------+--------------------+--------+\n|  pindao.blogbus.com|[4.07753744390572...|    good|\n|qhkPtNVo.www.00vg...|[4.29045944114839...|    good|\n|    bursa.shalala.ru|[4.00733318523247...|    good|\n+--------------------+--------------------+--------+\nonly showing top 3 rows\n\n+--------+-----+\n|category|count|\n+--------+-----+\n|     bad|  149|\n|    good|  179|\n+--------+-----+\n\n"}]}},{"text":"%pyspark\n\n# Create numeric indexes for our classes\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nlabelIndexer = StringIndexer(inputCol=\"category\", outputCol=\"indexedCategory\").fit(union_df)\n\n# Split into training and test sets using spark.ML API\ndomains_train, domains_test = union_df.randomSplit([0.7,0.3],seed=42)\n\n# Let's check the breakdown of categories in our test data\ndomains_test.groupBy('category').count().show()","dateUpdated":"2017-09-25T12:12:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-----+\n|category|count|\n+--------+-----+\n|     bad|   49|\n|    good|   53|\n+--------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1506333778379_437939421","id":"20170921-121650_838064234","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:114","user":"anonymous","dateFinished":"2017-09-25T12:13:58+0000","dateStarted":"2017-09-25T12:12:52+0000"},{"text":"%pyspark\n\n# Create a pipeline and fit a RandomForest Classifier using spark.ml\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Build our RF classifier\nrf = RandomForestClassifier(labelCol=\"indexedCategory\", featuresCol=\"vec\", numTrees=10)\n\n# Convert indexed labels back to original labels\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedCategory\", labels=labelIndexer.labels)\n\n# Define and run the full pipeline to train the model and make predictions                               \npipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])\nmodel=pipeline.fit(domains_train)\npredictions=model.transform(domains_test)\nprint(predictions.take(1))\npredictions.select(\"predictedCategory\", \"category\", \"vec\").show(5)\npredictions.groupBy('predictedCategory').count().show()\n","dateUpdated":"2017-09-25T12:13:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(domain=u'fernandovillamorjr.com', vec=DenseVector([4.2627, 0.4648, 0.0, 0.0, 0.0139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0278, 0.0, 0.0, 0.0278, 0.0556, 0.0139, 0.0139, 0.0139, 0.2639, 0.0556, 0.0, 0.0139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0139, 0.0, 0.0278, 0.0278, 0.0417, 0.0417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), category=u'bad', indexedCategory=1.0, rawPrediction=DenseVector([5.1427, 4.8573]), probability=DenseVector([0.5143, 0.4857]), prediction=0.0, predictedCategory=u'good')]\n+-----------------+--------+--------------------+\n|predictedCategory|category|                 vec|\n+-----------------+--------+--------------------+\n|             good|     bad|[4.26267987704131...|\n|              bad|     bad|[6.52356230614951...|\n|             good|     bad|[3.89182029811062...|\n|              bad|     bad|[9.70899219481046...|\n|             good|     bad|[7.86403565907245...|\n+-----------------+--------+--------------------+\nonly showing top 5 rows\n\n+-----------------+-----+\n|predictedCategory|count|\n+-----------------+-----+\n|              bad|   26|\n|             good|   76|\n+-----------------+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1506333778380_436015677","id":"20170921-121743_1948672338","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:115","user":"anonymous","dateFinished":"2017-09-25T12:20:21+0000","dateStarted":"2017-09-25T12:13:02+0000"},{"text":"%pyspark\n\n# Select (prediction, true label) and compute overall test accuracy (not that this is meaningful with imbalanced classes)\n# Binary evaluator only seems to support AUC metrics, so use Multiclass instead\n#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n#evaluator1 = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"indexedCategory\", metricName=\"areaUnderROC\")\n#evaluator2 = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"indexedCategory\", metricName=\"areaUnderPR\")\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator1 = MulticlassClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"accuracy\")\nevaluator2 = MulticlassClassificationEvaluator(labelCol=\"indexedCategory\", predictionCol=\"prediction\", metricName=\"f1\")\n\naccuracy = evaluator1.evaluate(predictions)\nf1=evaluator2.evaluate(predictions)\n\nprint(\"Accuracy=%g, F1=%g\" % (accuracy, f1))","dateUpdated":"2017-09-25T12:20:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Accuracy=0.519608, F1=0.488773\n"}]},"apps":[],"jobName":"paragraph_1506333778380_436015677","id":"20170921-121806_1465677517","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:116","user":"anonymous","dateFinished":"2017-09-25T12:25:41+0000","dateStarted":"2017-09-25T12:20:34+0000"},{"text":"%pyspark\n\n# Compute the precision, recall and f1-score with respect to the bad class manually\ntp=predictions[(predictions['predictedCategory']=='bad') & (predictions['category']=='bad')].count()\nfp=predictions[(predictions['predictedCategory']=='bad') & (predictions['category']=='good')].count()\nfn=predictions[(predictions['predictedCategory']=='good') & (predictions['category']=='bad')].count()\nprint(\"tp=\"+str(tp)+\",fp=\"+str(fp)+\",fn=\"+str(fn))\nprecision=float(tp)/(tp+fp)\nrecall=float(tp)/(tp+fn)\nf1=(2*precision*recall)/(precision+recall)\nprint(\"precision=\"+str(precision)+\", recall=\"+str(recall)+\", f1=\"+str(f1))\n\n# So we're not doing very well yet, but wouldn't expect to with the limited feature set (and possibly not great ground-truth) we're currently using.","dateUpdated":"2017-09-25T12:29:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506333778381_435630928","id":"20170921-141921_2024402912","dateCreated":"2017-09-25T10:02:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:117","user":"anonymous","dateFinished":"2017-09-25T12:28:48+0000","dateStarted":"2017-09-25T12:20:40+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"tp=13,fp=13,fn=36\nprecision=0.5, recall=0.265306122449, f1=0.346666666667\n"}]}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-09-25T11:32:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506338200457_1973356193","id":"20170925-111640_2116960848","dateCreated":"2017-09-25T11:16:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1774"}],"name":"Paul 3 - evaluate CC vectors on DNS-BH","id":"2CWP7Q9A7","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}