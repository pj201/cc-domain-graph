{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to create domain summaries based on the May/Jun/Jul 2017 CommonCrawl graph\n# as per description here: http://commoncrawl.org/2017/08/webgraph-2017-may-june-july/\n# PJ - 13 October 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n#LIMIT=10000000 # Temporary limit while developing code.\n\n# Import the PLD vertices list as a DataFrame\n#pld_schema=StructType([StructField(\"ID\", StringType(), False), StructField(\"PLD\", StringType(), False)])\n#pld_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices.txt.gz\")\n#temp_pld = pld_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#pld_df=temp_pld.toDF(pld_schema) #.limit(LIMIT) #.repartition(4)\n#pld_df.show(3)\n\n# Load in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\n#pld_df.coalesce(64).write.save(saveURI) # Use all default options\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\nprint(pld_df.count()) # Should have 91M domains","user":"anonymous","dateUpdated":"2017-10-13T08:40:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\n91034128\n"}]},"apps":[],"jobName":"paragraph_1507883348380_280878153","id":"20170929-081624_672091334","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T08:40:51+0000","dateFinished":"2017-10-13T08:41:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:196"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame\n#pld_edges_schema=StructType([StructField(\"src\", LongType(), False), StructField(\"dst\", LongType(), False)])\n#pld_edges_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges.txt.gz\")\n#temp_edges_pld = pld_edges_txt.map(lambda k: map(int, k.split())) # By default, splits on whitespace, which is what we want\n#pld_edges_df=temp_edges_pld.toDF(pld_edges_schema) #.limit(LIMIT*10) #.repartition(8)\n#pld_edges_df.show(3)\n\n# Load in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\n#pld_edges_df.coalesce(64).write.save(saveURI) # Use all default options\npld_edges_df=spark.read.load(saveURI)\npld_edges_df.show(3)\npld_edges_df.cache()","user":"anonymous","dateUpdated":"2017-10-13T08:40:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+\n|src|     dst|\n+---+--------+\n|  2| 9193244|\n| 20|75600973|\n| 21|46356172|\n+---+--------+\nonly showing top 3 rows\n\nDataFrame[src: bigint, dst: bigint]\n"}]},"apps":[],"jobName":"paragraph_1507883348383_281262902","id":"20170929-095050_1324183281","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T08:40:53+0000","dateFinished":"2017-10-13T08:41:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\n#host_schema=StructType([StructField(\"hostid\", StringType(), False), StructField(\"host\", StringType(), False)])\n#host_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices.txt.gz\")\n#temp_host = host_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#host_df=temp_host.toDF(host_schema) #.repartition(4)\n#host_df.show(3)\n\n# Save in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\n#host_df.coalesce(128).write.save(saveURI) # Use all default options\nhost_df=spark.read.load(saveURI).repartition(64)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","user":"anonymous","dateUpdated":"2017-10-13T10:53:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+------------+\n|  hostid|        host|\n+--------+------------+\n|19390217|cc.oqa9.oc2b|\n|19390281|cc.oqa9.on6s|\n|19390345|cc.oqa9.ozjc|\n+--------+------------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1507883348384_267027193","id":"20170929-095310_1201506389","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T10:53:46+0000","dateFinished":"2017-10-13T11:02:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"text":"%pyspark\n\n# Load in all harmonic centrality and page-ranks, and join based on reverse domain name\n# Format: #hc_pos #hc_val #pr_pos #pr_val #host_rev\n#pr_schema=StructType([StructField(\"hc_pos\", StringType(), False), StructField(\"hc_val\", StringType(), False), StructField(\"pr_pos\", StringType(), False), StructField(\"pr_val\", StringType(), False), StructField(\"host_rev\", StringType(), False)])\n#pr_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/ranks.txt.gz\")\n#header=pr_txt.first()\n#pr_txt=pr_txt.filter(lambda x: x!=header)\n#temp_pr = pr_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#pr_df=temp_pr.toDF(header.split()).withColumnRenamed(\"#host_rev\",\"host_rev\") #.limit(LIMIT*10) #.repartition(8)\n#pr_df.show(3)\n\n# Save in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/ranks/\"\n#pr_df.coalesce(64).write.save(saveURI) # Use all default options\npr_df=spark.read.load(saveURI)\npr_df.show(3)\npr_df.cache()\n#pr_df.count() # Should be 91M","user":"anonymous","dateUpdated":"2017-10-13T08:40:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------+-------+-------------------+--------------+\n|#hc_pos| #hc_val|#pr_pos|            #pr_val|      host_rev|\n+-------+--------+-------+-------------------+--------------+\n|      1|24989952|      1| 0.0155264576161686|  com.facebook|\n|      2|22460880|      3|0.00866038900847366|   com.twitter|\n|      3|22097514|      2| 0.0128827315785546|com.googleapis|\n+-------+--------+-------+-------------------+--------------+\nonly showing top 3 rows\n\nDataFrame[#hc_pos: string, #hc_val: string, #pr_pos: string, #pr_val: string, host_rev: string]\n"}]},"apps":[],"jobName":"paragraph_1507883348385_266642444","id":"20170929-093202_1772383833","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T08:41:58+0000","dateFinished":"2017-10-13T08:42:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%pyspark\n\n# Debug partitioning of our 4 big dataframes\nsc.getConf().getAll() #.mkString(\"\\n\")\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())\npr_df.rdd.getNumPartitions()","user":"anonymous","dateUpdated":"2017-10-13T11:04:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"256\n256\n64\n256\n"}]},"apps":[],"jobName":"paragraph_1507883348385_266642444","id":"20171006-161509_1868852031","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T11:04:37+0000","dateFinished":"2017-10-13T11:04:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"text":"%pyspark #--packages graphframes:graphframes:0.5.0-spark2.1-s_2.11\n\n# We now have everything we need in these four dataframes to create the summaries we need.\n\n# This code can't handle the complete edge lists, and produces this exception:\n# java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n#out_degrees_=dict(pld_edges_df.groupBy(\"src\").count().collect())\n#in_degrees=dict(pld_edges_df.groupBy(\"dst\").count().collect())\n#print(out_degrees['846558'])\n#print(in_degrees['846558'])\n\n# Instead, just create RDDs and use lookup()\nout_degrees=pld_edges_df.groupBy(\"src\").count()\nin_degrees=pld_edges_df.groupBy(\"dst\").count()\npld_edges_df.unpersist()\nout_degrees.show(3)\nin_degrees.show(3)\n#print(out_degrees.rdd.lookup(846558))\n#print(in_degrees.rdd.lookup(846558))","user":"anonymous","dateUpdated":"2017-10-13T08:40:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|src|count|\n+---+-----+\n| 26|    2|\n| 29|   34|\n|964|    1|\n+---+-----+\nonly showing top 3 rows\n\n+--------+-----+\n|     dst|count|\n+--------+-----+\n|      29|   40|\n|36750820|    5|\n|61427989| 3242|\n+--------+-----+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1507883348386_267796691","id":"20170929-095727_1596943627","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T08:42:06+0000","dateFinished":"2017-10-13T08:42:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when counting hosts\n# This code can't handle the full PLD list and produces this exception:\n# Stack trace: ExitCodeException exitCode=52\n#pld_lookup_table=dict(pld_df.rdd.map(lambda x: (x['PLD'], x['ID'])).collect()) # Bad!\n#print(pld_lookup_table[\"aaa.aaa\"])\n\n# Instead, just create an RDD and use lookup()\n#pld_lookup_table=pld_df.rdd.map(lambda x: (x['PLD'], x['ID']))\n#print(pld_lookup_table.lookup(\"aaa.aaa\")) # Very bad!\n\n# Or let's try creating as a BloomFilter, since we only want to record presence of a PLD\n#pld_bf = pld_df.stat.bloomFilter(\"PLD\", expectedNumItems, fpp) # Doesn't exist in pyspark API!\n#pld_bf.mightContain(\"aaa.aaa\")\n\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.rdd.collect(): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\nprint(pld_df.rdd.take(3))\nprint(pld_df.rdd.take(3)[2]['PLD'])\n#pld_bf.add(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\n# TODO: Fix this distributed BloomFilter implementation - can't figure out how to properly combine BFs in a reduce operation!\n#tmp=pld_df.rdd.map(lambda x: pld_bf.add(x['PLD'])) # Very bad - pld_bf gets copied to each of the workers then discarded!\n#tmp=pld_df.rdd.map(lambda x: (pld_bf.add(x['PLD']), pld_bf)).reduce(lambda x,y: x[1].union(y[1])) # Should work but complains that BloomFilter isn't iterable!\n#print(tmp.take(3))\n#print(tmp.count()) # Ensure it runs the map across the entire dataframe\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","user":"anonymous","dateUpdated":"2017-10-13T08:40:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(ID=u'0', PLD=u'aaa.a'), Row(ID=u'1', PLD=u'aaa.aa'), Row(ID=u'2', PLD=u'aaa.aaa')]\naaa.aaa\nTrue\n64\n90751305\nTrue\nFalse\nTrue\nFalse\n"}]},"apps":[],"jobName":"paragraph_1507883348387_267411942","id":"20170929-100048_2070118110","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T08:42:07+0000","dateFinished":"2017-10-13T09:19:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","user":"anonymous","dateUpdated":"2017-10-13T08:40:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]},"apps":[],"jobName":"paragraph_1507883348387_267411942","id":"20171004-091447_4214261","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T08:42:53+0000","dateFinished":"2017-10-13T09:19:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"text":"%pyspark\n\n# Now count the number of hosts per PLD in a scalable way, and create another dictionary\n# Still takes over an hour since host_df contains 1.3B rows but should complete without errors.\n# (An attempt to collectAsMap at the end results in java Integer.MAX_VALUE or memory errors!)\ncount_rdd=host_df.drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),1)).reduceByKey(lambda x,y: x+y) #.collectAsMap() # Consider using a CountMin sketch here in future?\nbool_rdd=host_df.drop('hostid').rdd.map(lambda x: (x['host'], is_a_pld(x['host']))).filter(lambda x: x[1]==True) #.collectAsMap() # Only outputs PLD hosts (so <91M rows)\n\nprint(count_rdd.take(3))\nprint(bool_rdd.take(3))\nprint(count_rdd.count())\nprint(bool_rdd.count())\n\nhost_df.unpersist()\n\n# Debugging\nprint(count_rdd.filter(lambda x: x[0]=='aaa.aaa').collect())\nprint(len(count_rdd.filter(lambda x: x[0]=='ERROR').collect())) # Should be zero once we've loaded all the PLDs!","user":"anonymous","dateUpdated":"2017-10-13T12:21:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"[(u'com.kako-lako', 1), (u'com.monkeypro', 1), (u'be.bowlingbrussels', 1)]\n[(u'cc.oqbpm', True), (u'cc.orozco', True), (u'cc.osteopathie', True)]\n90839924\n89276336\n[(u'aaa.aaa', 6)]\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-892606089958113834.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-892606089958113834.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 9, in <module>\nTypeError: count() takes exactly one argument (0 given)\n\n"}]},"apps":[],"jobName":"paragraph_1507883348388_265488197","id":"20171004-092350_1522843259","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T11:04:44+0000","dateFinished":"2017-10-13T11:57:34+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import col, when, lit\n\n# The following code works well when the data is small enough to collect into a python dictionary but our data is too big:\n# Define a UDF to perform column-based lookup\n#def translate(mapping):\n#    def translate_(col):\n#        if not mapping.get(col):\n#            return 0\n#        else:\n#            return mapping.get(col)\n#    return udf(translate_, IntegerType())\n# And a similar function for the Boolean map\n#def translate_bool(mapping):\n#    def translate_bool_(col):\n#        if not mapping.get(col):\n#            return False\n#        else:\n#            return mapping.get(col)\n#    return udf(translate_bool_, BooleanType())\n# Insert our count column back into the host summary dataframe, along with a boolean to say whether the PLD is a host in itself\n# While we're at it, let's add in the in and out-degrees too, and an indicator of whether the site has been crawled.\n#crawled_test=when(col(\"OutDegree\")==0, lit(False)).otherwise(lit(True))\n#pld_df_joined=pld_df.withColumn('NumHosts', translate(count_table)(\"PLD\"))\\\n                    #.withColumn('PLDisHost?', translate_bool(bool_table)(\"PLD\"))\n                    #.withColumn('InDegree', translate(in_degrees)(\"ID\"))\\\n                    #.withColumn('OutDegree', translate(out_degrees)(\"ID\"))\\\n                    #.withColumn('Crawled?', crawled_test)\n                    \n# Convert the result RDDs to dataframes, ready for joining\ncountschema=StructType([StructField(\"PLD2\", StringType(), False), StructField(\"NumHosts\", LongType(), False)])\ncount_df=count_rdd.toDF(countschema)\ncount_df.show(3)\nboolschema=StructType([StructField(\"PLD2\", StringType(), False), StructField(\"PLDtest\", BooleanType(), False)])\nbool_df=bool_rdd.toDF(boolschema)\nbool_df.show(3)\n\n# Join these new dataframes with the original dataframe (using fast equi-joins)\npld_df2=pld_df.join(count_df, count_df.PLD2==pld_df.PLD, \"leftOuter\").drop(\"PLD2\")\nbool_test=when(col(\"PLDtest\").isNull(), lit(False)).otherwise(lit(True))\npld_df_joined=pld_df2.join(bool_df, bool_df.PLD2==pld_df2.PLD, \"leftOuter\").drop(\"PLD2\").withColumn('PLDisHost?', bool_test).drop(\"PLDtest\")\n\npld_df.unpersist()\npld_df_joined.sort(\"NumHosts\", ascending=False).show(100)\npld_df_joined.cache()","user":"anonymous","dateUpdated":"2017-10-13T12:21:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------+--------+\n|              PLD2|NumHosts|\n+------------------+--------+\n|     com.kako-lako|       1|\n|     com.monkeypro|       1|\n|be.bowlingbrussels|       1|\n+------------------+--------+\nonly showing top 3 rows\n\n+--------------+-------+\n|          PLD2|PLDtest|\n+--------------+-------+\n|      cc.oqbpm|   true|\n|     cc.orozco|   true|\n|cc.osteopathie|   true|\n+--------------+-------+\nonly showing top 3 rows\n\n+--------+--------------------+--------+----------+\n|      ID|                 PLD|NumHosts|PLDisHost?|\n+--------+--------------------+--------+----------+\n| 8851519|           com.533b8|24285866|      true|\n|17565198|     com.composesite|15089842|      true|\n|23766155|com.getrichandgiv...|14513586|      true|\n|15069492|      com.bumpbabyme|13727426|      true|\n|35854802|        com.ourssite|12917619|      true|\n|14175746|        com.blogspot|12717586|      true|\n|14175787|        com.blogspot|12717586|      true|\n|13107744|          com.bbddaa|12620057|      true|\n|23394611|   com.game-stalkers|11675096|      true|\n|49161638|          com.zzmmaa|11535747|      true|\n|46884574|       com.w3cdomain|10147845|      true|\n|49160413|          com.zzjxtz| 9339180|     false|\n|47797448|       com.wordpress| 8883234|      true|\n|47797089|       com.wordpress| 8883234|      true|\n|30438975|           com.liao1| 8443777|      true|\n|42173462|          com.ssygjy| 7565218|      true|\n|45502107|          com.tumblr| 6636904|      true|\n|45502088|          com.tumblr| 6636904|      true|\n|42654327|     com.sucicurtain| 6566229|      true|\n|78999209|   org.securityscans| 6124364|      true|\n|41411388|         com.skyrock| 4951491|      true|\n|41411386|         com.skyrock| 4951491|      true|\n|19286912|      com.deviantart| 3265581|      true|\n|19286914|      com.deviantart| 3265581|      true|\n|28105741|           com.jnhgy| 2936233|      true|\n|31223538|com.luohaophotogr...| 2761719|      true|\n|47973485|          com.wwwbfh| 2409961|      true|\n|49150863|          com.zynews| 2340181|      true|\n|75514852|        online.yh834| 2279716|      true|\n|75514972|        online.yh989| 2279497|      true|\n|23267769|        com.fuzhouwl| 2138973|      true|\n|30739332|     com.livejournal| 2017689|      true|\n|31184970|    com.lugechuanmei| 1987046|      true|\n|71673306|            net.skur| 1981307|      true|\n|75514789|        online.yh759| 1919211|      true|\n|75514841|        online.yh821| 1918662|      true|\n|75514875|        online.yh861| 1918563|      true|\n|75514808|        online.yh784| 1917710|      true|\n|75514834|        online.yh812| 1917362|      true|\n|75514937|        online.yh944| 1915955|      true|\n|75514885|        online.yh874| 1915907|      true|\n|75514882|        online.yh871| 1915649|      true|\n|75514976|        online.yh995| 1913024|      true|\n|12689286|    com.backstagefix| 1860924|      true|\n|76377021|        org.chinamsg| 1847439|      true|\n|76377277|         org.chinapy| 1845723|      true|\n|22288972|     com.filetransit| 1821935|      true|\n|46885624|         com.w3snoop| 1792685|      true|\n|12642852|   com.babesvsrobots| 1756975|      true|\n|23595664|    com.geekgivepeek| 1750995|      true|\n|26613929|     com.iklangadget| 1746655|      true|\n|46799259|        com.voterlux| 1742350|      true|\n|30566518|       com.limemouse| 1736312|      true|\n|12747003|    com.bailedeltubo| 1726284|      true|\n|13320770|  com.beijingyourway| 1721778|      true|\n|42803824|    com.superkyuhyun| 1719247|      true|\n|17754673|      com.coolgigllc| 1710996|      true|\n|38584376|        com.recrocon| 1709465|      true|\n|19751104|           com.dnwjw| 1695455|      true|\n|39895366| com.samjonessextape| 1692086|      true|\n|27322776|      com.isignmeout| 1687903|      true|\n|19943989|    com.dotnetmaniac| 1679625|      true|\n|27736708|      com.jba-engine| 1679069|      true|\n|14864319|      com.brolithium| 1675511|      true|\n|19674265|          com.djixas| 1671565|      true|\n|44242732|       com.theoxhorn| 1655233|      true|\n|21350113|       com.ericneill| 1598189|      true|\n|75514851|        online.yh833| 1551012|      true|\n|75514801|        online.yh776| 1550725|      true|\n|75514847|        online.yh827| 1550570|      true|\n|75514962|        online.yh976| 1549767|      true|\n|75514895|        online.yh887| 1549574|      true|\n|75514793|        online.yh763| 1549447|      true|\n|75514933|        online.yh940| 1549330|      true|\n|75514892|        online.yh882| 1549150|      true|\n|75514795|        online.yh767| 1549021|      true|\n|75514898|        online.yh892| 1548588|      true|\n|75514922|        online.yh923| 1548043|      true|\n|75514956|        online.yh969| 1547897|      true|\n|75514873|        online.yh859| 1545915|      true|\n|75514857|        online.yh839| 1543871|      true|\n|38109522|        com.queit21g| 1503882|      true|\n|12635969|          com.babajj| 1468376|      true|\n| 7830191|        co.caller-id| 1381571|      true|\n|22090711|             com.fc2| 1360713|      true|\n|22090737|             com.fc2| 1360713|      true|\n|69702382|        net.eachtong| 1340854|      true|\n| 7182925|         cn.fjmeilun| 1338282|      true|\n| 7147207|           cn.dq2918| 1329916|     false|\n| 7358421|          cn.jinwolf| 1329710|     false|\n|32134194|     com.mayienglish| 1325006|      true|\n| 7332379|           cn.hzhemu| 1320528|     false|\n| 6920418|        cn.cnzhizhen| 1317289|     false|\n| 7201787|           cn.gdlswy| 1307459|      true|\n| 6917185|            cn.cn-oy| 1306957|      true|\n| 7130482|    cn.datongcompany| 1306765|     false|\n| 7143733|         cn.dmmarket| 1304280|     false|\n| 7288576|     cn.hailinfengde| 1300478|     false|\n| 7148929|           cn.dsenta| 1299703|      true|\n| 7205524|          cn.giantcn| 1297858|      true|\n+--------+--------------------+--------+----------+\nonly showing top 100 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean]\n"}]},"apps":[],"jobName":"paragraph_1507883348389_265103448","id":"20171004-100819_284908525","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T12:21:22+0000","dateFinished":"2017-10-13T12:43:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:205"},{"text":"%pyspark\n\n# Join with in-degree and out-degree dataframes\npld_df_joined2=pld_df_joined.join(out_degrees, out_degrees.src==pld_df_joined.ID, \"leftOuter\").drop(\"src\").withColumnRenamed(\"count\",\"OutDegree\")\npld_df_joined.unpersist()\npld_df_joined3=pld_df_joined2.join(in_degrees, in_degrees.dst==pld_df_joined2.ID, \"leftOuter\").drop(\"dst\").withColumnRenamed(\"count\",\"InDegree\")\npld_df_joined2.unpersist()\npld_df_joined3.show(5)\npld_df_joined3.cache()","user":"anonymous","dateUpdated":"2017-10-13T12:46:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+----------+---------+--------+\n|  ID|                 PLD|NumHosts|PLDisHost?|OutDegree|InDegree|\n+----+--------------------+--------+----------+---------+--------+\n|  26|             abb.nic|       3|      true|        2|       3|\n|  29|abbott.corelabora...|       2|      true|       34|      40|\n| 474|     ac.americancars|       1|      true|     null|       3|\n| 964|              ac.cmt|       1|     false|        1|    null|\n|1677|          ac.insight|       1|      true|        7|       1|\n+----+--------------------+--------+----------+---------+--------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean, OutDegree: bigint, InDegree: bigint]\n"}]},"apps":[],"jobName":"paragraph_1507883348389_265103448","id":"20171006-152234_1487383953","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T12:46:29+0000","dateFinished":"2017-10-13T13:10:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:206"},{"text":"%pyspark\n\n# Insert a flag to indicate whether the PLD has been crawled\ncrawled_test=when(col(\"OutDegree\").isNull(), lit(False)).otherwise(lit(True))\npld_df_joined4=pld_df_joined3.withColumn('Crawled?', crawled_test)\npld_df_joined3.unpersist()\npld_df_joined4.show(5)\npld_df_joined4.cache()","user":"anonymous","dateUpdated":"2017-10-13T13:10:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+----------+---------+--------+--------+\n|  ID|                 PLD|NumHosts|PLDisHost?|OutDegree|InDegree|Crawled?|\n+----+--------------------+--------+----------+---------+--------+--------+\n|  26|             abb.nic|       3|      true|        2|       3|    true|\n|  29|abbott.corelabora...|       2|      true|       34|      40|    true|\n| 474|     ac.americancars|       1|      true|     null|       3|   false|\n| 964|              ac.cmt|       1|     false|        1|    null|    true|\n|1677|          ac.insight|       1|      true|        7|       1|    true|\n+----+--------------------+--------+----------+---------+--------+--------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean, OutDegree: bigint, InDegree: bigint, Crawled?: boolean]\n"}]},"apps":[],"jobName":"paragraph_1507883348390_266257695","id":"20171006-155148_681542412","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T13:10:59+0000","dateFinished":"2017-10-13T13:37:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:207"},{"text":"%pyspark\n\n# Finally, join with the harmonic centrality and page-rank for each domain\n# Note: could probably speed this up using something like above techniques, or by presorting (but we don't really need to since this is only 91Mx91M)\npld_df_joined5=pld_df_joined4.drop(\"#hc_pos\").drop(\"#pr_pos\").join(pr_df, pr_df.host_rev==pld_df_joined4.PLD, \"leftOuter\").drop(\"host_rev\")\\\n                             .withColumnRenamed(\"#hc_val\",\"HarmonicCentrality\").withColumnRenamed(\"#pr_val\",\"PageRank\")\npld_df_joined4.unpersist()\npld_df_joined5.show(5)\npld_df_joined5.cache()","user":"anonymous","dateUpdated":"2017-10-13T13:43:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+----------+---------+--------+--------+--------+------------------+--------+--------------------+\n|  ID|                 PLD|NumHosts|PLDisHost?|OutDegree|InDegree|Crawled?| #hc_pos|HarmonicCentrality| #pr_pos|            PageRank|\n+----+--------------------+--------+----------+---------+--------+--------+--------+------------------+--------+--------------------+\n| 120|             abc.web|       1|     false|     null|       1|   false|38991028|          10015440| 1478887|3.78405976859536e-08|\n| 311|             ac.8411|       1|     false|     null|       1|   false|69935624|           9082498|36930613|4.76481484534919e-09|\n| 713|              ac.bgc|       1|     false|     null|       1|   false|63729192|           9237769|32796120|4.90517712841288e-09|\n| 871|          ac.casinos|       1|      true|        2|       1|    true|78150671|         7839579.5|12855010|7.68640254732439e-09|\n|1014|ac.cosmopolitanun...|       1|      true|     null|      18|   false| 1636792|          12615973|20034471|5.85933334251156e-09|\n+----+--------------------+--------+----------+---------+--------+--------+--------+------------------+--------+--------------------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean, OutDegree: bigint, InDegree: bigint, Crawled?: boolean, #hc_pos: string, HarmonicCentrality: string, #pr_pos: string, PageRank: string]\n"}]},"apps":[],"jobName":"paragraph_1507883348390_266257695","id":"20170929-122540_264490752","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T13:43:09+0000","dateFinished":"2017-10-13T14:10:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:208"},{"text":"%pyspark\n\n# Save final table to S3 in compressed CSV format, broken into smaller files\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_summaries3/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\npld_df_joined5.coalesce(1).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI)","user":"anonymous","dateUpdated":"2017-10-13T14:13:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1507883348391_265872946","id":"20170929-123834_882164555","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T14:13:36+0000","dateFinished":"2017-10-13T15:07:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:209"},{"text":"%pyspark\n\n# Clean up some objects to free memory if needed!\ncount_rdd.unpersist()\ncount_df.unpersist()\nbool_rdd.unpersist()\nbool_df.unpersist()\nin_degrees.unpersist()\nout_degrees.unpersist()\npld_edges_df.unpersist()\npld_bf_distrib.unpersist()\n\n# Encourage a garbage collection!\nimport gc\ncollected = gc.collect()\nprint \"Garbage collector: collected %d objects.\" % collected","user":"anonymous","dateUpdated":"2017-10-13T15:13:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Garbage collector: collected 578 objects.\n"}]},"apps":[],"jobName":"paragraph_1507883348392_263949202","id":"20170930-084538_879594277","dateCreated":"2017-10-13T08:29:08+0000","dateStarted":"2017-10-13T15:13:55+0000","dateFinished":"2017-10-13T15:13:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:210"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-10-13T08:40:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507883348393_263564453","id":"20171012-142038_800861977","dateCreated":"2017-10-13T08:29:08+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:211"}],"name":"Paul 5 - faster domain summary","id":"2CUPGC9RU","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}