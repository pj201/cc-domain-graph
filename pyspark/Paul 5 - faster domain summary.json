{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to create domain summaries based on the May/Jun/Jul 2017 CommonCrawl graph\n# as per description here: http://commoncrawl.org/2017/08/webgraph-2017-may-june-july/\n# PJ - 11 October 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n#LIMIT=10000000 # Temporary limit while developing code.\n\n# Import the PLD vertices list as a DataFrame\n#pld_schema=StructType([StructField(\"ID\", StringType(), False), StructField(\"PLD\", StringType(), False)])\n#pld_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices.txt.gz\")\n#temp_pld = pld_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#pld_df=temp_pld.toDF(pld_schema) #.limit(LIMIT) #.repartition(4)\n#pld_df.show(3)\n\n# Load in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\n#pld_df.coalesce(64).write.save(saveURI) # Use all default options\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\nprint(pld_df.count()) # Should have 91M domains","user":"anonymous","dateUpdated":"2017-10-11T14:36:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\n91034128\n"}]},"apps":[],"jobName":"paragraph_1507732119062_1203980067","id":"20170929-081624_672091334","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:36:00+0000","dateFinished":"2017-10-11T14:37:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:486"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame\n#pld_edges_schema=StructType([StructField(\"src\", LongType(), False), StructField(\"dst\", LongType(), False)])\n#pld_edges_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges.txt.gz\")\n#temp_edges_pld = pld_edges_txt.map(lambda k: map(int, k.split())) # By default, splits on whitespace, which is what we want\n#pld_edges_df=temp_edges_pld.toDF(pld_edges_schema) #.limit(LIMIT*10) #.repartition(8)\n#pld_edges_df.show(3)\n\n# Load in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\n#pld_edges_df.coalesce(64).write.save(saveURI) # Use all default options\npld_edges_df=spark.read.load(saveURI)\npld_edges_df.show(3)\npld_edges_df.cache()","user":"anonymous","dateUpdated":"2017-10-11T14:36:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+\n|src|     dst|\n+---+--------+\n|  2| 9193244|\n| 20|75600973|\n| 21|46356172|\n+---+--------+\nonly showing top 3 rows\n\nDataFrame[src: bigint, dst: bigint]\n"}]},"apps":[],"jobName":"paragraph_1507732119071_1200517326","id":"20170929-095050_1324183281","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:36:02+0000","dateFinished":"2017-10-11T14:38:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:487"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\n#host_schema=StructType([StructField(\"hostid\", StringType(), False), StructField(\"host\", StringType(), False)])\n#host_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices.txt.gz\")\n#temp_host = host_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#host_df=temp_host.toDF(host_schema) #.repartition(4)\n#host_df.show(3)\n\n# Save in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\n#host_df.coalesce(128).write.save(saveURI) # Use all default options\nhost_df=spark.read.load(saveURI)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-------+\n|hostid|   host|\n+------+-------+\n|     0|  aaa.a|\n|     1| aaa.aa|\n|     2|aaa.aaa|\n+------+-------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1507732119072_1186281617","id":"20170929-095310_1201506389","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:37:53+0000","dateFinished":"2017-10-11T14:38:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:488"},{"text":"%pyspark\n\n# Load in all harmonic centrality and page-ranks, and join based on reverse domain name\n# Format: #hc_pos #hc_val #pr_pos #pr_val #host_rev\n#pr_schema=StructType([StructField(\"hc_pos\", StringType(), False), StructField(\"hc_val\", StringType(), False), StructField(\"pr_pos\", StringType(), False), StructField(\"pr_val\", StringType(), False), StructField(\"host_rev\", StringType(), False)])\n#pr_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/ranks.txt.gz\")\n#header=pr_txt.first()\n#pr_txt=pr_txt.filter(lambda x: x!=header)\n#temp_pr = pr_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#pr_df=temp_pr.toDF(header.split()).withColumnRenamed(\"#host_rev\",\"host_rev\") #.limit(LIMIT*10) #.repartition(8)\n#pr_df.show(3)\n\n# Save in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/ranks/\"\n#pr_df.coalesce(64).write.save(saveURI) # Use all default options\npr_df=spark.read.load(saveURI)\npr_df.show(3)\npr_df.cache()\n#pr_df.count() # Should be 91M","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------+-------+-------------------+--------------+\n|#hc_pos| #hc_val|#pr_pos|            #pr_val|      host_rev|\n+-------+--------+-------+-------------------+--------------+\n|      1|24989952|      1| 0.0155264576161686|  com.facebook|\n|      2|22460880|      3|0.00866038900847366|   com.twitter|\n|      3|22097514|      2| 0.0128827315785546|com.googleapis|\n+-------+--------+-------+-------------------+--------------+\nonly showing top 3 rows\n\nDataFrame[#hc_pos: string, #hc_val: string, #pr_pos: string, #pr_val: string, host_rev: string]\n"}]},"apps":[],"jobName":"paragraph_1507732119072_1186281617","id":"20170929-093202_1772383833","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:38:08+0000","dateFinished":"2017-10-11T14:38:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:489"},{"text":"%pyspark\n\n# Debug partitioning of our 4 big dataframes\nsc.getConf().getAll() #.mkString(\"\\n\")\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())\npr_df.rdd.getNumPartitions()","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"128\n128\n128\n128\n"}]},"apps":[],"jobName":"paragraph_1507732119073_1185896868","id":"20171006-161509_1868852031","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:38:33+0000","dateFinished":"2017-10-11T14:38:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:490"},{"text":"%pyspark #--packages graphframes:graphframes:0.5.0-spark2.1-s_2.11\n\n# We now have everything we need in these four dataframes to create the summaries we need.\n\n# This code can't handle the complete edge lists, and produces this exception:\n# java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n#out_degrees_=dict(pld_edges_df.groupBy(\"src\").count().collect())\n#in_degrees=dict(pld_edges_df.groupBy(\"dst\").count().collect())\n#print(out_degrees['846558'])\n#print(in_degrees['846558'])\n\n# Instead, just create RDDs and use lookup()\nout_degrees=pld_edges_df.groupBy(\"src\").count()\nin_degrees=pld_edges_df.groupBy(\"dst\").count()\npld_edges_df.unpersist()\nout_degrees.show(3)\nin_degrees.show(3)\n#print(out_degrees.rdd.lookup(846558))\n#print(in_degrees.rdd.lookup(846558))","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|src|count|\n+---+-----+\n| 26|    2|\n| 29|   34|\n|964|    1|\n+---+-----+\nonly showing top 3 rows\n\n+--------+-----+\n|     dst|count|\n+--------+-----+\n|      29|   40|\n|36750820|    5|\n|61427989| 3242|\n+--------+-----+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1507732119074_1187051115","id":"20170929-095727_1596943627","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:38:47+0000","dateFinished":"2017-10-11T14:47:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:491"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when counting hosts\n# This code can't handle the full PLD list and produces this exception:\n# Stack trace: ExitCodeException exitCode=52\n#pld_lookup_table=dict(pld_df.rdd.map(lambda x: (x['PLD'], x['ID'])).collect()) # Bad!\n#print(pld_lookup_table[\"aaa.aaa\"])\n\n# Instead, just create an RDD and use lookup()\n#pld_lookup_table=pld_df.rdd.map(lambda x: (x['PLD'], x['ID']))\n#print(pld_lookup_table.lookup(\"aaa.aaa\")) # Very bad!\n\n# Or let's try creating as a BloomFilter, since we only want to record presence of a PLD\n#pld_bf = pld_df.stat.bloomFilter(\"PLD\", expectedNumItems, fpp) # Doesn't exist in pyspark API!\n#pld_bf.mightContain(\"aaa.aaa\")\n\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.limit(10000000).rdd.collect(): # TODO: Still bad (and exceeds spark.driver.MaxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\nprint(pld_df.rdd.take(3))\nprint(pld_df.rdd.take(3)[2]['PLD'])\n#pld_bf.add(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\n# TODO: Fix this distributed BloomFilter iplementation - can't figure out how to properly combine BFs in a reduce operation!\n#tmp=pld_df.rdd.map(lambda x: pld_bf.add(x['PLD'])) # Very bad - pld_bf gets copied to each of the workers then discarded!\n#tmp=pld_df.rdd.map(lambda x: (pld_bf.add(x['PLD']), pld_bf)).reduce(lambda x,y: x[1].union(y[1])) # Should work but complains that BloomFilter isn't iterable!\n#print(tmp.take(3))\n#print(tmp.count()) # Ensure it runs the map across the entire dataframe\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(ID=u'0', PLD=u'aaa.a'), Row(ID=u'1', PLD=u'aaa.aa'), Row(ID=u'2', PLD=u'aaa.aaa')]\naaa.aaa\nFalse\n64\n9970668\nFalse\nFalse\nFalse\nFalse\n"}]},"apps":[],"jobName":"paragraph_1507732119074_1187051115","id":"20170929-100048_2070118110","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:38:47+0000","dateFinished":"2017-10-11T14:51:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:492"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ERROR\nFalse\n"}]},"apps":[],"jobName":"paragraph_1507732119075_1186666366","id":"20171004-091447_4214261","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:47:08+0000","dateFinished":"2017-10-11T14:51:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:493"},{"text":"%pyspark\n\n# Now count the number of hosts per PLD in a scalable way, and create another dictionary\n# Still takes over an hour since host_df contains 1.3B rows but should complete without errors.\n# (An attempt to collectAsMap at the end results in java Integer.MAX_VALUE or memory errors!)\ncount_rdd=host_df.drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),1)).reduceByKey(lambda x,y: x+y) #.collectAsMap() # Consider using a CountMin sketch here in future?\nbool_rdd=host_df.drop('hostid').rdd.map(lambda x: (x['host'], is_a_pld(x['host']))).filter(lambda x: x[1]==True) #.collectAsMap() # Only outputs PLD hosts (so <91M rows)\n\nprint(count_rdd.take(3))\nprint(bool_rdd.take(3))\nprint(count_rdd.count())\nprint(bool_rdd.count())\n\nhost_df.unpersist()\n\n# Debugging\n#print(count_rdd.filter(lambda x: x['['aaa.aaa'])\n#print(bool_table['aaa.aaa'])\n#print(count_table['ERROR']) # Should be zero once we've loaded all the PLDs!\n\n# TODO: Fix error in collect()\n# java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[(u'com.topratedmichaelkorsoutlet', 1), (u'com.shoppingviponline', 1), (u'com.xinjistone', 1)]\n[(u'com.sergioarboleda', True), (u'com.sergioarcaya', True), (u'com.sergioarevalo', True)]\n9970669\n9835722\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1507732119076_1184742622","id":"20171004-092350_1522843259","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:51:38+0000","dateFinished":"2017-10-11T17:12:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:494"},{"text":"%pyspark\n\n#from pyspark.sql.types import IntegerType\n#from pyspark.sql.functions import udf, col, when, lit\n\n# This code works well when the data is small enough to collect into a python dictionary but our data is too big.\n\n# Define a UDF to perform column-based lookup\n#def translate(mapping):\n#    def translate_(col):\n#        if not mapping.get(col):\n#            return 0\n#        else:\n#            return mapping.get(col)\n#    return udf(translate_, IntegerType())\n\n# And a similar function for the Boolean map\n#def translate_bool(mapping):\n#    def translate_bool_(col):\n#        if not mapping.get(col):\n#            return False\n#        else:\n#            return mapping.get(col)\n#    return udf(translate_bool_, BooleanType())\n    \n# Insert our count column back into the host summary dataframe, along with a boolean to say whether the PLD is a host in itself\n# While we're at it, let's add in the in and out-degrees too, and an indicator of whether the site has been crawled.\n#crawled_test=when(col(\"OutDegree\")==0, lit(False)).otherwise(lit(True))\n#pld_df_joined=pld_df.withColumn('NumHosts', translate(count_table)(\"PLD\"))\\\n                    #.withColumn('PLDisHost?', translate_bool(bool_table)(\"PLD\"))\n                    #.withColumn('InDegree', translate(in_degrees)(\"ID\"))\\\n                    #.withColumn('OutDegree', translate(out_degrees)(\"ID\"))\\\n                    #.withColumn('Crawled?', crawled_test)\n                    \n# Instead, just join our NumHosts and IsAPLD RDDs with the original dataframe\ncountschema=StructType([StructField(\"PLD2\", StringType(), False), StructField(\"NumHosts\", LongType(), False)])\ncount_df=count_rdd.toDF(countschema)\ncount_df.show(3)\nboolschema=StructType([StructField(\"PLD2\", StringType(), False), StructField(\"PLDisHost?\", BooleanType(), False)])\nbool_df=bool_rdd.toDF(boolschema)\nbool_df.show(3)\npld_df2=pld_df.join(count_df, count_df.PLD2==pld_df.PLD, \"leftOuter\").drop(\"PLD2\")\npld_df_joined=pld_df2.join(bool_df, bool_df.PLD2==pld_df2.PLD, \"leftOuter\").drop(\"PLD2\")\n\npld_df.unpersist()\npld_df_joined.sort(\"NumHosts\", ascending=False).show(100)\npld_df_joined.cache()","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------+--------+\n|             PLD2|NumHosts|\n+-----------------+--------+\n| com.seslibooking|       1|\n|com.specchiopiuma|       1|\n|  com.tosdownload|       3|\n+-----------------+--------+\nonly showing top 3 rows\n\n+------------------+----------+\n|              PLD2|PLDisHost?|\n+------------------+----------+\n|com.sergioarboleda|      true|\n|  com.sergioarcaya|      true|\n| com.sergioarevalo|      true|\n+------------------+----------+\nonly showing top 3 rows\n\n+--------+--------------------+--------+----------+\n|      ID|                 PLD|NumHosts|PLDisHost?|\n+--------+--------------------+--------+----------+\n|49161638|          com.zzmmaa|11536574|      true|\n|46884574|       com.w3cdomain|10148585|      true|\n|49160413|          com.zzjxtz| 9339857|      null|\n|47797089|       com.wordpress| 8883925|      true|\n|47797448|       com.wordpress| 8883925|      true|\n|42173462|          com.ssygjy| 7565758|      true|\n|45502088|          com.tumblr| 6637374|      true|\n|45502107|          com.tumblr| 6637374|      true|\n|42654327|     com.sucicurtain| 6566693|      true|\n|41411388|         com.skyrock| 4951840|      true|\n|41411386|         com.skyrock| 4951840|      true|\n|47973485|          com.wwwbfh| 2410107|      true|\n|49150863|          com.zynews| 2340359|      true|\n|46885624|         com.w3snoop| 1792807|      true|\n|46799259|        com.voterlux| 1742478|      true|\n|42803824|    com.superkyuhyun| 1719367|      true|\n|44242732|       com.theoxhorn| 1655357|      true|\n|45657350|     com.typecon2002| 1145362|      true|\n|41305746|       com.siterankd| 1091950|      true|\n|42830597|   com.suprashoescom| 1082105|      true|\n|42449639|    com.stones-trade| 1020527|      true|\n|48125828|  com.xingwangyanhua| 1018003|      true|\n|46672497|      com.visualgrep|  998213|      true|\n|43783647|       com.the3chies|  930889|      true|\n|43781044|     com.the1000club|  828749|      true|\n|45740006|com.uggboots-deut...|  780092|      true|\n|46946024|     com.wallstreety|  763638|      true|\n|47939518|       com.wuhangyee|  643642|      true|\n|46279764|    com.vdoclipmovie|  634531|      true|\n|46150326|    com.vadercentral|  631464|      true|\n|44992181|   com.topuggbootsjp|  600750|      true|\n|45744510|   com.uggsnederland|  599429|      true|\n|45744508|   com.uggsnederland|  599429|      true|\n|47225510|          com.weebly|  593812|      true|\n|47225508|          com.weebly|  593812|      true|\n|47742044|        com.womanask|  563955|      true|\n|47296929|       com.wenpin168|  540955|      true|\n|45695847|       com.ubicmedia|  538615|      true|\n|43168927|      com.taiyingled|  501750|      true|\n|42135942|       com.sre-music|  501190|      true|\n|48582938|         com.ynymbrh|  480693|      true|\n|40839206|     com.shealliance|  480385|      true|\n|43743710|     com.thanks2play|  479767|      true|\n|41921553|     com.spacejordan|  478366|      true|\n|47112361|      com.wearecolin|  477882|      true|\n|43522746|  com.teeniexxxclips|  476680|      true|\n|41430281|       com.sleekbitz|  471572|      true|\n|49114030|          com.zts007|  470453|      true|\n|49055072|         com.znxdszx|  470144|      true|\n|48863824|  com.zapatathemovie|  469456|      true|\n|40921645|     com.shiningnail|  468316|      true|\n|41852088|         com.souqmnf|  466682|      true|\n|47553935|      com.william-li|  464415|      true|\n|45777476|         com.ukprada|  463144|      true|\n|41083613|       com.shzysteel|  462942|      true|\n|42049262|      com.spoonfeedr|  459379|      true|\n|45483517|     com.tudousihome|  457124|      true|\n|40983896|   com.shopbymoncler|  456782|      true|\n|48844840|      com.zaharahaze|  454833|      true|\n|41592165|          com.snoget|  454661|      true|\n|41347146|          com.skarar|  453736|      true|\n|48560226|          com.ykqgpx|  453373|      true|\n|45742406|     com.uggnewshoes|  451428|      true|\n|43434112|       com.teamdoing|  450170|      true|\n|41144884|          com.sijiwl|  450159|      true|\n|43441980|    com.teamnovasoft|  449645|      true|\n|47868047|    com.wowemblem365|  431637|      true|\n|45739614|  com.uggaustraliace|  426465|      true|\n|45649671|       com.tylarmade|  424835|      true|\n|44673366|      com.timesframe|  421863|      true|\n|42313342|     com.steffenmode|  420980|      true|\n|48769095|            com.yuku|  420680|      true|\n|48769099|            com.yuku|  420680|      true|\n|40899839|        com.shida520|  416653|      true|\n|42837387|       com.sure-leds|  413834|      true|\n|40996031|          com.shopl2|  413082|      true|\n|48114655|         com.xibergy|  408290|      true|\n|48618136|      com.yoonhosoul|  397018|      true|\n|48350326|           com.xxhtr|  396558|      true|\n|48168731|          com.xmyyjr|  395531|      true|\n|49012592| com.zipaitoupaiwang|  395386|      true|\n|48961351|        com.zhan-dui|  393420|      true|\n|46297493|  com.vegasmysteries|  391669|      true|\n|48625387|        com.yorubijo|  382961|      true|\n|49154891|          com.zyzsyy|  382743|      true|\n|46405401|          com.vfonex|  376022|      true|\n|44930614|           com.toobt|  374277|      true|\n|43110227|            com.t790|  373985|      true|\n|41076029|      com.shuxiangcn|  365465|      true|\n|47950322|           com.wvamt|  360947|      true|\n|48423989|      com.yanhanhome|  360394|      true|\n|45670750|         com.tzjiuge|  359158|      true|\n|48986909|    com.zhuangxiuapp|  356508|      true|\n|49266024|           cz.booked|  330839|      true|\n|49112755|          com.ztkyhl|  321460|      null|\n|47794541|    com.worddetector|  305116|      true|\n|48467181|          com.ycsthy|  303559|      true|\n|42124715|     com.squarespace|  298593|      true|\n|46940414|      com.wallinside|  297138|      true|\n|47181275|            com.webs|  292218|      true|\n+--------+--------------------+--------+----------+\nonly showing top 100 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean]\n"}]},"apps":[],"jobName":"paragraph_1507732119076_1184742622","id":"20171004-100819_284908525","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T14:51:38+0000","dateFinished":"2017-10-11T18:39:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:495"},{"text":"%pyspark\n\n# Join with in-degree and out-degree dataframes\npld_df_joined2=pld_df_joined.join(out_degrees, out_degrees.src==pld_df_joined.ID, \"leftOuter\").drop(\"src\").withColumnRenamed(\"count\",\"OutDegree\")\npld_df_joined.unpersist()\npld_df_joined3=pld_df_joined2.join(in_degrees, in_degrees.dst==pld_df_joined2.ID, \"leftOuter\").drop(\"dst\").withColumnRenamed(\"count\",\"InDegree\")\npld_df_joined2.unpersist()\npld_df_joined3.show(5)\npld_df_joined3.cache()","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+----------+---------+--------+\n|  ID|                 PLD|NumHosts|PLDisHost?|OutDegree|InDegree|\n+----+--------------------+--------+----------+---------+--------+\n|  26|             abb.nic|    null|      null|        2|       3|\n|  29|abbott.corelabora...|    null|      null|       34|      40|\n| 474|     ac.americancars|    null|      null|     null|       3|\n| 964|              ac.cmt|    null|      null|        1|    null|\n|1677|          ac.insight|    null|      null|        7|       1|\n+----+--------------------+--------+----------+---------+--------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean, OutDegree: bigint, InDegree: bigint]\n"}]},"apps":[],"jobName":"paragraph_1507732119077_1184357873","id":"20171006-152234_1487383953","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T17:12:49+0000","dateFinished":"2017-10-11T19:22:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:496"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import col, when, lit\n\n# Insert a flag to indicate whether the PLD has been crawled\ncrawled_test=when(col(\"OutDegree\").isNull(), lit(False)).otherwise(lit(True))\npld_df_joined4=pld_df_joined3.withColumn('Crawled?', crawled_test)\npld_df_joined3.unpersist()\npld_df_joined4.show(5)\npld_df_joined4.cache()","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+----------+---------+--------+--------+\n|  ID|                 PLD|NumHosts|PLDisHost?|OutDegree|InDegree|Crawled?|\n+----+--------------------+--------+----------+---------+--------+--------+\n|  26|             abb.nic|    null|      null|        2|       3|    true|\n|  29|abbott.corelabora...|    null|      null|       34|      40|    true|\n| 474|     ac.americancars|    null|      null|     null|       3|   false|\n| 964|              ac.cmt|    null|      null|        1|    null|    true|\n|1677|          ac.insight|    null|      null|        7|       1|    true|\n+----+--------------------+--------+----------+---------+--------+--------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean, OutDegree: bigint, InDegree: bigint, Crawled?: boolean]\n"}]},"apps":[],"jobName":"paragraph_1507732119078_1185512119","id":"20171006-155148_681542412","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T18:39:54+0000","dateFinished":"2017-10-11T20:01:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:497"},{"text":"%pyspark\n\n# Finally, join with the harmonic centrality and page-rank for each domain\n# Note: could probably speed this up using something like above techniques, or by presorting (but we don't really need to since this is only 91Mx91M)\npld_df_joined5=pld_df_joined4.join(pr_df, pr_df.host_rev==pld_df_joined4.PLD, \"leftOuter\").drop(\"#hc_pos\").drop(\"#pr_pos\").drop(\"host_rev\")\\\n                             .withColumnRenamed(\"#hc_val\",\"HarmonicCentrality\").withColumnRenamed(\"#pr_val\",\"PageRank\")\npld_df_joined4.unpersist()\npld_df_joined5.show(5)\npld_df_joined5.cache()","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507732119079_1185127370","id":"20170929-122540_264490752","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T19:22:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:498","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+----------+---------+--------+--------+------------------+--------------------+\n|  ID|                 PLD|NumHosts|PLDisHost?|OutDegree|InDegree|Crawled?|HarmonicCentrality|            PageRank|\n+----+--------------------+--------+----------+---------+--------+--------+------------------+--------------------+\n| 120|             abc.web|    null|      null|     null|       1|   false|          10015440|3.78405976859536e-08|\n| 311|             ac.8411|    null|      null|     null|       1|   false|           9082498|4.76481484534919e-09|\n| 713|              ac.bgc|    null|      null|     null|       1|   false|           9237769|4.90517712841288e-09|\n| 871|          ac.casinos|    null|      null|        2|       1|    true|         7839579.5|7.68640254732439e-09|\n|1014|ac.cosmopolitanun...|    null|      null|     null|      18|   false|          12615973|5.85933334251156e-09|\n+----+--------------------+--------+----------+---------+--------+--------+------------------+--------------------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, NumHosts: bigint, PLDisHost?: boolean, OutDegree: bigint, InDegree: bigint, Crawled?: boolean, HarmonicCentrality: string, PageRank: string]\n"}]},"dateFinished":"2017-10-11T20:39:23+0000"},{"text":"%pyspark\n\n# Save final table to S3 in compressed CSV format, broken into smaller files\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_summaries2/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\npld_df_joined5.coalesce(16).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI)","user":"anonymous","dateUpdated":"2017-10-11T14:36:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507732119080_1183203626","id":"20170929-123834_882164555","dateCreated":"2017-10-11T14:28:39+0000","dateStarted":"2017-10-11T20:01:50+0000","status":"RUNNING","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:499"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-10-11T14:36:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507732119081_1182818877","id":"20170930-084538_879594277","dateCreated":"2017-10-11T14:28:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:500"}],"name":"Paul 5 - faster domain summary","id":"2CWFY978R","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}