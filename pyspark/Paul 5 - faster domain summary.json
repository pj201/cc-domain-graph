{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to create domain summaries based on the May/Jun/Jul 2017 CommonCrawl graph\n# as per description here: http://commoncrawl.org/2017/08/webgraph-2017-may-june-july/\n# PJ - 10 October 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n#LIMIT=10000000 # Temporary limit while developing code.\n\n# Import the PLD vertices list as a DataFrame\n#pld_schema=StructType([StructField(\"ID\", StringType(), False), StructField(\"PLD\", StringType(), False)])\n#pld_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices.txt.gz\")\n#temp_pld = pld_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#pld_df=temp_pld.toDF(pld_schema) #.limit(LIMIT) #.repartition(4)\n#pld_df.show(3)\n\n# Load in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\n#pld_df.coalesce(64).write.save(saveURI) # Use all default options\npld_df=spark.read.load(saveURI)\npld_df.show(3)\npld_df.cache()\nprint(pld_df.count()) # Should have 91M domains","user":"anonymous","dateUpdated":"2017-10-10T08:02:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\n91034128\n"}]},"apps":[],"jobName":"paragraph_1507540944225_1403330731","id":"20170929-081624_672091334","dateCreated":"2017-10-09T09:22:24+0000","dateStarted":"2017-10-10T07:49:31+0000","dateFinished":"2017-10-10T07:49:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2049"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame\n#pld_edges_schema=StructType([StructField(\"src\", LongType(), False), StructField(\"dst\", LongType(), False)])\n#pld_edges_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges.txt.gz\")\n#temp_edges_pld = pld_edges_txt.map(lambda k: map(int, k.split())) # By default, splits on whitespace, which is what we want\n#pld_edges_df=temp_edges_pld.toDF(pld_edges_schema) #.limit(LIMIT*10) #.repartition(8)\n#pld_edges_df.show(3)\n\n# Load in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\n#pld_edges_df.coalesce(64).write.save(saveURI) # Use all default options\npld_edges_df=spark.read.load(saveURI)\npld_edges_df.show(3)\npld_edges_df.cache()","user":"anonymous","dateUpdated":"2017-10-10T07:50:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+\n|src|     dst|\n+---+--------+\n|  2| 9193244|\n| 20|75600973|\n| 21|46356172|\n+---+--------+\nonly showing top 3 rows\n\nDataFrame[src: bigint, dst: bigint]\n"}]},"apps":[],"jobName":"paragraph_1507540944228_1402176484","id":"20170929-095050_1324183281","dateCreated":"2017-10-09T09:22:24+0000","dateStarted":"2017-10-10T07:50:23+0000","dateFinished":"2017-10-10T07:50:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2050"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\n#host_schema=StructType([StructField(\"hostid\", StringType(), False), StructField(\"host\", StringType(), False)])\n#host_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices.txt.gz\")\n#temp_host = host_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#host_df=temp_host.toDF(host_schema) #.repartition(4)\n#host_df.show(3)\n\n# Save in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\n#host_df.coalesce(128).write.save(saveURI) # Use all default options\nhost_df=spark.read.load(saveURI)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","user":"anonymous","dateUpdated":"2017-10-10T08:02:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-------+\n|hostid|   host|\n+------+-------+\n|     0|  aaa.a|\n|     1| aaa.aa|\n|     2|aaa.aaa|\n+------+-------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1507540944229_1401791735","id":"20170929-095310_1201506389","dateCreated":"2017-10-09T09:22:24+0000","dateStarted":"2017-10-10T07:51:18+0000","dateFinished":"2017-10-10T07:51:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2051"},{"text":"%pyspark\n\n# Load in all harmonic centrality and page-ranks, and join based on reverse domain name\n# Format: #hc_pos #hc_val #pr_pos #pr_val #host_rev\n#pr_schema=StructType([StructField(\"hc_pos\", StringType(), False), StructField(\"hc_val\", StringType(), False), StructField(\"pr_pos\", StringType(), False), StructField(\"pr_val\", StringType(), False), StructField(\"host_rev\", StringType(), False)])\n#pr_txt=sc.textFile(\"s3://commoncrawl/projects/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/ranks.txt.gz\")\n#header=pr_txt.first()\n#pr_txt=pr_txt.filter(lambda x: x!=header)\n#temp_pr = pr_txt.map(lambda k: k.split()) # By default, splits on whitespace, which is what we want\n#pr_df=temp_pr.toDF(header.split()).withColumnRenamed(\"#host_rev\",\"host_rev\") #.limit(LIMIT*10) #.repartition(8)\n#pr_df.show(3)\n\n# Save in an uncompressed, partitioned format, for fast reading in the future\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/ranks/\"\n#pr_df.coalesce(64).write.save(saveURI) # Use all default options\npr_df=spark.read.load(saveURI)\npr_df.show(3)\npr_df.cache()\n#pr_df.count() # Should be 91M","user":"anonymous","dateUpdated":"2017-10-10T08:02:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------+-------+-------------------+--------------+\n|#hc_pos| #hc_val|#pr_pos|            #pr_val|      host_rev|\n+-------+--------+-------+-------------------+--------------+\n|      1|24989952|      1| 0.0155264576161686|  com.facebook|\n|      2|22460880|      3|0.00866038900847366|   com.twitter|\n|      3|22097514|      2| 0.0128827315785546|com.googleapis|\n+-------+--------+-------+-------------------+--------------+\nonly showing top 3 rows\n\nDataFrame[#hc_pos: string, #hc_val: string, #pr_pos: string, #pr_val: string, host_rev: string]\n"}]},"apps":[],"jobName":"paragraph_1507540944230_1402945982","id":"20170929-093202_1772383833","dateCreated":"2017-10-09T09:22:24+0000","dateStarted":"2017-10-10T07:52:01+0000","dateFinished":"2017-10-10T07:52:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2052"},{"text":"%pyspark\n\n# Debug partitioning of our 4 big dataframes\nsc.getConf().getAll() #.mkString(\"\\n\")\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())\npr_df.rdd.getNumPartitions()","user":"anonymous","dateUpdated":"2017-10-10T07:52:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"64\n64\n117\n64\n"}]},"apps":[],"jobName":"paragraph_1507540944231_1402561233","id":"20171006-161509_1868852031","dateCreated":"2017-10-09T09:22:24+0000","dateStarted":"2017-10-10T07:52:19+0000","dateFinished":"2017-10-10T07:52:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2053"},{"text":"%pyspark #--packages graphframes:graphframes:0.5.0-spark2.1-s_2.11\n\n# We now have everything we need in these four dataframes to create the summaries we need.\n\n# This code can't handle the complete edge lists, and produces this exception:\n# java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n#out_degrees_=dict(pld_edges_df.groupBy(\"src\").count().collect())\n#in_degrees=dict(pld_edges_df.groupBy(\"dst\").count().collect())\n#print(out_degrees['846558'])\n#print(in_degrees['846558'])\n\n# Instead, just create RDDs and use lookup()\nout_degrees=pld_edges_df.groupBy(\"src\").count()\nin_degrees=pld_edges_df.groupBy(\"dst\").count()\npld_edges_df.unpersist()\nout_degrees.show(3)\nin_degrees.show(3)\n#print(out_degrees.rdd.lookup(846558))\n#print(in_degrees.rdd.lookup(846558))","dateUpdated":"2017-10-10T07:52:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+\n|src|count|\n+---+-----+\n| 26|    2|\n| 29|   34|\n|964|    1|\n+---+-----+\nonly showing top 3 rows\n\n+--------+-----+\n|     dst|count|\n+--------+-----+\n|      29|   40|\n|36750820|    5|\n|61427989| 3242|\n+--------+-----+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1507540944232_1400637488","id":"20170929-095727_1596943627","dateCreated":"2017-10-09T09:22:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2054","user":"anonymous","dateFinished":"2017-10-10T07:56:33+0000","dateStarted":"2017-10-10T07:52:34+0000"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when counting hosts\n# This code can't handle the full PLD list and produces this exception:\n# Stack trace: ExitCodeException exitCode=52\n#pld_lookup_table=dict(pld_df.rdd.map(lambda x: (x['PLD'], x['ID'])).collect())\n#print(pld_lookup_table[\"aaa.aaa\"])\n\n# Instead, just create an RDD and use lookup()\n#pld_lookup_table=pld_df.rdd.map(lambda x: (x['PLD'], x['ID']))\n#print(pld_lookup_table.lookup(\"aaa.aaa\"))\n\n# Or let's try creating as a BloomFilter, since we only want to record presence of a PLD\nexpectedNumItems=91000000 \nfpp=0.005\n#pld_bf = pld_df.stat.bloomFilter(\"PLD\", expectedNumItems, fpp) # Doesn't exist in pyspark API!\n#pld_bf.mightContain(\"aaa.aaa\")\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=expectedNumItems, error_rate=fpp)\nfor row in pld_df.rdd.collect(): # TODO: Do this as a map with a function?\n    pld_bf.add(row['PLD'])\n\nprint(\"aaa.aaa\" in pld_bf)\nprint(\"aaa.aaa.bla\" in pld_bf)\n\n# Next, broadcast this map so it's available on all the slave nodes - this seems to break access later!\npld_bf_distrib=sc.broadcast(pld_bf)","user":"anonymous","dateUpdated":"2017-10-10T07:57:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7944601921887552320.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7944601921887552320.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 5, in <module>\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 809, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 70 tasks (8.2 GB) is bigger than spark.driver.maxResultSize (8.0 GB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1690)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1678)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1677)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1677)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:855)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:855)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1905)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1849)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]},"apps":[],"jobName":"paragraph_1507540944233_1400252740","id":"20170929-100048_2070118110","dateCreated":"2017-10-09T09:22:24+0000","dateStarted":"2017-10-10T07:57:24+0000","dateFinished":"2017-10-10T08:00:13+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2055"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n        \n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\"))","dateUpdated":"2017-10-10T08:02:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7944601921887552320.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-7944601921887552320.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 22, in <module>\n  File \"<stdin>\", line 7, in convert_hostname\nNameError: global name 'pld_bf_distrib' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1507540944234_1401406986","id":"20171004-091447_4214261","dateCreated":"2017-10-09T09:22:24+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2056","user":"anonymous","dateFinished":"2017-10-10T08:02:54+0000","dateStarted":"2017-10-10T08:02:54+0000"},{"text":"%pyspark\n\n# Now count the number of hosts per PLD in a scalable way, and create another dictionary\n# Takes 5mins for first 10M rows -> approx 8 hours for all 1.3B rows?\ncount_table=host_df.drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),1)).reduceByKey(lambda x,y: x+y).collectAsMap() # TODO: No collect, Join! CountMin sketch?\nbool_table=host_df.drop('hostid').rdd.map(lambda x: (x['host'], is_a_pld(x['host']))).filter(lambda x: x[1]==True).collectAsMap() # TODO: Do we need this?\nhost_df.unpersist()\nprint(count_table['aaa.aaa'])\nprint(bool_table['aaa.aaa'])\nprint(count_table['ERROR']) # Should be zero once we've loaded all the PLDs!\n\n# TODO: Fix error in collect()\n# java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE\n","dateUpdated":"2017-10-09T10:31:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1570, in collectAsMap\n    return dict(self.collect())\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 809, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 65, in deco\n    s = e.java_exception.toString()\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n    answer = self.gateway_client.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/usr/lib64/python2.7/socket.py\", line 451, in readline\n    data = self._sock.recv(self._rbufsize)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 237, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\n"}]},"apps":[],"jobName":"paragraph_1507540944235_1401022237","id":"20171004-092350_1522843259","dateCreated":"2017-10-09T09:22:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2057"},{"text":"%pyspark\n\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import udf, col, when, lit\n\n# Define a UDF to perform column-based lookup\ndef translate(mapping):\n    def translate_(col):\n        if not mapping.get(col):\n            return 0\n        else:\n            return mapping.get(col)\n    return udf(translate_, IntegerType())\n\n# And a similar function for the Boolean map\ndef translate_bool(mapping):\n    def translate_bool_(col):\n        if not mapping.get(col):\n            return False\n        else:\n            return mapping.get(col)\n    return udf(translate_bool_, BooleanType())\n    \n# Insert our count column back into the host summary dataframe, along with a boolean to say whether the PLD is a host in itself\n# While we're at it, let's add in the in and out-degrees too, and an indicator of whether the site has been crawled.\n#crawled_test=when(col(\"OutDegree\")==0, lit(False)).otherwise(lit(True))\npld_df_joined=pld_df.withColumn('NumHosts', translate(count_table)(\"PLD\"))\\\n                    .withColumn('PLDisHost?', translate_bool(bool_table)(\"PLD\"))\n                    #.withColumn('InDegree', translate(in_degrees)(\"ID\"))\\\n                    #.withColumn('OutDegree', translate(out_degrees)(\"ID\"))\\\n                    #.withColumn('Crawled?', crawled_test)\npld_df.unpersist()\npld_df_joined.sort(\"NumHosts\", ascending=False).show(100)\npld_df_joined.cache()","dateUpdated":"2017-10-09T09:22:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 17, in <module>\nNameError: name 'count_table' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1507540944236_1399098493","id":"20171004-100819_284908525","dateCreated":"2017-10-09T09:22:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2058"},{"text":"%pyspark\n\n# Join with in-degree and out-degree dataframes\npld_df_joined2=pld_df_joined.join(out_degrees, out_degrees.src==pld_df_joined.ID, \"leftOuter\").drop(\"src\").withColumnRenamed(\"count\",\"OutDegree\")\npld_df_joined.unpersist()\npld_df_joined3=pld_df_joined2.join(in_degrees, in_degrees.dst==pld_df_joined2.ID, \"leftOuter\").drop(\"dst\").withColumnRenamed(\"count\",\"InDegree\")\npld_df_joined2.unpersist()\npld_df_joined3.show(5)\npld_df_joined3.cache()","dateUpdated":"2017-10-09T09:22:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'pld_df_joined' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1507540944237_1398713744","id":"20171006-152234_1487383953","dateCreated":"2017-10-09T09:22:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2059"},{"text":"%pyspark\n\n# Insert a flag to indicate whether the PLD has been crawled\ncrawled_test=when(col(\"OutDegree\").isNull(), lit(False)).otherwise(lit(True))\npld_df_joined4=pld_df_joined3.withColumn('Crawled?', crawled_test)\npld_df_joined3.unpersist()\npld_df_joined4.show(5)\npld_df_joined4.cache()","dateUpdated":"2017-10-09T09:22:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\nNameError: name 'pld_df_joined3' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1507540944238_1399867991","id":"20171006-155148_681542412","dateCreated":"2017-10-09T09:22:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2060"},{"text":"%pyspark\n\n# Finally, join with the harmonic centrality and page-rank for each domain\n# Note: could probably speed this up using something like above techniques, or by presorting (but we don't really need to since this is only 91Mx91M)\npld_df_joined5=pld_df_joined4.join(pr_df, pr_df.host_rev==pld_df_joined4.PLD, \"leftOuter\").drop(\"#hc_pos\").drop(\"#pr_pos\").drop(\"host_rev\")\\\n                             .withColumnRenamed(\"#hc_val\",\"HarmonicCentrality\").withColumnRenamed(\"#pr_val\",\"PageRank\")\npld_df_joined4.unpersist()\npld_df_joined5.show(5)\npld_df_joined5.cache()","dateUpdated":"2017-10-09T09:22:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'pld_df_joined4' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1507540944239_1399483242","id":"20170929-122540_264490752","dateCreated":"2017-10-09T09:22:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2061"},{"text":"%pyspark\n\n# Save final table to S3 in compressed CSV format, broken into smaller files\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_summaries2/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\npld_df_joined5.coalesce(16).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI)","dateUpdated":"2017-10-09T09:22:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9191304803286420325.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 3, in <module>\nNameError: name 'pld_df_joined5' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1507540944240_1409871462","id":"20170929-123834_882164555","dateCreated":"2017-10-09T09:22:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2062"},{"text":"%pyspark\n","dateUpdated":"2017-10-09T09:22:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1507540944241_1409486713","id":"20170930-084538_879594277","dateCreated":"2017-10-09T09:22:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2063"}],"name":"Paul 5 - faster domain summary","id":"2CUAUW6FN","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}