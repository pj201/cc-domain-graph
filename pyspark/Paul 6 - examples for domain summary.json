{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to extract host and in/out-link examples for each of the PLDs in the CommonCrawl webgraph\n# Complements summaries produced in 'Paul 5', and gets combined with these in 'Paul 7'.\n# PJ - 25 October 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Load the saved files from Paul 5.\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\npld_df=spark.read.load(loadURI)\npld_df.show(3)\npld_df.cache()\n#print(pld_df.count()) # Should have 91M domains","user":"anonymous","dateUpdated":"2017-10-25T17:03:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\nDataFrame[ID: string, PLD: string]\n"}]},"apps":[],"jobName":"paragraph_1508951015181_1675734501","id":"20170929-081624_672091334","dateCreated":"2017-10-25T17:03:35+0000","dateStarted":"2017-10-25T17:03:46+0000","dateFinished":"2017-10-25T17:04:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:431"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame - i.e. in/out links\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\npld_edges_df=spark.read.load(loadURI)\npld_edges_df.show(3)\npld_edges_df.cache()","user":"anonymous","dateUpdated":"2017-10-25T17:04:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+\n|src|     dst|\n+---+--------+\n|  2| 9193244|\n| 20|75600973|\n| 21|46356172|\n+---+--------+\nonly showing top 3 rows\n\nDataFrame[src: bigint, dst: bigint]\n"}]},"apps":[],"jobName":"paragraph_1508951015185_1686507471","id":"20170929-095050_1324183281","dateCreated":"2017-10-25T17:03:35+0000","dateStarted":"2017-10-25T17:04:42+0000","dateFinished":"2017-10-25T17:04:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:432"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\nhost_df=spark.read.load(saveURI).repartition(64)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","user":"anonymous","dateUpdated":"2017-10-25T17:04:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+--------------------+\n| hostid|                host|\n+-------+--------------------+\n|9110117|br.mus.musicas.di...|\n|9110181|br.mus.musicas.di...|\n|9110245|br.mus.musicas.di...|\n+-------+--------------------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1508951015197_1681890484","id":"20170929-095310_1201506389","dateCreated":"2017-10-25T17:03:35+0000","dateStarted":"2017-10-25T17:04:49+0000","dateFinished":"2017-10-25T17:06:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:433"},{"text":"%pyspark\n\n# Debug partitioning of our 3 big dataframes\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())","user":"anonymous","dateUpdated":"2017-10-25T17:04:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"64\n64\n64\n"}]},"apps":[],"jobName":"paragraph_1508951015198_1683044730","id":"20171006-161509_1868852031","dateCreated":"2017-10-25T17:03:35+0000","dateStarted":"2017-10-25T17:04:52+0000","dateFinished":"2017-10-25T17:06:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:434"},{"text":"%pyspark #--packages graphframes:graphframes:0.5.0-spark2.1-s_2.11\n\n# First, extract in and out-link examples.\n\n# Use window functions to extract 10 examples of links per PLD\n#from pyspark.sql import Window\nfrom pyspark.sql.functions import col, rand, row_number, collect_list\n\n#w_src=Window.partitionBy(\"src\")\n#w_dst=Window.partitionBy(col(\"dst\"))\n\n# Add row number over window and use this to limit to 10 observations per PLD\n#out_degree_examples=pld_edges_df.withColumn(\"rn_\", row_number().over(w_src).where(\"rn_\"<= 10)).drop(\"rn_\")\n\nout_degree_examples=pld_edges_df.groupBy(\"src\").agg(collect_list(\"dst\")) # TODO: Limit to max 10 IDs per PLD\n\n#print(collect_list(\"dst\"))\n#in_degree_examples=pld_edges_df.withColumn(\"rn_\", row_number().over(w_dst)).where(col(\"rn_\") <= 10).drop(\"rn_\")\n\n# TODO: Convert all IDs back to unreversed PLDs, using a HashMap of PLDs\n\n#pld_edges_df.unpersist()\nout_degree_examples.show(30)\n#in_degreee_examples.show(30)","dateUpdated":"2017-10-25T17:03:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+--------------------+\n|  src|   collect_list(dst)|\n+-----+--------------------+\n|   26| [9258593, 46356172]|\n|   29|[42, 51, 53, 6959...|\n|  964|          [24231253]|\n| 1677|[21860195, 242312...|\n| 1806|[11476464, 196987...|\n| 2214|[467998, 469587, ...|\n| 2453|[14494197, 242312...|\n| 2509|[14269322, 315273...|\n| 3506|[21040110, 242312...|\n| 3764|          [64984045]|\n| 4590|[24230475, 242312...|\n| 4823|[2080986, 8025718...|\n| 4894|[19457650, 242312...|\n| 5385|[17908633, 218601...|\n| 9715|[14494197, 206412...|\n| 9945|[21860195, 242312...|\n|10156|[21860195, 270317...|\n|11190|[10010, 10646, 11...|\n|11276|          [24372296]|\n|11745|             [22133]|\n|13460|[21860195, 242312...|\n|13518|[25727, 31974, 10...|\n|13723|[12249, 12966, 13...|\n|14117|[21860195, 242298...|\n|14719|[21860195, 27031700]|\n|15057|[19452728, 218601...|\n|15194|   [15299, 24231253]|\n|15322|[21860195, 27031700]|\n|15371|[15363, 21860195,...|\n|15375|[13077, 21934, 21...|\n+-----+--------------------+\nonly showing top 30 rows\n\n"}]},"apps":[],"jobName":"paragraph_1508951015199_1682659982","id":"20170929-095727_1596943627","dateCreated":"2017-10-25T17:03:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:435"},{"text":"%pyspark\n\n# Debugging\n#help(collect_list(\"dst\"))","dateUpdated":"2017-10-25T17:03:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2014516088161793475.py\", line 349, in <module>\n    [code.body[-(nhooks + 1)]])\nIndexError: list index out of range\n"}]},"apps":[],"jobName":"paragraph_1508951015205_1666500528","id":"20171025-075140_290274377","dateCreated":"2017-10-25T17:03:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:436"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when extracting host examples\n\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.rdd.take(10000): #collect(): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\n#print(pld_df.rdd.take(3))\n#print(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","user":"anonymous","dateUpdated":"2017-10-25T17:06:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"True\n64\n9994\nTrue\nFalse\nTrue\nFalse\n"}]},"apps":[],"jobName":"paragraph_1508951015206_1667654774","id":"20170929-100048_2070118110","dateCreated":"2017-10-25T17:03:35+0000","dateStarted":"2017-10-25T17:06:45+0000","dateFinished":"2017-10-25T17:07:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:437"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n\n# TODO: New function here to return only up to 10 host examples to a list for each PLD  \n#def add_10_hostnames_to_list(hostname)\n\n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","user":"anonymous","dateUpdated":"2017-10-25T17:07:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]},"apps":[],"jobName":"paragraph_1508951015207_1667270026","id":"20171004-091447_4214261","dateCreated":"2017-10-25T17:03:35+0000","dateStarted":"2017-10-25T17:07:16+0000","dateFinished":"2017-10-25T17:07:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:438"},{"text":"%pyspark\n\n# TODO: Generate 10 host examples per PLD.\n\n# Now count the number of hosts per PLD in a scalable way, and create another dictionary\n# Still takes over an hour since host_df contains 1.3B rows but should complete without errors.\n# (An attempt to collectAsMap at the end results in java Integer.MAX_VALUE or memory errors!)\n#count_rdd=host_df.drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),1)).reduceByKey(lambda x,y: x+y) #.collectAsMap() # Consider using a CountMin sketch here in future?\n\nhost_example_rdd=host_df.limit(10).drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),x['host'])).reduceByKey(lambda x,y: x.append(y)) # Add successive hosts into a list\nprint(host_example_rdd.take(3))\n\n# TODO: Restrict list of hosts to a maximum of 10\nhost_example10_rdd=host_example_rdd.map(lambda x,y: (x, y[0:10]))\nprint(host_example10_rdd.take(3))\n\n# TODO: Figure out why SparkContext is getting shut down for no apparent reason!!\n#Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n#: org.apache.spark.SparkException: Job 15 cancelled because SparkContext was shut down\n\n#print(host_example_rdd.count())\n#host_df.unpersist()","user":"anonymous","dateUpdated":"2017-10-25T18:18:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1508951015209_1664961532","id":"20171004-092350_1522843259","dateCreated":"2017-10-25T17:03:35+0000","dateStarted":"2017-10-25T17:25:37+0000","dateFinished":"2017-10-25T18:17:20+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:439","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8284019807379807604.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8284019807379807604.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1343, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 992, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job 15 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:861)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:859)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:859)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1929)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1842)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1920)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]}},{"text":"%pyspark\n\n# TODO: Join with in-degree and out-degree example dataframes\npld_df_joined2=pld_df_joined.join(out_degrees, out_degrees.src==pld_df_joined.ID, \"leftOuter\").drop(\"src\").withColumnRenamed(\"count\",\"pldLinksOut\")\npld_df_joined.unpersist()\npld_df_joined3=pld_df_joined2.join(in_degrees, in_degrees.dst==pld_df_joined2.ID, \"leftOuter\").drop(\"dst\").withColumnRenamed(\"count\",\"pldLinksIn\")\npld_df_joined2.unpersist()\npld_df_joined3.show(5)\npld_df_joined3.cache()","dateUpdated":"2017-10-25T17:03:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+-------------+-----------+----------+\n|  ID|                 PLD|numHosts|pldIsHostFlag|pldLinksOut|pldLinksIn|\n+----+--------------------+--------+-------------+-----------+----------+\n|  26|             abb.nic|       3|         true|          2|         3|\n|  29|abbott.corelabora...|       2|         true|         34|        40|\n| 474|     ac.americancars|       1|         true|       null|         3|\n| 964|              ac.cmt|       1|        false|          1|      null|\n|1677|          ac.insight|       1|         true|          7|         1|\n+----+--------------------+--------+-------------+-----------+----------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, numHosts: bigint, pldIsHostFlag: boolean, pldLinksOut: bigint, pldLinksIn: bigint]\n"}]},"apps":[],"jobName":"paragraph_1508951015209_1664961532","id":"20171006-152234_1487383953","dateCreated":"2017-10-25T17:03:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:440"},{"text":"%pyspark\n\n# TODO: Reverse PLD names if this is still needed\nfrom pyspark.sql.functions import udf, col, when, lit\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook.abc\"))\n\nudf_reverse_domain = udf(reverse_domain, StringType())\nsummary_df2=pld_df_joined5.drop(\"ID\").withColumnRenamed(\"PLD\",\"PLD_rev\").withColumn(\"payLevelDomain\",udf_reverse_domain(\"PLD_rev\")).drop(\"PLD_rev\").drop(\"HarmonicCentrality\").drop(\"PageRank\")\nsummary_df2.show(3)\nsummary_df2.cache()","dateUpdated":"2017-10-25T17:03:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"abc.facebook.com\n+--------+-------------+-----------+----------+--------------+--------+--------+--------------+\n|numHosts|pldIsHostFlag|pldLinksOut|pldLinksIn|wasCrawledFlag|  hc_pos|  pr_pos|payLevelDomain|\n+--------+-------------+-----------+----------+--------------+--------+--------+--------------+\n|       1|        false|       null|         1|         false|38991028| 1478887|       web.abc|\n|       1|        false|       null|         1|         false|69935624|36930613|       8411.ac|\n|       1|        false|       null|         1|         false|63729192|32796120|        bgc.ac|\n+--------+-------------+-----------+----------+--------------+--------+--------+--------------+\nonly showing top 3 rows\n\nDataFrame[numHosts: bigint, pldIsHostFlag: boolean, pldLinksOut: bigint, pldLinksIn: bigint, wasCrawledFlag: boolean, hc_pos: string, pr_pos: string, payLevelDomain: string]\n"}]},"apps":[],"jobName":"paragraph_1508951015210_1666115779","id":"20171012-142038_800861977","dateCreated":"2017-10-25T17:03:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:441"},{"text":"%pyspark\n\n# TODO: Save final table to S3 in compressed CSV format, broken into smaller files\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_examples1/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\n#pld_df_joined5.coalesce(1).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI)\npld_df_joined5.write.save(outputURI)","dateUpdated":"2017-10-25T17:03:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1508951015215_1664192034","id":"20171018-175711_441001507","dateCreated":"2017-10-25T17:03:35+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:442"}],"name":"Paul 6 - examples for domain summary","id":"2CXN1RK73","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}