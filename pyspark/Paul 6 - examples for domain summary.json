{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to extract host and in/out-link examples for each of the PLDs in the CommonCrawl webgraph\n# Complements summaries produced in 'Paul 5', and gets combined with these in 'Paul 7'.\n# Recomended config for complete run: 3xr4.8xlarge, and set spark.driver.maxResultSize to 16g\n# PJ - 30 October 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Load the saved files from Paul 5.\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\npld_df_tmp=spark.read.load(loadURI)\npld_df=pld_df_tmp.select(pld_df_tmp.ID.cast(\"long\"),pld_df_tmp.PLD) # Cast IDs from String to LongInt\npld_df.show(3)\npld_df.cache()\n#print(pld_df.count()) # Should have 91M domains","user":"anonymous","dateUpdated":"2017-10-30T13:06:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\nDataFrame[ID: bigint, PLD: string]\n"}]},"apps":[],"jobName":"paragraph_1509368644252_-1244967307","id":"20170929-081624_672091334","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T13:06:39+0000","dateFinished":"2017-10-30T13:07:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6785"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame - i.e. in/out links\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\npld_edges_df=spark.read.load(loadURI).limit(10000000).repartition(16) # TODO: Remove temp limit once avoiding spark context shutdown!!\npld_edges_df.show(3)\npld_edges_df.cache()","user":"anonymous","dateUpdated":"2017-10-30T15:16:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2812599164993814262.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2812599164993814262.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 159, in load\n    return self._df(self._jreader.load(path))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o458.load.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:33)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkSession(SparkInterpreter.java:361)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkSession(SparkInterpreter.java:233)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:826)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.spark.PySparkInterpreter.getSparkInterpreter(PySparkInterpreter.java:564)\norg.apache.zeppelin.spark.PySparkInterpreter.createGatewayServerAndStartScript(PySparkInterpreter.java:208)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:162)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:491)\norg.apache.zeppelin.scheduler.Job.run(Job.java:175)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2320)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:546)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:177)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:177)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:176)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]},"apps":[],"jobName":"paragraph_1509368644256_-1258818268","id":"20170929-095050_1324183281","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T15:16:24+0000","dateFinished":"2017-10-30T15:16:24+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6786"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\nhost_df=spark.read.load(saveURI) #.repartition(64)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","user":"anonymous","dateUpdated":"2017-10-30T13:06:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-------+\n|hostid|   host|\n+------+-------+\n|     0|  aaa.a|\n|     1| aaa.aa|\n|     2|aaa.aaa|\n+------+-------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1509368644256_-1258818268","id":"20170929-095310_1201506389","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T13:07:24+0000","dateFinished":"2017-10-30T13:07:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6787"},{"text":"%pyspark\n\n# Debug partitioning of our 3 big dataframes\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())","user":"anonymous","dateUpdated":"2017-10-30T15:13:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"128\n64\n128\n"}]},"apps":[],"jobName":"paragraph_1509368644256_-1258818268","id":"20171006-161509_1868852031","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T15:13:27+0000","dateFinished":"2017-10-30T15:13:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6788"},{"text":"%pyspark\n\n# Create a dictionary of PLDs (for ID to PLD mapping of in/out links)\npld_dict=pld_df.rdd.collectAsMap()\n\n# Distribute and test\npld_dict_distrib=sc.broadcast(pld_dict)\nprint(pld_dict_distrib.value[2]) # Should be aaa.aaa\n","user":"anonymous","dateUpdated":"2017-10-30T13:06:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\n"}]},"apps":[],"jobName":"paragraph_1509368644257_-1259203017","id":"20171027-121229_2107241041","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T13:07:32+0000","dateFinished":"2017-10-30T13:19:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6789"},{"text":"%pyspark\n\n# Function to lookup and unreverse PLDs\nfrom pyspark.sql.functions import udf\ndef reverse_domain_from_ID(id):\n    domain=pld_dict_distrib.value[id]\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain_from_ID(2002))\nudf_reverse_domain_from_ID = udf(reverse_domain_from_ID, StringType())\n\n#  First, create a new edges dataframe consisting of unreversed PLDs\npld_edges_df2=pld_edges_df.withColumn(\"src2\",udf_reverse_domain_from_ID(\"src\")).drop(\"src\").withColumn(\"dst2\",udf_reverse_domain_from_ID(\"dst\")).drop(\"dst\")\npld_edges_df.unpersist()\npld_edges_df2.show(5)","user":"anonymous","dateUpdated":"2017-10-30T15:13:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"londonmet.ac\n+-------------------+-----------------+\n|               src2|             dst2|\n+-------------------+-----------------+\n|         aus.abbott|        pt.abbott|\n|          ca.abbott|        it.abbott|\n|          ca.abbott|        abbott.pl|\n|diabetescare.abbott|myworkdayjobs.com|\n|          es.abbott|        pt.abbott|\n+-------------------+-----------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1509368644257_-1259203017","id":"20170929-095727_1596943627","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T15:13:41+0000","dateFinished":"2017-10-30T15:14:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6790"},{"text":"%pyspark\n\n# Next use reduceByKey to aggregate and ensure no more than 10 per PLD - note we create a list for the map values (then + appends)\nout_degree_examples=pld_edges_df2.rdd.map(lambda x:(x['src2'],[x['dst2']])).reduceByKey(lambda acc,pld: acc if len(acc)>=10 else acc+pld)\nin_degree_examples=pld_edges_df2.rdd.map(lambda x:(x['dst2'],[x['src2']])).reduceByKey(lambda acc,pld: acc if len(acc)>=10 else acc+pld)\n\n# Convert back to dataframes\nout_schema = StructType([StructField('PLDout', StringType(), False),StructField('outLinkPLDs', StringType(), False)])\nout_degree_examples_df=out_degree_examples.toDF(out_schema)     \nin_schema = StructType([StructField('PLDin', StringType(), False),StructField('inLinkPLDs', StringType(), False)])\nin_degree_examples_df=in_degree_examples.toDF(in_schema)   \n\n# TODO: Investigate slave lost and SparkContext shut down errors with LIMIT>=10M edges above!!\n# Note that the below also works but not sure how to restrict to only 10 IDs per PLD:\n#from pyspark.sql.functions import collect_list\n#out_degree_examples=pld_edges_df.groupBy(\"src\").agg(collect_list(\"dst\"))\n\npld_edges_df2.unpersist()\nout_degree_examples_df.show(10)\nin_degree_examples_df.show(10)","user":"anonymous","dateUpdated":"2017-10-30T15:16:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2812599164993814262.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2812599164993814262.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 8, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 336, in show\n    print(self._jdf.showString(n, 20))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o406.showString.\n: org.apache.spark.SparkException: Job 34 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:861)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:859)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:859)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1929)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1842)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1920)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]},"apps":[],"jobName":"paragraph_1509368644258_-1258048770","id":"20171027-140940_62611545","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T15:14:39+0000","dateFinished":"2017-10-30T15:15:01+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:6791"},{"text":"%pyspark\n\n# Join the In/Out-Link examples together\npld_df_joined=out_degree_examples_df.join(in_degree_examples_df, out_degree_examples_df.PLDout==in_degree_examples_df.PLDin, \"outer\").drop(\"PLDout\")\nout_degree_examples_df.unpersist()\nin_degree_examples_df.unpersist()\npld_df_joined.show(5)\npld_df_joined.cache()\npld_df_joined.count() # Should still be 91M","user":"anonymous","dateUpdated":"2017-10-30T15:12:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+-----------------+-----------------+\n|outLinkPLDs|            PLDin|       inLinkPLDs|\n+-----------+-----------------+-----------------+\n|       null|         00744.ru|          [do.am]|\n|       null|           00w.cc| [ladypopular.ae]|\n|       null|         0430.com|[blogspot.com.ar]|\n|       null|        09661.com|[yogaretreats.ae]|\n|       null|101zenstories.com|         [acs.ac]|\n+-----------+-----------------+-----------------+\nonly showing top 5 rows\n\n580362\n"}]},"apps":[],"jobName":"paragraph_1509368644258_-1258048770","id":"20171030-112424_1733367457","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T15:12:09+0000","dateFinished":"2017-10-30T15:12:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6792"},{"text":"%pyspark\n\n# Debugging\n#help(collect_list(\"dst\"))\n#help(host_df.rdd.reduceByKey(lambda x,y: x+y))\nprint(\"Debug\")","user":"anonymous","dateUpdated":"2017-10-30T13:06:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Debug\n"}]},"apps":[],"jobName":"paragraph_1509368644259_-1258433519","id":"20171025-075140_290274377","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T13:22:58+0000","dateFinished":"2017-10-30T13:23:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6793"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when extracting host examples\n\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.rdd.collect(): #.take(10000): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\n#print(pld_df.rdd.take(3))\n#print(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","user":"anonymous","dateUpdated":"2017-10-30T13:06:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"True\n64\n90751305\nTrue\nFalse\nTrue\nFalse\n"}]},"apps":[],"jobName":"paragraph_1509368644259_-1258433519","id":"20170929-100048_2070118110","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T13:23:01+0000","dateFinished":"2017-10-30T13:49:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6794"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n\n# Function to do the hostname->pld conversion, if the reversed pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","user":"anonymous","dateUpdated":"2017-10-30T15:07:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]},"apps":[],"jobName":"paragraph_1509368644260_-1260357263","id":"20171004-091447_4214261","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T13:23:01+0000","dateFinished":"2017-10-30T13:49:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6795"},{"text":"%pyspark\n\n# Generate 10 host examples per PLD.\n\n# Firstly, define a reverse domain function\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook\"))\n#udf_reverse_domain = udf(reverse_domain, StringType())\n\n# Now reverse all host names after conversion to PLDs (including lookup) but prior to summarization.\n#host_example_rdd=unrev_host_df.rdd.map(lambda x: (convert_hostname(x['host']),[x['host']])).reduceByKey(lambda acc,host: acc if len(acc)>=10 else acc+host)\nhost_example_rdd=host_df.rdd.map(lambda x: (reverse_domain(convert_hostname(x['host'])),[reverse_domain(x['host'])])).reduceByKey(lambda acc,host: acc if len(acc)>=10 else acc+host)\nprint(host_example_rdd.take(20))\n\n#print(host_example_rdd.count())\n#host_df.unpersist()","user":"anonymous","dateUpdated":"2017-10-30T14:35:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509368644260_-1260357263","id":"20171004-092350_1522843259","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T14:35:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6796","dateFinished":"2017-10-30T15:00:19+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"facebook.com\n[(u'kapsisclassiccars.com', [u'kapsisclassiccars.com']), (u'mywpm.com', [u'zdunex25.mywpm.com']), (u'autosportevents.com', [u'autosportevents.com']), (u'alsident.co.uk', [u'alsident.co.uk']), (u'macottawaeid.com', [u'macottawaeid.com']), (u'modnihouse.co.kr', [u'modnihouse.co.kr']), (u'business-co.ru', [u'business-co.ru']), (u'diyworkouts.com', [u'diyworkouts.com']), (u'dovira.kiev.ua', [u'dovira.kiev.ua']), (u'austinstarroofing.com', [u'austinstarroofing.com']), (u'equipemenr.gouv.fr', [u'manche.equipemenr.gouv.fr']), (u'labtechnika.sk', [u'labtechnika.sk']), (u'xiaoliaobaike.cc', [u'xiaoliaobaike.cc']), (u'microgrades.net', [u'microgrades.net']), (u'blackhattersguide.com', [u'blackhattersguide.com']), (u'vibeffe.it', [u'vibeffe.it']), (u'sexsexysexo.com', [u'sexsexysexo.com']), (u'depix.biz', [u'depix.biz']), (u'cdfanclub.com', [u'cdfanclub.com']), (u'cubagoa.com', [u'cubagoa.com'])]\n"}]}},{"text":"%pyspark\n\n#print(host_example_rdd.take(100))\n\n# Convert host examples back to a dataframe\nout_schema = StructType([StructField('PLD', StringType(), False),StructField('hostExamples', StringType(), False)])\nhost_examples_df=host_example_rdd.toDF(out_schema) \nhost_examples_df.show(100)","user":"anonymous","dateUpdated":"2017-10-30T15:02:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509368644261_-1260742012","id":"20171027-113721_1699720093","dateCreated":"2017-10-30T13:04:04+0000","dateStarted":"2017-10-30T15:02:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6797","dateFinished":"2017-10-30T15:02:30+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|                 PLD|        hostExamples|\n+--------------------+--------------------+\n|kapsisclassiccars...|[kapsisclassiccar...|\n|           mywpm.com|[zdunex25.mywpm.com]|\n| autosportevents.com|[autosportevents....|\n|      alsident.co.uk|    [alsident.co.uk]|\n|    macottawaeid.com|  [macottawaeid.com]|\n|    modnihouse.co.kr|  [modnihouse.co.kr]|\n|      business-co.ru|    [business-co.ru]|\n|     diyworkouts.com|   [diyworkouts.com]|\n|      dovira.kiev.ua|    [dovira.kiev.ua]|\n|austinstarroofing...|[austinstarroofin...|\n|  equipemenr.gouv.fr|[manche.equipemen...|\n|      labtechnika.sk|    [labtechnika.sk]|\n|    xiaoliaobaike.cc|  [xiaoliaobaike.cc]|\n|     microgrades.net|   [microgrades.net]|\n|blackhattersguide...|[blackhattersguid...|\n|          vibeffe.it|        [vibeffe.it]|\n|     sexsexysexo.com|   [sexsexysexo.com]|\n|           depix.biz|         [depix.biz]|\n|       cdfanclub.com|     [cdfanclub.com]|\n|         cubagoa.com|       [cubagoa.com]|\n|texasestateofmind...|[texasestateofmin...|\n|    minoanseminar.gr|  [minoanseminar.gr]|\n|       emderforum.de|     [emderforum.de]|\n|    taiteenteen.info|  [taiteenteen.info]|\n|matchwomensfestiv...|[matchwomensfesti...|\n|         duenkhy.com|       [duenkhy.com]|\n|        inndeknip.nl|      [inndeknip.nl]|\n|   gamblechoices.com|[gamblechoices.co...|\n| arnhemcomingsoon.nl|[arnhemcomingsoon...|\n|fa5a8ntasyfootbal...|[fa5a8ntasyfootba...|\n|the15ers-germany-...|[the15ers-germany...|\n|caja-kaeseveredel...|[caja-kaeseverede...|\n|          bjysza.com|        [bjysza.com]|\n|          cultlib.ru|        [cultlib.ru]|\n|     iwebsupport.com|[iwebsupport.com,...|\n|        bytmaster.by|      [bytmaster.by]|\n|              his.lt|            [his.lt]|\n|          mws3032.tk|        [mws3032.tk]|\n| footclanleagues.com|[footclanleagues....|\n|           ucdctm.ro|         [ucdctm.ro]|\n|          ew8db18.cn|[7f5jer6h.ew8db18...|\n|    kartikasposa.com|  [kartikasposa.com]|\n|     jeuxminiclip.fr|   [jeuxminiclip.fr]|\n|          melixia.hu|        [melixia.hu]|\n|plainviewdogpound...|[plainviewdogpoun...|\n|          jucarne.es|        [jucarne.es]|\n|urbatek-promotion.fr|[urbatek-promotio...|\n|xn--ihq84cs22bez5...|[xn--ihq84cs22bez...|\n|alisonharrison.co.uk|[alisonharrison.c...|\n|          myteam.dev| [nbrown.myteam.dev]|\n| smallpressworld.com|[smallpressworld....|\n|         nabet18.org|       [nabet18.org]|\n|         weber-ld.de|       [weber-ld.de]|\n|    crictimenews.com|  [crictimenews.com]|\n|        watergoed.eu|      [watergoed.eu]|\n|             cac.mil|           [cac.mil]|\n|    unicornjones.com|  [unicornjones.com]|\n|   bsidesoftware.com| [bsidesoftware.com]|\n|        vesisorb.com|      [vesisorb.com]|\n|aluminumprofiles....|[aluminumprofiles...|\n|         vetdent.com|       [vetdent.com]|\n|     leticiareig.com|   [leticiareig.com]|\n|     speeddemonz.net|   [speeddemonz.net]|\n|foundationstones....|[foundationstones...|\n|       foxicon.co.za|     [foxicon.co.za]|\n|  c3churchwatson.com|[c3churchwatson.com]|\n|    usd482.k12.ks.us|[usd482.k12.ks.us...|\n|  auto-kaiser-elz.de|[auto-kaiser-elz.de]|\n|      adb-konsult.se|    [adb-konsult.se]|\n|       sterishoe.com|[sterishoe.com, b...|\n|  chezsylviapixel.fr|[chezsylviapixel.fr]|\n|iessantarosadelim...|[iessantarosadeli...|\n| ersatzteil-scout.de|[ersatzteil-scout...|\n|        rosylips.net|      [rosylips.net]|\n|        serna.com.ua|      [serna.com.ua]|\n|badcredithelpkits...|[badcredithelpkit...|\n|   colorsfestival.fi| [colorsfestival.fi]|\n|     zerosumband.com|   [zerosumband.com]|\n|     a-t-travels.com|   [a-t-travels.com]|\n|bagombergandassoc...|[bagombergandasso...|\n|            cfqtw.cn|[cfqtw.cn, 8ew.cf...|\n|profisminkkeszite...|[profisminkkeszit...|\n| responsiveninja.com|[responsiveninja....|\n|    gocreditshop.com|  [gocreditshop.com]|\n|   lindsayoleary.com| [lindsayoleary.com]|\n|        rynoland.com|      [rynoland.com]|\n|   guide-site-web.fr| [guide-site-web.fr]|\n|      keelungfarm.tw|    [keelungfarm.tw]|\n|    arenda-mebeli.su|  [arenda-mebeli.su]|\n|    transdialogue.eu|  [transdialogue.eu]|\n|          parukat.ee|        [parukat.ee]|\n|     pogonia-film.ru|   [pogonia-film.ru]|\n|    ashergroup.co.ke|  [ashergroup.co.ke]|\n|g09c78g4v7i3oz5n7...|[g09c78g4v7i3oz5n...|\n|      viralposti.com|    [viralposti.com]|\n|physiotherapyinot...|[physiotherapyino...|\n|        deamadre.net|      [deamadre.net]|\n|          poehsig.de|        [poehsig.de]|\n|         viva.net.nz|       [viva.net.nz]|\n|communitycentre55...|[communitycentre5...|\n+--------------------+--------------------+\nonly showing top 100 rows\n\n"}]}},{"text":"%pyspark\n\n# Join in/out-link summaries with host examples dataframe\nexample_df=pld_df_joined.join(host_examples_df, pld_df_joined.PLDin==host_examples_df.PLD, \"leftOuter\").drop(\"PLDin\").select(\"PLD\",\"hostExamples\",\"inLinkPLDs\",\"outLinkPLDs\")\nexample_df.show(10)\nexample_df.cache()","user":"anonymous","dateUpdated":"2017-10-30T15:04:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+--------------------+\n|                 PLD|        hostExamples|          inLinkPLDs|         outLinkPLDs|\n+--------------------+--------------------+--------------------+--------------------+\n|         3-60.com.ar|       [3-60.com.ar]| [bamboletta.com.ar]|   [hostal-azul.com]|\n|      4reefer.com.ar|    [4reefer.com.ar]|[4commerce.com.ar...|[4commerce.com.ar...|\n|    aadeporte.com.ar|  [aadeporte.com.ar]|[argentinamateur....|[aatenis.com.ar, ...|\n|       abalok.com.ar|     [abalok.com.ar]|       [abac.com.ar]|[abac.com.ar, max...|\n|       abchoy.com.ar|     [abchoy.com.ar]|[3lclipping.com.a...|[alejandrodgatti....|\n|abogadodefamilia....|[abogadodefamilia...|   [blogspot.com.ar]|    [macromedia.com]|\n|        admission.ac|      [admission.ac]|            [dti.ac]|[moedu.gov.bd, da...|\n|agarratecatalinaw...|[agarratecatalina...|   [blogspot.com.ar]|    [macromedia.com]|\n|            agedi.ae|          [agedi.ae]|[masdar.ac.ae, en...|         [agedi.org]|\n|agroconsultasonli...|[agroconsultasonl...|   [blogspot.com.ar]|[facebook.com, go...|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\nDataFrame[PLD: string, hostExamples: string, inLinkPLDs: string, outLinkPLDs: string]\n"}]},"apps":[],"jobName":"paragraph_1509368644262_-1259587765","id":"20171012-142038_800861977","dateCreated":"2017-10-30T13:04:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6798","dateFinished":"2017-10-30T15:05:20+0000","dateStarted":"2017-10-30T15:04:20+0000"},{"text":"%pyspark\n\n# Save final table to S3 in parquet format, broken into smaller files (for fast reading into Paul 7 that will combine original summaries with examples)\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_examples2/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\nexample_df.coalesce(1).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI) # Single GZIP initially for debugging\n#example_df.write.save(outputURI)","user":"anonymous","dateUpdated":"2017-10-30T15:05:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509368644262_-1259587765","id":"20171018-175711_441001507","dateCreated":"2017-10-30T13:04:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6799","dateFinished":"2017-10-30T15:07:21+0000","dateStarted":"2017-10-30T15:05:53+0000"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-10-30T13:06:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509368644263_-1259972514","id":"20171027-094657_452033266","dateCreated":"2017-10-30T13:04:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6800"}],"name":"Paul 6 - examples for domain summary","id":"2CYHXF43Q","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}