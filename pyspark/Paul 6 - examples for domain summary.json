{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to extract host and in/out-link examples for each of the PLDs in the CommonCrawl webgraph\n# Complements summaries produced in 'Paul 5', and gets combined with these in 'Paul 7'.\n# PJ - 23 October 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Load the saved files from Paul 5.\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\npld_df=spark.read.load(loadURI)\npld_df.show(3)\npld_df.cache()\nprint(pld_df.count()) # Should have 91M domains","dateUpdated":"2017-10-23T12:16:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\n91034128\n"}]},"apps":[],"jobName":"paragraph_1508758800318_1816346176","id":"20170929-081624_672091334","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6964"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame - i.e. in/out links\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\npld_edges_df=spark.read.load(loadURI)\npld_edges_df.show(3)\npld_edges_df.cache()","dateUpdated":"2017-10-23T12:17:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+\n|src|     dst|\n+---+--------+\n|  2| 9193244|\n| 20|75600973|\n| 21|46356172|\n+---+--------+\nonly showing top 3 rows\n\nDataFrame[src: bigint, dst: bigint]\n"}]},"apps":[],"jobName":"paragraph_1508758800318_1816346176","id":"20170929-095050_1324183281","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6965"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\nhost_df=spark.read.load(saveURI).repartition(64)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","dateUpdated":"2017-10-23T12:17:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-----------+\n|hostid|       host|\n+------+-----------+\n|    17|  aaa.bzzzz|\n|    81|   abc.2020|\n|   145|abc.tenmien|\n+------+-----------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1508758800319_1815961427","id":"20170929-095310_1201506389","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6966"},{"text":"%pyspark\n\n# Debug partitioning of our 3 big dataframes\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())","dateUpdated":"2017-10-23T12:17:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"286\n384\n64\n384\n"}]},"apps":[],"jobName":"paragraph_1508758800320_1801725718","id":"20171006-161509_1868852031","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6967"},{"text":"%pyspark #--packages graphframes:graphframes:0.5.0-spark2.1-s_2.11\n\n# We now have everything we need in these three dataframes to create the summaries we need.\n\n# Use window functions to extract 10 examples of links per PLD\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, rand, rowNumber\n\nw_src=Window.partitionBy(col(\"src\"))\nw_dst=Window.partitionBy(col(\"dst\"))\n\n# Add row number over window and use this to limit to 10 observations per PLD\nout_degree_examples=pld_edges_df.withColumn(\"rn_\", rowNumber().over(w_src)).where(col(\"rn_\") <= 10).drop(\"rn_\")\nin_degree_examples=pld_edges_df.withColumn(\"rn_\", rowNumber().over(w_dst)).where(col(\"rn_\") <= 10).drop(\"rn_\")\n\n# TODO: Combine from separate rows into a single list per PLD, something like the following:\n#df.groupBy($\"col1\").agg( collect_list($\"col2\") )\n\npld_edges_df.unpersist()\nout_degree_examples.show(30)\nin_degreee_examples.show(30)","dateUpdated":"2017-10-23T12:13:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-----+\n|     src|count|\n+--------+-----+\n|13208927|    3|\n|13209857|    1|\n|13210113|   12|\n+--------+-----+\nonly showing top 3 rows\n\n+--------+-----+\n|     dst|count|\n+--------+-----+\n|      29|   40|\n|36750820|    5|\n|61427989| 3242|\n+--------+-----+\nonly showing top 3 rows\n\n"}]},"apps":[],"jobName":"paragraph_1508758800320_1801725718","id":"20170929-095727_1596943627","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6968"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when extracting host examples\n\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.rdd.collect(): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\nprint(pld_df.rdd.take(3))\nprint(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","dateUpdated":"2017-10-23T12:08:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[Row(ID=u'0', PLD=u'aaa.a'), Row(ID=u'1', PLD=u'aaa.aa'), Row(ID=u'2', PLD=u'aaa.aaa')]\naaa.aaa\nTrue\n64\n90751305\nTrue\nFalse\nTrue\nFalse\n"}]},"apps":[],"jobName":"paragraph_1508758800320_1801725718","id":"20170929-100048_2070118110","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6969"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n\n# TODO: New function here to add (up to 10) host examples to a list for each PLD  \n\n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","dateUpdated":"2017-10-23T11:43:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]},"apps":[],"jobName":"paragraph_1508758800321_1801340969","id":"20171004-091447_4214261","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6970"},{"text":"%pyspark\n\n# TODO: Generate 10 host examples per PLD.\n\n# Now count the number of hosts per PLD in a scalable way, and create another dictionary\n# Still takes over an hour since host_df contains 1.3B rows but should complete without errors.\n# (An attempt to collectAsMap at the end results in java Integer.MAX_VALUE or memory errors!)\ncount_rdd=host_df.drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),1)).reduceByKey(lambda x,y: x+y) #.collectAsMap() # Consider using a CountMin sketch here in future?\n\nprint(count_rdd.take(3))\nprint(count_rdd.count())\n\nhost_df.unpersist()\n\n# Debugging\nprint(count_rdd.filter(lambda x: x[0]=='aaa.aaa').collect())\nprint(len(count_rdd.filter(lambda x: x[0]=='ERROR').collect())) # Should be zero once we've loaded all the PLDs!","dateUpdated":"2017-10-23T12:12:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[(u'jp.kids-labo', 1), (u'org.g7fyp39crg0065nzu94cq1m4e35k793os', 1), (u'org.palletizer', 1)]\n[(u'cn.bjbworld', True), (u'cn.bjcnw', True), (u'cn.bjcxbz36', True)]\n90839924\n89276336\n[(u'aaa.aaa', 6)]\n1\n"}]},"apps":[],"jobName":"paragraph_1508758800321_1801340969","id":"20171004-092350_1522843259","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6971"},{"text":"%pyspark\n\n# TODO: Join with in-degree and out-degree example dataframes\npld_df_joined2=pld_df_joined.join(out_degrees, out_degrees.src==pld_df_joined.ID, \"leftOuter\").drop(\"src\").withColumnRenamed(\"count\",\"pldLinksOut\")\npld_df_joined.unpersist()\npld_df_joined3=pld_df_joined2.join(in_degrees, in_degrees.dst==pld_df_joined2.ID, \"leftOuter\").drop(\"dst\").withColumnRenamed(\"count\",\"pldLinksIn\")\npld_df_joined2.unpersist()\npld_df_joined3.show(5)\npld_df_joined3.cache()","dateUpdated":"2017-10-23T12:09:49+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----+--------------------+--------+-------------+-----------+----------+\n|  ID|                 PLD|numHosts|pldIsHostFlag|pldLinksOut|pldLinksIn|\n+----+--------------------+--------+-------------+-----------+----------+\n|  26|             abb.nic|       3|         true|          2|         3|\n|  29|abbott.corelabora...|       2|         true|         34|        40|\n| 474|     ac.americancars|       1|         true|       null|         3|\n| 964|              ac.cmt|       1|        false|          1|      null|\n|1677|          ac.insight|       1|         true|          7|         1|\n+----+--------------------+--------+-------------+-----------+----------+\nonly showing top 5 rows\n\nDataFrame[ID: string, PLD: string, numHosts: bigint, pldIsHostFlag: boolean, pldLinksOut: bigint, pldLinksIn: bigint]\n"}]},"apps":[],"jobName":"paragraph_1508758800322_1802495216","id":"20171006-152234_1487383953","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6973"},{"text":"%pyspark\n\n# TODO: Reverse PLD names if this is still needed\nfrom pyspark.sql.functions import udf, col, when, lit\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook.abc\"))\n\nudf_reverse_domain = udf(reverse_domain, StringType())\nsummary_df2=pld_df_joined5.drop(\"ID\").withColumnRenamed(\"PLD\",\"PLD_rev\").withColumn(\"payLevelDomain\",udf_reverse_domain(\"PLD_rev\")).drop(\"PLD_rev\").drop(\"HarmonicCentrality\").drop(\"PageRank\")\nsummary_df2.show(3)\nsummary_df2.cache()","dateUpdated":"2017-10-23T12:10:36+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"abc.facebook.com\n+--------+-------------+-----------+----------+--------------+--------+--------+--------------+\n|numHosts|pldIsHostFlag|pldLinksOut|pldLinksIn|wasCrawledFlag|  hc_pos|  pr_pos|payLevelDomain|\n+--------+-------------+-----------+----------+--------------+--------+--------+--------------+\n|       1|        false|       null|         1|         false|38991028| 1478887|       web.abc|\n|       1|        false|       null|         1|         false|69935624|36930613|       8411.ac|\n|       1|        false|       null|         1|         false|63729192|32796120|        bgc.ac|\n+--------+-------------+-----------+----------+--------------+--------+--------+--------------+\nonly showing top 3 rows\n\nDataFrame[numHosts: bigint, pldIsHostFlag: boolean, pldLinksOut: bigint, pldLinksIn: bigint, wasCrawledFlag: boolean, hc_pos: string, pr_pos: string, payLevelDomain: string]\n"}]},"apps":[],"jobName":"paragraph_1508758800324_1800186722","id":"20171012-142038_800861977","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6976"},{"text":"%pyspark\n\n# TODO: Save final table to S3 in compressed CSV format, broken into smaller files\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_examples1/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\n#pld_df_joined5.coalesce(1).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI)\npld_df_joined5.write.save(outputURI)","dateUpdated":"2017-10-23T12:11:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1508758800326_1800956220","id":"20171018-175711_441001507","dateCreated":"2017-10-23T11:40:00+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6980"}],"name":"Paul 6 - examples for domain summary","id":"2CYQDFKVC","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}