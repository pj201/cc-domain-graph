{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to extract host and in/out-link examples for each of the PLDs in the CommonCrawl webgraph\n# Complements summaries produced in 'Paul 5', and gets combined with these in 'Paul 7'.\n# Recomended config for complete run: 3xr4.8xlarge, and set spark.driver.maxResultSize to 16g\n# PJ - 27 October 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Load the saved files from Paul 5.\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\npld_df_tmp=spark.read.load(loadURI)\npld_df=pld_df_tmp.select(pld_df_tmp.ID.cast(\"long\"),pld_df_tmp.PLD) # Cast IDs from String to LongInt\npld_df.show(3)\npld_df.cache()\n#print(pld_df.count()) # Should have 91M domains","user":"anonymous","dateUpdated":"2017-10-27T17:04:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\nDataFrame[ID: bigint, PLD: string]\n"}]},"apps":[],"jobName":"paragraph_1509100543737_555266335","id":"20170929-081624_672091334","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T17:04:02+0000","dateFinished":"2017-10-27T17:04:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2212"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame - i.e. in/out links\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\npld_edges_df=spark.read.load(loadURI)\npld_edges_df.show(3)\npld_edges_df.cache()","user":"anonymous","dateUpdated":"2017-10-27T17:04:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+\n|src|     dst|\n+---+--------+\n|  2| 9193244|\n| 20|75600973|\n| 21|46356172|\n+---+--------+\nonly showing top 3 rows\n\nDataFrame[src: bigint, dst: bigint]\n"}]},"apps":[],"jobName":"paragraph_1509100543740_554112088","id":"20170929-095050_1324183281","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T17:04:03+0000","dateFinished":"2017-10-27T17:04:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2213"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\nhost_df=spark.read.load(saveURI) #.repartition(64)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-------+\n|hostid|   host|\n+------+-------+\n|     0|  aaa.a|\n|     1| aaa.aa|\n|     2|aaa.aaa|\n+------+-------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1509100543740_554112088","id":"20170929-095310_1201506389","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T17:04:41+0000","dateFinished":"2017-10-27T17:04:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2214"},{"text":"%pyspark\n\n# Debug partitioning of our 3 big dataframes\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"128\n128\n128\n"}]},"apps":[],"jobName":"paragraph_1509100543741_553727339","id":"20171006-161509_1868852031","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T17:04:46+0000","dateFinished":"2017-10-27T17:04:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2215"},{"text":"%pyspark\n\n# Create a dictionary of PLDs (for ID to PLD mapping of in/out links)\npld_dict=pld_df.rdd.collectAsMap()\n\n# Distribute and test\npld_dict_distrib=sc.broadcast(pld_dict)\nprint(pld_dict_distrib.value[2]) # Should be aaa.aaa\n","user":"anonymous","dateUpdated":"2017-10-27T17:04:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509106349282_1282743829","id":"20171027-121229_2107241041","dateCreated":"2017-10-27T12:12:29+0000","dateStarted":"2017-10-27T17:04:48+0000","dateFinished":"2017-10-27T14:39:42+0000","status":"RUNNING","progressUpdateIntervalMs":500,"$$hashKey":"object:2216","errorMessage":""},{"text":"%pyspark\n\n# Function to lookup and unreverse PLDs\nfrom pyspark.sql.functions import udf\ndef reverse_domain_from_ID(id):\n    domain=pld_dict_distrib.value[id]\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain_from_ID(2002))\nudf_reverse_domain_from_ID = udf(reverse_domain_from_ID, StringType())\n\n#  First, create a new edges dataframe consisting of unreversed PLDs\npld_edges_df2=pld_edges_df.withColumn(\"src2\",udf_reverse_domain_from_ID(\"src\")).drop(\"src\").withColumn(\"dst2\",udf_reverse_domain_from_ID(\"dst\")).drop(\"dst\")\npld_edges_df2.show(5)","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509100543741_553727339","id":"20170929-095727_1596943627","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T17:04:48+0000","dateFinished":"2017-10-27T14:43:24+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2217","errorMessage":""},{"text":"%pyspark\n\n# Next use reduceByKey to aggregate and ensure no more than 10 per PLD - note we create a list for the map values (then + appends)\nout_degree_examples=pld_edges_df2.rdd.map(lambda x:(x['src2'],[x['dst2']])).reduceByKey(lambda acc,pld: acc if len(acc)>=10 else acc+pld)\n#in_degree_examples=pld_edges_df2.rdd.map(lambda x:(x['dst2'],[x['src2']])).reduceByKey(lambda acc,pld: acc if len(acc)>=10 else acc+pld)\n\n# Convert back to dataframes\nout_schema = StructType([StructField('PLD', StringType(), False),StructField('OutLinkPLDs', StringType(), False)])\nout_degree_examples_df=out_degree_examples.toDF(out_schema)     \n#in_schema = StructType([StructField('PLDin', StringType(), False),StructField('InLinkPLDs', StringType(), False)])\n#in_degree_examples_df=in_degree_examples.toDF(in_schema)   \n\n# Combine examples for each PLD\n#link_examples_df=out_degree_examples_df.join(in_degree_examples_df, in_degree_examples_df.PLDin==out_degree_examples_df.PLD).drop(\"PLDin\")\n\n# This also works but not sure how to restrict to only 10 IDs per PLD\n#from pyspark.sql.functions import collect_list\n#out_degree_examples=pld_edges_df.groupBy(\"src\").agg(collect_list(\"dst\"))\n\n#pld_edges_df2.unpersist()\nout_degree_examples_df.show(10)\n#link_examples_df.show(10)","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 9, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 336, in show\n    print(self._jdf.showString(n, 20))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o175.showString.\n: org.apache.spark.SparkException: Job 12 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:861)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:859)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:859)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1929)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1842)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1920)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:671)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]},"apps":[],"jobName":"paragraph_1509113380832_1535040740","id":"20171027-140940_62611545","dateCreated":"2017-10-27T14:09:40+0000","dateStarted":"2017-10-27T14:39:42+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2218"},{"text":"%pyspark\n\n# Debugging\n#help(collect_list(\"dst\"))\n#help(host_df.rdd.reduceByKey(lambda x,y: x+y))\nprint(\"Debug\")","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 349, in <module>\n    [code.body[-(nhooks + 1)]])\nIndexError: list index out of range\n"}]},"apps":[],"jobName":"paragraph_1509100543741_553727339","id":"20171025-075140_290274377","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T14:43:24+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2219"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when extracting host examples\n\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.rdd.collect(): #.take(10000): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\n#print(pld_df.rdd.take(3))\n#print(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 3, in <module>\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 809, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2014)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]},"apps":[],"jobName":"paragraph_1509100543742_554881586","id":"20170929-100048_2070118110","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T15:04:37+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2220"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n\n# TODO: New function here to return only up to 10 host examples to a list for each PLD  \n#def add_10_hostnames_to_list(hostname)\n\n# Define a function to do the hostname->pld conversion, if the pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 22, in <module>\n  File \"<stdin>\", line 7, in convert_hostname\nNameError: global name 'pld_bf_distrib' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1509100543743_554496837","id":"20171004-091447_4214261","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T15:04:37+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2221"},{"text":"%pyspark\n\n# TODO: Generate 10 host examples per PLD.\n\n# Now count the number of hosts per PLD in a scalable way, and create another dictionary\n# Still takes over an hour since host_df contains 1.3B rows but should complete without errors.\n# (An attempt to collectAsMap at the end results in java Integer.MAX_VALUE or memory errors!)\n#count_rdd=host_df.drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),1)).reduceByKey(lambda x,y: x+y) #.collectAsMap() # Consider using a CountMin sketch here in future?\n\n# Add successive hosts into a list\n\n# TODO: Reverse all host names prior to this.\nhost_example_rdd=host_df.drop('hostid').rdd.map(lambda x: (convert_hostname(x['host']),[x['host']])).reduceByKey(lambda acc,host: acc if len(acc)>=10 else acc+host)\nprint(host_example_rdd.take(20))\n\n# TODO: Restrict list of hosts to a maximum of 10\n#host_example10_rdd=host_example_rdd.map(lambda x,y: (x, y[0:10]))\n#print(host_example10_rdd.take(3))\n\n# TODO: Figure out why SparkContext is getting shut down for no apparent reason!!\n#Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n#: org.apache.spark.SparkException: Job 15 cancelled because SparkContext was shut down\n\n#print(host_example_rdd.count())\n#host_df.unpersist()","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1608, in reduceByKey\n    return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1834, in combineByKey\n    numPartitions = self._defaultReducePartitions()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2242, in _defaultReducePartitions\n    return self.ctx.defaultParallelism\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 391, in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o247.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:38)\norg.apache.zeppelin.spark.Utils.invokeMethod(Utils.java:33)\norg.apache.zeppelin.spark.SparkInterpreter.createSparkSession(SparkInterpreter.java:361)\norg.apache.zeppelin.spark.SparkInterpreter.getSparkSession(SparkInterpreter.java:233)\norg.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:826)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.spark.PySparkInterpreter.getSparkInterpreter(PySparkInterpreter.java:564)\norg.apache.zeppelin.spark.PySparkInterpreter.createGatewayServerAndStartScript(PySparkInterpreter.java:208)\norg.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:162)\norg.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\norg.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:491)\norg.apache.zeppelin.scheduler.Job.run(Job.java:175)\norg.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\njava.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\njava.util.concurrent.FutureTask.run(FutureTask.java:266)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2320)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]},"apps":[],"jobName":"paragraph_1509100543743_554496837","id":"20171004-092350_1522843259","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T15:04:37+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2222"},{"text":"%pyspark\n\nprint(host_example_rdd.take(100))","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'host_example_rdd' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1509104241910_-1220635542","id":"20171027-113721_1699720093","dateCreated":"2017-10-27T11:37:21+0000","dateStarted":"2017-10-27T15:04:37+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2223"},{"text":"%pyspark\n\n# TODO: Join with example dataframe\npld_df_joined2=pld_df_joined.join(out_degrees, out_degrees.src==pld_df_joined.ID, \"leftOuter\").drop(\"src\").withColumnRenamed(\"count\",\"pldLinksOut\")\npld_df_joined.unpersist()\npld_df_joined3=pld_df_joined2.join(in_degrees, in_degrees.dst==pld_df_joined2.ID, \"leftOuter\").drop(\"dst\").withColumnRenamed(\"count\",\"pldLinksIn\")\npld_df_joined2.unpersist()\npld_df_joined3.show(5)\npld_df_joined3.cache()","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'pld_df_joined' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1509100543744_638756846","id":"20171006-152234_1487383953","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T15:04:37+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2224"},{"text":"%pyspark\n\n# TODO: Reverse PLD names if this is still needed\n\nsummary_df2=pld_df_joined5.drop(\"ID\").withColumnRenamed(\"PLD\",\"PLD_rev\").withColumn(\"payLevelDomain\",udf_reverse_domain(\"PLD_rev\")).drop(\"PLD_rev\").drop(\"HarmonicCentrality\").drop(\"PageRank\")\nsummary_df2.show(3)\nsummary_df2.cache()","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'pld_df_joined5' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1509100543744_638756846","id":"20171012-142038_800861977","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T15:04:37+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2225"},{"text":"%pyspark\n\n# TODO: Save final table to S3 in compressed CSV format, broken into smaller files\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_examples1/\"\ncodec=\"org.apache.hadoop.io.compress.GzipCodec\"\n#pld_df_joined5.coalesce(1).write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI)\npld_df_joined5.write.save(outputURI)","user":"anonymous","dateUpdated":"2017-10-27T17:04:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-871131374168902899.py\", line 360, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 3, in <module>\nNameError: name 'pld_df_joined5' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1509100543745_638372097","id":"20171018-175711_441001507","dateCreated":"2017-10-27T10:35:43+0000","dateStarted":"2017-10-27T15:04:37+0000","dateFinished":"2017-10-27T15:04:37+0000","status":"PENDING","progressUpdateIntervalMs":500,"$$hashKey":"object:2226"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-10-27T14:26:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509100543745_638372097","id":"20171027-094657_452033266","dateCreated":"2017-10-27T10:35:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2227"}],"name":"Paul 6 - examples for domain summary","id":"2CVQSB6XB","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}