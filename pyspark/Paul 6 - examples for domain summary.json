{"paragraphs":[{"text":"%pyspark\n\n# Zeppelin notebook to extract host and in/out-link examples for each of the PLDs in the CommonCrawl webgraph\n# Complements summaries produced in 'Paul 5', and gets combined with these in 'Paul 7'.\n# Recomended config for complete run: 4xr4.8xlarge, and set spark.driver.maxResultSize to 16g\n# PJ - 3 November 2017\n\nimport boto\nfrom pyspark.sql.types import *\n\n# Load the saved files from Paul 5.\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/vertices/\"\npld_df_tmp=spark.read.load(loadURI) #.limit(100) #.repartition(256)\npld_df=pld_df_tmp.select(pld_df_tmp.ID.cast(\"long\"),pld_df_tmp.PLD) # Cast IDs from String to LongInt\npld_df.show(3)\npld_df.cache()\n#print(pld_df.count()) # Should have 91M domains","user":"anonymous","dateUpdated":"2017-11-03T13:00:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+\n| ID|    PLD|\n+---+-------+\n|  0|  aaa.a|\n|  1| aaa.aa|\n|  2|aaa.aaa|\n+---+-------+\nonly showing top 3 rows\n\nDataFrame[ID: bigint, PLD: string]\n"}]},"apps":[],"jobName":"paragraph_1509710570542_-1161060447","id":"20170929-081624_672091334","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:00:03+0000","dateFinished":"2017-11-03T13:00:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7995"},{"text":"%pyspark\n\n# Firstly, define a reverse domain function\nfrom pyspark.sql.functions import udf\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook\"))\nudf_reverse_domain = udf(reverse_domain, StringType())\n\n# Create a reversed version of the PLD dataframe\npld_unrev_df=pld_df.withColumn(\"PLD_unrev\", udf_reverse_domain(\"PLD\")).drop(\"PLD\").withColumnRenamed(\"PLD_unrev\", \"PLD\")\npld_unrev_df.show(5)","user":"anonymous","dateUpdated":"2017-11-03T12:40:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"facebook.com\n+---+----------+\n| ID|       PLD|\n+---+----------+\n|  0|     a.aaa|\n|  1|    aa.aaa|\n|  2|   aaa.aaa|\n|  3|  aaaa.aaa|\n|  4|aaaaaa.aaa|\n+---+----------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1509710570545_-1151441724","id":"20171103-111333_867220084","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T12:12:58+0000","dateFinished":"2017-11-03T12:13:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7996"},{"text":"%pyspark\n\n# Next import the PLD edges as a DataFrame - i.e. in/out links\nloadURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/domaingraph/edges/\"\npld_edges_df=spark.read.load(loadURI) #.limit(1000) #.limit(100000000).repartition(8) # TODO: Remove temp limit once avoiding spark context shutdown!!\npld_edges_df.show(3)\npld_edges_df.cache()","user":"anonymous","dateUpdated":"2017-11-03T12:13:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+--------+\n|src|     dst|\n+---+--------+\n|  2| 9193244|\n| 20|75600973|\n| 21|46356172|\n+---+--------+\nonly showing top 3 rows\n\nDataFrame[src: bigint, dst: bigint]\n"}]},"apps":[],"jobName":"paragraph_1509710570546_-1150287477","id":"20170929-095050_1324183281","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T12:13:07+0000","dateFinished":"2017-11-03T12:13:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7997"},{"text":"%pyspark\n\n# Join dataframes using fast equi-joins (equivalent to reduceByKey on RDDs) - avoids the need to create and broadcast an ID-PLD dictionary\npld_edges_df1=pld_unrev_df.join(pld_edges_df, pld_unrev_df.ID==pld_edges_df.src).drop(\"ID\").drop(\"src\").withColumnRenamed(\"PLD\",\"PLD_src\") #rdd.union(rdd1).reduceByKey(lambda x,y : x+y)\npld_edges_df1.show(5)\npld_edges_df2=pld_edges_df1.join(pld_unrev_df, pld_edges_df1.dst==pld_unrev_df.ID).drop(\"ID\").drop(\"dst\").withColumnRenamed(\"PLD\", \"PLD_dst\")\npld_edges_df2.show(5)","user":"anonymous","dateUpdated":"2017-11-03T12:41:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------+\n|             PLD_src|     dst|\n+--------------------+--------+\n|             nic.abb| 9258593|\n|             nic.abb|46356172|\n|corelaboratory.ab...|      42|\n|corelaboratory.ab...|      51|\n|corelaboratory.ab...|      53|\n+--------------------+--------+\nonly showing top 5 rows\n\n+-------------------+--------------------+\n|            PLD_src|             PLD_dst|\n+-------------------+--------------------+\n|          icann.org|             nic.abb|\n|      wikipedia.org|             nic.abb|\n|       namestat.org|             nic.abb|\n|    websitelists.in|corelaboratory.ab...|\n|hercegovinalijek.ba|corelaboratory.ab...|\n+-------------------+--------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1509710570546_-1150287477","id":"20171103-104722_615033791","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T12:13:28+0000","dateFinished":"2017-11-03T12:16:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7998"},{"text":"%pyspark\n\n# Load the host-level graph vertices in the same way\nsaveURI=\"s3://billsdata.net/CommonCrawl/hyperlinkgraph/cc-main-2017-may-jun-jul/hostgraph/vertices/\"\nhost_df=spark.read.load(saveURI) #.repartition(64)\nhost_df.show(3)\nhost_df.cache()\n#print(host_df.count()) # Should have 1.3B hosts","user":"anonymous","dateUpdated":"2017-11-03T13:00:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+-------+\n|hostid|   host|\n+------+-------+\n|     0|  aaa.a|\n|     1| aaa.aa|\n|     2|aaa.aaa|\n+------+-------+\nonly showing top 3 rows\n\nDataFrame[hostid: string, host: string]\n"}]},"apps":[],"jobName":"paragraph_1509710570547_-1150672226","id":"20170929-095310_1201506389","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:00:29+0000","dateFinished":"2017-11-03T13:00:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7999"},{"text":"%pyspark\n\n# Debug partitioning of our 3 big dataframes\nprint(pld_df.rdd.getNumPartitions())\nprint(pld_edges_df.rdd.getNumPartitions())\nprint(host_df.rdd.getNumPartitions())","user":"anonymous","dateUpdated":"2017-11-03T13:00:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"192\n"},{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1041518579918016243.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-1041518579918016243.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 2, in <module>\nNameError: name 'pld_edges_df' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1509710570548_-1152595971","id":"20171006-161509_1868852031","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:00:58+0000","dateFinished":"2017-11-03T13:00:58+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:8000"},{"text":"%pyspark\n\n# DON'T DO THIS - KILLS EXECUTOR MEMORY ON SLAVES, AND DOESN'T SCALE!\n\n# Create a dictionary of PLDs (for ID to PLD mapping of in/out links)\n#pld_dict=pld_df.rdd.collectAsMap()\n#print(pld_dict[2])\n\n# Distribute and test\n#pld_dict_distrib=sc.broadcast(pld_dict) # Maybe broadcasting this huge map to the slaves is what's causing them to die!\n#print(pld_dict_distrib.value[2]) # Should be aaa.aaa\nprint(\"Avoided badness...\")","user":"anonymous","dateUpdated":"2017-11-03T13:01:11+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Avoided badness...\n"}]},"apps":[],"jobName":"paragraph_1509710570549_-1152980720","id":"20171027-121229_2107241041","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:01:11+0000","dateFinished":"2017-11-03T13:01:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8001"},{"text":"%pyspark\n\n# DON'T DO THIS EITHER...\n\n# Function to lookup and unreverse PLDs\n#from pyspark.sql.functions import udf\n#def reverse_domain_from_ID(id):\n#    domain=pld_dict_distrib.value[id]\n#    #domain=pld_dict[id] # This will be horribly slow since all queries will have to go via the driver but it might work.\n#    return '.'.join(reversed(domain.split('.')))\n#print(reverse_domain_from_ID(2002))\n#udf_reverse_domain_from_ID = udf(reverse_domain_from_ID, StringType())\n\n#  First, create a new edges dataframe consisting of unreversed PLDs\n#pld_edges_df2=pld_edges_df.withColumn(\"src2\",udf_reverse_domain_from_ID(\"src\")).drop(\"src\").withColumn(\"dst2\",udf_reverse_domain_from_ID(\"dst\")).drop(\"dst\")\n##pld_edges_df.unpersist()\n#pld_edges_df2.show(5)\nprint(\"Avoided more badness...\")","user":"anonymous","dateUpdated":"2017-11-03T13:01:12+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Avoided more badness...\n"}]},"apps":[],"jobName":"paragraph_1509710570549_-1152980720","id":"20170929-095727_1596943627","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:01:13+0000","dateFinished":"2017-11-03T13:01:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8002"},{"text":"%pyspark\n\n# Save this new edges dataframe for future use in parquet format, so we can reload later without having the big dict in memory!\n#edgesURI=\"s3://billsdata.net/CommonCrawl/domain_examples_new_edges5/\"\n#pld_edges_df2.write.save(edgesURI)\n#pld_edges_df2=spark.read.load(edgesURI)\n#print(pld_edges_df2.rdd.getNumPartitions())\nprint(\"Code to load/save.\")","user":"anonymous","dateUpdated":"2017-11-03T13:01:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Code to load/save.\n"}]},"apps":[],"jobName":"paragraph_1509710570550_-1151826473","id":"20171102-135417_1202091008","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:01:16+0000","dateFinished":"2017-11-03T13:01:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8003"},{"text":"%pyspark\n\n# Next use reduceByKey to aggregate and ensure no more than 10 per PLD - note we create a list for the map values (then + appends)\nout_schema = StructType([StructField('PLDout', StringType(), False),StructField('exampleOutLinkPlds', StringType(), False)])\nout_degree_examples_df=pld_edges_df2.rdd.map(lambda x:(x['PLD_src'],[x['PLD_dst']])).reduceByKey(lambda acc,pld: acc if len(acc)>=10 else acc+pld).toDF(out_schema) \nout_degree_examples_df.show(10)\n\n# Note that the below also works but not sure how to restrict to only 10 IDs per PLD:\n#from pyspark.sql.functions import collect_list\n#out_degree_examples=pld_edges_df.groupBy(\"src\").agg(collect_list(\"dst\"))","user":"anonymous","dateUpdated":"2017-11-03T12:45:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|              PLDout|  exampleOutLinkPlds|\n+--------------------+--------------------+\n|  luttespaysannes.be|[nihil-obstat.be,...|\n|        biporteur.fr|[nantescargobike....|\n|cooperativadedise...|[vimeo.com, youtu...|\n|     atouchofwood.be|[proxi.tools, pro...|\n|ergonomie-stockag...|           [ovh.net]|\n|       musicordes.ch|      [websaiten.ch]|\n|  thevalleylocal.net|[rrpicturearchive...|\n|ydk-international.de|[deenet.org, wfg-...|\n|   famhillenbrand.eu|             [df.eu]|\n|           fowtcg.de|     [fowsystem.com]|\n+--------------------+--------------------+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1509710570550_-1151826473","id":"20171027-140940_62611545","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T12:18:14+0000","dateFinished":"2017-11-03T12:37:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8004"},{"text":"%pyspark\n\n# Now do the same for in-degrees\nin_schema = StructType([StructField('PLDin', StringType(), False),StructField('exampleInLinkPlds', StringType(), False)])\nin_degree_examples_df=pld_edges_df2.rdd.map(lambda x:(x['PLD_dst'],[x['PLD_src']])).reduceByKey(lambda acc,pld: acc if len(acc)>=10 else acc+pld).toDF(in_schema)   \nin_degree_examples_df.show(10)\npld_edges_df2.unpersist() # Don't need this any more","user":"anonymous","dateUpdated":"2017-11-03T12:38:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|               PLDin|   exampleInLinkPlds|\n+--------------------+--------------------+\n|           y-ota.com|          [hama2.jp]|\n|ianhenrysimmonds....|[pennedinthemargi...|\n|       sexbest24.com|[sharpei-apso.ru,...|\n|beachviewflorists...|[list-of-domains....|\n|   agent-fashion.com|     [mirnevest.com]|\n|        coralclub.ru|[incatalogues.ru,...|\n|        komaroku.com|[websitelists.in,...|\n|      georgesiga.com|[unbelievable-fac...|\n|    eadsummit.com.br|[vidadestartup.or...|\n|  notengowebsite.com|[lunadominante.co...|\n+--------------------+--------------------+\nonly showing top 10 rows\n\nDataFrame[PLD_src: string, PLD_dst: string]\n"}]},"apps":[],"jobName":"paragraph_1509710570551_-1152211222","id":"20171102-120823_1325716344","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T12:38:51+0000","dateFinished":"2017-11-03T12:46:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8005"},{"text":"%pyspark\n\n# Join the In/Out-Link examples together\npld_df_joined=out_degree_examples_df.join(in_degree_examples_df, out_degree_examples_df.PLDout==in_degree_examples_df.PLDin, \"outer\").drop(\"PLDout\")\nout_degree_examples_df.unpersist()\nin_degree_examples_df.unpersist()\npld_df_joined.show(5)\npld_df_joined.cache()\npld_df_joined.count() # Should still be 91M","user":"anonymous","dateUpdated":"2017-11-03T12:46:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+\n|  exampleOutLinkPlds|               PLDin|   exampleInLinkPlds|\n+--------------------+--------------------+--------------------+\n|                null|0----------------...|         [nomina.ru]|\n|[wordpress.org, g...|           0-0la.com|[jessicawilson.co...|\n|                null|           0-3-6.com|[3d114.com, menok...|\n|                null|           0-3ani.ro|[cere.ro, adedir....|\n|                null|           0-5-1.com|    [allthecom.info]|\n+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n90661578\n"}]},"apps":[],"jobName":"paragraph_1509710570551_-1152211222","id":"20171030-112424_1733367457","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T12:46:55+0000","dateFinished":"2017-11-03T12:51:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8006"},{"text":"%pyspark\n\n# Save the in-out examples to S3 so we can just load them next time, and avoid the above expensive processing!\n#pld_df_joined2=pld_df_joined.select(\"PLDin\",\"exampleInLinkPlds\",\"exampleOutLinkPlds\").withColumnRenamed(\"PLDin\",\"PLD\")\nlinkExamplesURI=\"s3://billsdata.net/CommonCrawl/domain_examples_links/\"\n#pld_df_joined.write.save(linkExamplesURI)\npld_df_joined=spark.read.load(linkExamplesURI)\nprint(pld_df_joined.rdd.getNumPartitions())","user":"anonymous","dateUpdated":"2017-11-03T13:01:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"200\n"}]},"apps":[],"jobName":"paragraph_1509710570552_-1154134966","id":"20171025-075140_290274377","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:01:28+0000","dateFinished":"2017-11-03T13:01:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8007"},{"text":"%pyspark\n\n# Next, we'll construct a local dictionary from of all the PLDS (key is the PLD, value is the ID)\n# This is our truth-table of known PLDs that we'll use when extracting host examples\n\n# Create a bloom filter using a pure python package (might be a little slow)\nfrom pybloom import BloomFilter\npld_bf = BloomFilter(capacity=91000000, error_rate=0.005)\n\nfor row in pld_df.rdd.collect(): #.take(10000): # limit(10000000) # TODO: Still bad (and exceeds spark.driver.maxResultSize with all rows)!\n    pld_bf.add(row['PLD'])\n\n#print(pld_df.rdd.take(3))\n#print(pld_df.rdd.take(3)[2]['PLD'])\nprint(\"aaa.aaa\" in pld_bf) # Should be True\n\nimport sys\nprint(sys.getsizeof(pld_bf))\nprint(len(pld_bf)) # Should match number of items entered\n\n# Broadcast the bloom filter so it's available on all the slave nodes - we don't need to change\n# it any more so it's fine being immutable.\npld_bf_distrib=sc.broadcast(pld_bf)\n\nprint(\"aaa.aaa\" in pld_bf) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf) # Should be false\nprint(\"aaa.aaa\" in pld_bf_distrib.value) # Should be true\nprint(\"aaa.aaa.bla\" in pld_bf_distrib.value) # Should be false","user":"anonymous","dateUpdated":"2017-11-03T13:01:33+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"True\n64\n90751305\nTrue\nFalse\nTrue\nFalse\n"}]},"apps":[],"jobName":"paragraph_1509710570552_-1154134966","id":"20170929-100048_2070118110","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:01:33+0000","dateFinished":"2017-11-03T13:27:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8008"},{"text":"%pyspark\n\n# Returns a Boolean to say whether PLD is a hostname in itself\ndef is_a_pld(hostname):\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return True\n    else:\n        return False\n\n# Function to do the hostname->pld conversion, if the reversed pld exists in our dictionary \ndef convert_hostname(hostname):\n    # Return hostname as-is, if this is already a PLD\n    #if hostname in pld_lookup_table:\n    #if pld_lookup_table.filter(lambda a: a == hostname).count()>0:\n    if hostname in pld_bf_distrib.value:\n        return hostname\n    # Otherwise we're going to have to split it up and test the parts\n    try:\n        parts=hostname.split('.')\n        if (len(parts)>4 and is_a_pld('.'.join(parts[0:4]))):\n            return '.'.join(parts[0:4])\n        if (len(parts)>3 and is_a_pld('.'.join(parts[0:3]))):\n            return '.'.join(parts[0:3])\n        if (len(parts)>2 and is_a_pld('.'.join(parts[0:2]))):\n            return '.'.join(parts[0:2])\n        if (len(parts)>1 and is_a_pld('.'.join(parts[0:1]))):\n            return '.'.join(parts[0:1])\n        return \"ERROR\" # Couldn't find a corresponding PLD - this should never happen!\n    except:\n        return \"ERROR\"\n        \n# Test\nprint(convert_hostname(\"aaa.aaa\"))\nprint(is_a_pld(\"aaa.aaa\")) # Should be true","user":"anonymous","dateUpdated":"2017-11-03T13:01:38+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"aaa.aaa\nTrue\n"}]},"apps":[],"jobName":"paragraph_1509710570553_-1154519715","id":"20171004-091447_4214261","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:01:38+0000","dateFinished":"2017-11-03T13:27:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8009"},{"text":"%pyspark\n\n# Generate 10 host examples per PLD.\n\n# Firstly, define a reverse domain function\ndef reverse_domain(domain):\n    return '.'.join(reversed(domain.split('.')))\nprint(reverse_domain(\"com.facebook\"))\n#udf_reverse_domain = udf(reverse_domain, StringType())\n\n# Now reverse all host names after conversion to PLDs (including lookup) but prior to summarization.\n#host_example_rdd=unrev_host_df.rdd.map(lambda x: (convert_hostname(x['host']),[x['host']])).reduceByKey(lambda acc,host: acc if len(acc)>=10 else acc+host)\nhost_example_rdd=host_df.rdd.map(lambda x: (reverse_domain(convert_hostname(x['host'])),[reverse_domain(x['host'])])).reduceByKey(lambda acc,host: acc if len(acc)>=10 else acc+host)\nprint(host_example_rdd.take(20))\n\n#print(host_example_rdd.count())\n#host_df.unpersist()","user":"anonymous","dateUpdated":"2017-11-03T13:01:47+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"facebook.com\n[(u'eddyseel.com', [u'eddyseel.com']), (u'qkglxvbiyphb.com', [u'qkglxvbiyphb.com']), (u'sexbest24.com', [u'sexbest24.com']), (u'cmtcdmrbyzbd.com', [u'cmtcdmrbyzbd.com']), (u'gfxthai.org', [u'gfxthai.org']), (u'sieheunten.de', [u'sieheunten.de']), (u'komaroku.com', [u'komaroku.com', u'novel.komaroku.com', u'test.komaroku.com']), (u'kerbalaya.net', [u'kerbalaya.net']), (u'monclerjacketssales2012.com', [u'monclerjacketssales2012.com']), (u'restaurantdurmitor.com', [u'restaurantdurmitor.com']), (u'violetaorgaz.com', [u'violetaorgaz.com']), (u'fa-altmark.de', [u'fa-altmark.de']), (u'agapelive.co.za', [u'agapelive.co.za']), (u'fccv.org.ve', [u'fccv.org.ve']), (u'vitapharmed.org', [u'vitapharmed.org']), (u'truckandplantonline.com', [u'truckandplantonline.com']), (u'techbrosnetworks.net', [u'techbrosnetworks.net']), (u'libre-planete.fr', [u'libre-planete.fr']), (u'blackhattersguide.com', [u'blackhattersguide.com']), (u'vibeffe.it', [u'vibeffe.it'])]\n"}]},"apps":[],"jobName":"paragraph_1509710570553_-1154519715","id":"20171004-092350_1522843259","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:27:57+0000","dateFinished":"2017-11-03T13:49:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8010"},{"text":"%pyspark\n\n#print(host_example_rdd.take(100))\n\n# Convert host examples back to a dataframe\nout_schema = StructType([StructField('PLD', StringType(), False),StructField('exampleHosts', StringType(), False)])\nhost_examples_df=host_example_rdd.toDF(out_schema) \nhost_examples_df.show(100)","user":"anonymous","dateUpdated":"2017-11-03T13:01:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+\n|                 PLD|        exampleHosts|\n+--------------------+--------------------+\n|  galeriaccrj.com.br|[galeriaccrj.com.br]|\n|cooperativadedise...|[cooperativadedis...|\n|klemens-transport...|[klemens-transpor...|\n|   agent-fashion.com| [agent-fashion.com]|\n| xishannongjiale.com|[xishannongjiale....|\n|         gfxthai.org|       [gfxthai.org]|\n|       sieheunten.de|     [sieheunten.de]|\n|       carvenrose.gr|     [carvenrose.gr]|\n|      georgesiga.com|    [georgesiga.com]|\n|restaurantdurmito...|[restaurantdurmit...|\n| zhangxiaohui.com.cn|[zhangxiaohui.com...|\n|       fa-altmark.de|     [fa-altmark.de]|\n|        remibaby.com|      [remibaby.com]|\n|         cclb.com.my|       [cclb.com.my]|\n|truckandplantonli...|[truckandplantonl...|\n|       savemouse.com|     [savemouse.com]|\n|    meta-maniera.com|[meta-maniera.com...|\n|          vibeffe.it|        [vibeffe.it]|\n|halikarnasbodrum.com|[halikarnasbodrum...|\n|conversecollectio...|[conversecollecti...|\n|         softgame.it|       [softgame.it]|\n|gartenverein-oest...|[gartenverein-oes...|\n|  brain-terminal.net|[brain-terminal.net]|\n|  renholdsservice.as|[renholdsservice.as]|\n|   shadewoodfarm.com| [shadewoodfarm.com]|\n|clientesatisfeiro...|[clientesatisfeir...|\n|    minoanseminar.gr|  [minoanseminar.gr]|\n|saveyoursightscre...|[saveyoursightscr...|\n|      frockology.com|    [frockology.com]|\n|       medicbank.com|     [medicbank.com]|\n|          ment.gv.at|        [ment.gv.at]|\n|      webhonchoz.com|    [webhonchoz.com]|\n|resourcesinaction...|[resourcesinactio...|\n|giant-enkai-serie...|[giant-enkai-seri...|\n|  breakawayhooks.net|[breakawayhooks.net]|\n|    goldsuntv.com.tw|  [goldsuntv.com.tw]|\n|  akiba-handbook.com|[akiba-handbook.com]|\n|impresaedilebolog...|[impresaedilebolo...|\n|           belva.net|         [belva.net]|\n|         topshape.de|       [topshape.de]|\n|        taylorpd.com|      [taylorpd.com]|\n|        unagi-ice.jp|      [unagi-ice.jp]|\n|fa5a8ntasyfootbal...|[fa5a8ntasyfootba...|\n|     monacollege.com|   [monacollege.com]|\n|        mcmrh.com.br|      [mcmrh.com.br]|\n|             geil.io|           [geil.io]|\n|personal-injury-r...|[personal-injury-...|\n|   premiumclicks.com| [premiumclicks.com]|\n|xn-----6kcaj6asiw...|[xn-----6kcaj6asi...|\n|acheirestaurantes...|[acheirestaurante...|\n|      nylon-spot.com|    [nylon-spot.com]|\n|    fantasyflower.ru|  [fantasyflower.ru]|\n|ristorantealbergo...|[ristorantealberg...|\n|cosmeticsurgeryst...|[cosmeticsurgerys...|\n|        utpdp.com.ua|      [utpdp.com.ua]|\n|   anima-diamante.ch| [anima-diamante.ch]|\n|   shangliguzhen.org| [shangliguzhen.org]|\n|restaurantehuerto...|[restaurantehuert...|\n|       livarbors.com|     [livarbors.com]|\n|      ipedigree.info|    [ipedigree.info]|\n| matildemenendez.com|[matildemenendez....|\n| nmprolivestream.com|[nmprolivestream....|\n|   1sttrusttitle.com| [1sttrusttitle.com]|\n|         frogzog.com|       [frogzog.com]|\n|          ektran.com|        [ektran.com]|\n|   selmabusiness.com| [selmabusiness.com]|\n|          shikpik.ir|        [shikpik.ir]|\n|lendasurbanasbr.c...|[lendasurbanasbr....|\n|         hackfbs.com|       [hackfbs.com]|\n|australianmovieli...|[australianmoviel...|\n|   todayswhisper.com| [todayswhisper.com]|\n| bushidotenerife.com|[bushidotenerife....|\n|          hjalmur.de|        [hjalmur.de]|\n|     veerupopuri.com|   [veerupopuri.com]|\n|     satsumababy.com|   [satsumababy.com]|\n|denisevargasadvoc...|[denisevargasadvo...|\n|        axtionk9.com|      [axtionk9.com]|\n|             dr13.cf|           [dr13.cf]|\n|unlockedapplephon...|[unlockedapplepho...|\n|   italymodas.com.ar| [italymodas.com.ar]|\n|goddessdanielledi...|[goddessdanielled...|\n|       jilliemae.com|     [jilliemae.com]|\n|rebeldesmarketing...|[rebeldesmarketin...|\n|downend-motors.co.uk|[downend-motors.c...|\n|    theploughpub.com|  [theploughpub.com]|\n|  summerstrust.co.uk|[summerstrust.co.uk]|\n|        sliksock.com|      [sliksock.com]|\n|          shavel.com|[shavel.com, dev....|\n|bioenergetichealt...|[bioenergeticheal...|\n|xn--marktplatz-gg...|[xn--marktplatz-g...|\n|   familievanbeek.eu| [familievanbeek.eu]|\n|     areamatatui.com|   [areamatatui.com]|\n|        xxl-sport.de|      [xxl-sport.de]|\n|zonepaydayloans.c...|[zonepaydayloans....|\n|     leticiareig.com|   [leticiareig.com]|\n|  ostrigemeos.com.br|[ostrigemeos.com.br]|\n|   harasdhurlvent.fr| [harasdhurlvent.fr]|\n|senaduriasantiago...|[senaduriasantiag...|\n|     theolivegal.com|   [theolivegal.com]|\n|carpetcleanersmor...|[carpetcleanersmo...|\n+--------------------+--------------------+\nonly showing top 100 rows\n\n"}]},"apps":[],"jobName":"paragraph_1509710570553_-1154519715","id":"20171027-113721_1699720093","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:27:57+0000","dateFinished":"2017-11-03T13:49:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8011"},{"text":"%pyspark\n\n# Join in/out-link summaries with host examples dataframe\nexample_df=pld_df_joined.join(host_examples_df, pld_df_joined.PLDin==host_examples_df.PLD, \"outer\").drop(\"PLDin\").select(\"PLD\",\"exampleHosts\",\"exampleInLinkPlds\",\"exampleOutLinkPlds\")\nexample_df.show(100)\nexample_df.cache()\nexample_df.count() # Should still be 91M!","user":"anonymous","dateUpdated":"2017-11-03T13:53:05+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------------+--------------------+--------------------+\n|                 PLD|        exampleHosts|   exampleInLinkPlds|  exampleOutLinkPlds|\n+--------------------+--------------------+--------------------+--------------------+\n|0----------------...|[0---------------...|         [nomina.ru]|                null|\n|           0-0la.com|         [0-0la.com]|[jessicawilson.co...|[wordpress.org, g...|\n|           0-3-6.com|         [0-3-6.com]|[3d114.com, menok...|                null|\n|           0-3ani.ro|         [0-3ani.ro]|[cere.ro, adedir....|                null|\n|           0-5-1.com|         [0-5-1.com]|    [allthecom.info]|                null|\n|       0-60times.net|     [0-60times.net]|[lodekka.com, kev...|                null|\n|            0-744.cn|          [0-744.cn]|     [wordpress.com]|                null|\n|0-ads-free-web-pa...|[0-ads-free-web-p...|[list-of-domains....|                null|\n|       0-artlove.net|     [0-artlove.net]|[list-of-domains....|                null|\n|  0-clubpenguin-0.tk|[0-clubpenguin-0.tk]|  [similarsites.com]|                null|\n+--------------------+--------------------+--------------------+--------------------+\nonly showing top 10 rows\n\n99405158\n"}]},"apps":[],"jobName":"paragraph_1509710570554_-1153365469","id":"20171012-142038_800861977","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:49:37+0000","dateFinished":"2017-11-03T13:51:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8012"},{"text":"%pyspark\n\n# Save final table to S3 in parquet format, broken into smaller files (for fast reading into Paul 7 that will combine original summaries with examples)\noutputURI=\"s3://billsdata.net/CommonCrawl/domain_examples4/\"\n#codec=\"org.apache.hadoop.io.compress.GzipCodec\"\n#example_df.write.format('com.databricks.spark.csv').options(header='true', codec=codec).save(outputURI) # Single GZIP initially for debugging\nexample_df.write.save(outputURI)","user":"anonymous","dateUpdated":"2017-11-03T13:52:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509710570555_-1153750217","id":"20171018-175711_441001507","dateCreated":"2017-11-03T12:02:50+0000","dateStarted":"2017-11-03T13:52:01+0000","dateFinished":"2017-11-03T13:57:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8013"},{"text":"%pyspark\n","dateUpdated":"2017-11-03T12:02:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509710570555_-1153750217","id":"20171027-094657_452033266","dateCreated":"2017-11-03T12:02:50+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8014"}],"name":"Paul 6 - examples for domain summary","id":"2CYSRQYVA","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}